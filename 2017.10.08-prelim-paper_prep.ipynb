{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2017.10.08 - prelim - paper prep**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.08-prelim-paper_prep.ipynb#Reliability\" data-toc-modified-id=\"Reliability-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Reliability</a></span></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.08-prelim-paper_prep.ipynb#Disagreements\" data-toc-modified-id=\"Disagreements-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Disagreements</a></span><ul class=\"toc-item\"><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.08-prelim-paper_prep.ipynb#Disagreements---Human-Error\" data-toc-modified-id=\"Disagreements---Human-Error-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Disagreements - Human Error</a></span></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.08-prelim-paper_prep.ipynb#Disagreements---Computer-Error\" data-toc-modified-id=\"Disagreements---Computer-Error-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Disagreements - Computer Error</a></span></li></ul></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.08-prelim-paper_prep.ipynb#Additional-analysis\" data-toc-modified-id=\"Additional-analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Additional analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.08-prelim-paper_prep.ipynb#Human-Precision-and-Recall\" data-toc-modified-id=\"Human-Precision-and-Recall-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Human Precision and Recall</a></span></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.08-prelim-paper_prep.ipynb#Network-Analysis\" data-toc-modified-id=\"Network-Analysis-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Network Analysis</a></span></li></ul></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.08-prelim-paper_prep.ipynb#Paper-Edits\" data-toc-modified-id=\"Paper-Edits-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Paper Edits</a></span></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.08-prelim-paper_prep.ipynb#TODO\" data-toc-modified-id=\"TODO-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>TODO</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliability\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, need to make sure I understand which results are the final results.\n",
    "\n",
    "To view results: [https://research.local/sourcenet/sourcenet/analysis/reliability/names/results/view](https://research.local/sourcenet/sourcenet/analysis/reliability/names/results/view)\n",
    "\n",
    "I am pretty sure that the human-only results (the ones I will write about) are results with labels:\n",
    "\n",
    "- \"`prelim_reliability_combined_human_final`\"\n",
    "- and \"`prelim_reliability_combined_human`\" (this is old code, versus \"final\" calculated with rewritten code - numbers should be identical).\n",
    "\n",
    "Path to Dropbox folder that holds PDF and Excel file output of reliability numbers:\n",
    "\n",
    "- Dropbox/academia/MSU/program_stuff/prelim_paper/analysis/reliability/2016-data\n",
    "\n",
    "Code to actually calculate reliability numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start to support python 3:\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "\n",
    "#==============================================================================#\n",
    "# ! imports\n",
    "#==============================================================================#\n",
    "\n",
    "# grouped by functional area, then alphabetical order by package, then\n",
    "#     alphabetical order by name of thing being imported.\n",
    "\n",
    "# sourcenet_analysis imports\n",
    "from sourcenet_analysis.reliability.reliability_names_analyzer import ReliabilityNamesAnalyzer\n",
    "\n",
    "#==============================================================================#\n",
    "# ! logic\n",
    "#==============================================================================#\n",
    "\n",
    "# declare variables\n",
    "my_analysis_instance = None\n",
    "label = \"\"\n",
    "indices_to_process = -1\n",
    "result_status = \"\"\n",
    "\n",
    "# make reliability instance\n",
    "my_analysis_instance = ReliabilityNamesAnalyzer()\n",
    "\n",
    "# database connection information - 2 options...  Enter it here:\n",
    "#my_analysis_instance.db_username = \"\"\n",
    "#my_analysis_instance.db_password = \"\"\n",
    "#my_analysis_instance.db_host = \"localhost\"\n",
    "#my_analysis_instance.db_name = \"sourcenet\"\n",
    "\n",
    "# Or set up the following properties in Django_Config, inside the django admins.\n",
    "#     All have application of: \"sourcenet-db-admin\":\n",
    "#     - db_username\n",
    "#     - db_password\n",
    "#     - db_host\n",
    "#     - db_port\n",
    "#     - db_name\n",
    "\n",
    "# run the analyze method, see what happens.\n",
    "#label = \"prelim_reliability_test\"\n",
    "#indices_to_process = 3\n",
    "#label = \"prelim_reliability_combined_human\"\n",
    "#indices_to_process = 3\n",
    "#label = \"name_data_test_combined_human\"\n",
    "#indices_to_process = 3\n",
    "#label = \"prelim_reliability_combined_human_final\"\n",
    "#indices_to_process = 3\n",
    "#label = \"prelim_reliability_combined_all\"\n",
    "#indices_to_process = 4\n",
    "#label = \"prelim_reliability_combined_all_final\"\n",
    "#indices_to_process = 4\n",
    "#label = \"prelim_reliability_test_human\"\n",
    "#indices_to_process = 3\n",
    "#label = \"prelim_reliability_test_all\"\n",
    "#indices_to_process = 4\n",
    "label = \"prelim_month\"\n",
    "indices_to_process = 2\n",
    "result_status = my_analysis_instance.analyze_reliability_names( label, indices_to_process )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disagreements\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Look at stats for disagreements and evaluation, including human and computer errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disagreements - Human Error\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Human error - Per article (how many have ground truth?) and per decision?  How many errors, compared to total number of decisions, and what kind of errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disagreements - Computer Error\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Computer error - look over classes of error for trends (systemic error) and interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional analysis\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Precision and Recall\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "How hard to calculate precision and recall for humans versus ground truth?  Should be able to set it up so that coder 1 is as it was for computer (ground_truth having precedence) and then set up coder 2 up the same way, but without ground_truth..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Generate some basic network statistics from the ground truth and automated attribution data, characterize and compare using QAP (including explaining substantial limitations of this given sparseness of networks).\n",
    "\n",
    "- find code used to derive network information last time.\n",
    "- run it again on full month of data, rather than just a week.\n",
    "- examine traits of ground_truth and automated networks - compare with QAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Edits\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "TODO:\n",
    "\n",
    "- path to paper: `Dropbox/academia/MSU/program_stuff/prelim_paper/paper/2017.10.30/Morgan-Prelim.docx`\n",
    "- cut the shit out of lit. review.\n",
    "- update methods\n",
    "\n",
    "    - generate content analysis data.\n",
    "    - assess reliability.\n",
    "    - generate attribution data using OpenCalais API.\n",
    "    - evaluate disagreements to establish ground truth (fix human errors).\n",
    "    - calculate precision and recall.\n",
    "    - examine resulting networks.\n",
    "\n",
    "- update results\n",
    "- update discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- try to figure out why there are not coder IDs in some of the cells in `Reliability_Names_Results` rows for labels \"`prelim_reliability_combined_human_final`\" and \"`prelim_reliability_combined_human`\" (I think it is because I added logic to account for more than one person being included across the coding for an index, and this lack of a single coder ID means more than one coder was present).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sourcenet (Python 3)",
   "language": "python",
   "name": "sourcenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
