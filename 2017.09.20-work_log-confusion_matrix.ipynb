{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__work log - confusion matrix__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#Setup---Imports\" data-toc-modified-id=\"Setup---Imports-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Setup - Imports</a></span></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#Setup---Initialize-Django\" data-toc-modified-id=\"Setup---Initialize-Django-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Setup - Initialize Django</a></span></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#Setup---Tools\" data-toc-modified-id=\"Setup---Tools-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Setup - Tools</a></span></li></ul></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#Build-Confusion-Matrix-Data\" data-toc-modified-id=\"Build-Confusion-Matrix-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Build Confusion Matrix Data</a></span><ul class=\"toc-item\"><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#Person-detection\" data-toc-modified-id=\"Person-detection-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Person detection</a></span></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#Person-lookup\" data-toc-modified-id=\"Person-lookup-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Person lookup</a></span></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#Person-type\" data-toc-modified-id=\"Person-type-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Person type</a></span><ul class=\"toc-item\"><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#Person-type---Subjects\" data-toc-modified-id=\"Person-type---Subjects-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Person type - Subjects</a></span></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#Person-type---Sources\" data-toc-modified-id=\"Person-type---Sources-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Person type - Sources</a></span></li></ul></li></ul></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#Notes\" data-toc-modified-id=\"Notes-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Notes</a></span></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#TODO\" data-toc-modified-id=\"TODO-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>TODO</a></span><ul class=\"toc-item\"><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#TODO---filter-articles-that-are-not-news\" data-toc-modified-id=\"TODO---filter-articles-that-are-not-news-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>TODO - filter articles that are not news</a></span></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#Debugging\" data-toc-modified-id=\"Debugging-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Debugging</a></span></li></ul></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#DONE\" data-toc-modified-id=\"DONE-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>DONE</a></span></li><li><span><a href=\"https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.09.20-work_log-confusion_matrix.ipynb#NEXT\" data-toc-modified-id=\"NEXT-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>NEXT</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Imports\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages imported at 2017-09-21 03:36:10.705711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathanmorgan/.virtualenvs/sourcenet/lib/python3.5/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import numpy\n",
    "import pandas\n",
    "import patsy\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import six\n",
    "import statsmodels\n",
    "import statsmodels.api\n",
    "\n",
    "print( \"packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jonathanmorgan/work/sourcenet/django/research/work/msu_phd_work'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Initialize Django\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, initialize my dev django project, so I can run code in this notebook that references my django models and can talk to the database using my project's settings.\n",
    "\n",
    "You need to have installed your virtualenv with django as a kernel, then select that kernel for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "django initialized at 2017-09-21 01:47:01.973604\n"
     ]
    }
   ],
   "source": [
    "%run django_init.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import any `sourcenet` or `sourcenet_analysis` models or classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourcenet and sourcenet_analysis packages imported at 2017-09-21 01:47:05.297395\n"
     ]
    }
   ],
   "source": [
    "# sourcenet_analysis models.\n",
    "from sourcenet_analysis.models import Reliability_Names\n",
    "\n",
    "print( \"sourcenet and sourcenet_analysis packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Tools\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Write functions here to do math, so that we can reuse said tools below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Confusion Matrix Data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "A basic confusion matrix ( [https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix) ) contains counts of true positives, true negatives, false positives, and false negatives for a given binary or boolean (yes/no) classification decision you are asking someone or something to make.\n",
    "\n",
    "To create a confusion matrix, you need two associated vectors containing classification decisions (0s and 1s), one that contains ground truth, and one that contains values predicted by whatever coder you are testing.  For each associated pair of values:\n",
    "\n",
    "- Start with the predicted value: positive (1) or negative (0).\n",
    "- Look at the corresponding ground truth value.  If they match, it is \"true\".  If not, it is \"false\".\n",
    "- So, predicted 1 and ground_truth 1 is a \"true positive\".\n",
    "- Add one to the counter for the class of prediction: \"true positive\", \"true negative\", \"false positive\", \"false negative\".\n",
    "\n",
    "Once you have your basic confusion matrix, the counts of true positives, true negatives, false positives, and false negatives can then be used to calculate a set of different scores and values one can use to assess the quality of predictive models.  These scores include \"precision and recall\", \"accuracy\", an \"F1 score\" (a harmonic mean), and a \"diagnostic odds ratio\", among many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person detection\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For each person detected across the set of articles, look at whether the automated coder correctly detected the person, independent of eventual lookup or person type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2446 rows with label in ['prelim_month']\n",
      "==> Predicted positives: 2383\n",
      "==> Ground truth positives: 2378\n",
      "==> True positives: 2315\n",
      "==> False positives: 68\n",
      "==> Predicted negatives: 63\n",
      "==> Ground truth negatives: 68\n",
      "==> True negatives: 0\n",
      "==> False negatives: 63\n",
      "==> Precision (true positive/predicted positive): 0.9714645404951742\n",
      "==> Recall (true positive/ground truth positive): 0.9735071488645921\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "reliability_names_label = None\n",
    "label_in_list = []\n",
    "reliability_names_qs = None\n",
    "ground_truth_coder_index = 1\n",
    "predicted_coder_index = 2\n",
    "\n",
    "# processing\n",
    "column_name = \"\"\n",
    "predicted_value = -1\n",
    "predicted_list = []\n",
    "ground_truth_value = -1\n",
    "ground_truth_list = []\n",
    "ground_truth_positive_count = -1\n",
    "predicted_positive_count = -1\n",
    "true_positive_count = -1\n",
    "false_positive_count = -1\n",
    "ground_truth_negative_count = -1\n",
    "predicted_negative_count = -1\n",
    "true_negative_count = -1\n",
    "false_negative_count = -1\n",
    "reliability_names_instance = None\n",
    "\n",
    "# set label\n",
    "reliability_names_label = \"prelim_month\"\n",
    "\n",
    "# lookup Reliability_Names for selected label\n",
    "label_in_list.append( reliability_names_label )\n",
    "reliability_names_qs = Reliability_Names.objects.filter( label__in = label_in_list )\n",
    "\n",
    "print( \"Found \" + str( reliability_names_qs.count() ) + \" rows with label in \" + str( label_in_list ) )\n",
    "\n",
    "# loop over records\n",
    "predicted_value = -1\n",
    "predicted_list = []\n",
    "ground_truth_value = -1\n",
    "ground_truth_list = []\n",
    "ground_truth_positive_count = 0\n",
    "predicted_positive_count = 0\n",
    "true_positive_count = 0\n",
    "false_positive_count = 0\n",
    "ground_truth_negative_count = 0\n",
    "predicted_negative_count = 0\n",
    "true_negative_count = 0\n",
    "false_negative_count = 0\n",
    "for reliability_names_instance in reliability_names_qs:\n",
    "    \n",
    "    # get detected flag from ground truth and predicted columns and add them to list.\n",
    "    \n",
    "    # ==> ground truth\n",
    "    column_name = Reliability_Names.FIELD_NAME_PREFIX_CODER\n",
    "    column_name += str( ground_truth_coder_index )\n",
    "    column_name += \"_\" + Reliability_Names.FIELD_NAME_SUFFIX_DETECTED\n",
    "    ground_truth_value = getattr( reliability_names_instance, column_name )\n",
    "    ground_truth_list.append( ground_truth_value )\n",
    "    \n",
    "    # ==> predicted\n",
    "    column_name = Reliability_Names.FIELD_NAME_PREFIX_CODER\n",
    "    column_name += str( predicted_coder_index )\n",
    "    column_name += \"_\" + Reliability_Names.FIELD_NAME_SUFFIX_DETECTED\n",
    "    predicted_value = getattr( reliability_names_instance, column_name )\n",
    "    predicted_list.append( predicted_value )\n",
    "    \n",
    "    # add to counts\n",
    "    \n",
    "    # ==> ground truth\n",
    "    if ( ground_truth_value == 0 ):\n",
    "        \n",
    "        # ground truth negative\n",
    "        ground_truth_negative_count += 1\n",
    "        \n",
    "    # not zero - so 1 (or supports other integer values)\n",
    "    else:\n",
    "\n",
    "        # ground truth positive\n",
    "        ground_truth_positive_count += 1\n",
    "                \n",
    "    #-- END check to see if positive or negative --# \n",
    "    \n",
    "    \n",
    "    if ( predicted_value == 0 ):\n",
    "        \n",
    "        # predicted negative\n",
    "        predicted_negative_count += 1\n",
    "        \n",
    "        # equal to ground_truth?\n",
    "        if ( predicted_value == ground_truth_value ):\n",
    "            \n",
    "            # true negative\n",
    "            true_negative_count += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # false negative\n",
    "            false_negative_count += 1\n",
    "            \n",
    "        #-- END check to see if true or false --#\n",
    "        \n",
    "    # not zero - so 1 (or supports other integer values)\n",
    "    else:\n",
    "\n",
    "        # predicted positive\n",
    "        predicted_positive_count += 1\n",
    "        \n",
    "        # equal to ground_truth?\n",
    "        if ( predicted_value == ground_truth_value ):\n",
    "            \n",
    "            # true positive\n",
    "            true_positive_count += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # false positive\n",
    "            false_positive_count += 1\n",
    "            \n",
    "        #-- END check to see if true or false --#\n",
    "        \n",
    "    #-- END check to see if positive or negative --# \n",
    "        \n",
    "#-- END loop over Reliability_Names instances. --#\n",
    "\n",
    "print( \"==> Predicted positives: \" + str( predicted_positive_count ) )\n",
    "print( \"==> Ground truth positives: \" + str( ground_truth_positive_count ) )\n",
    "print( \"==> True positives: \" + str( true_positive_count ) )\n",
    "print( \"==> False positives: \" + str( false_positive_count ) )\n",
    "print( \"==> Predicted negatives: \" + str( predicted_negative_count ) )\n",
    "print( \"==> Ground truth negatives: \" + str( ground_truth_negative_count ) )\n",
    "print( \"==> True negatives: \" + str( true_negative_count ) )\n",
    "print( \"==> False negatives: \" + str( false_negative_count ) )\n",
    "print( \"==> Precision (true positive/predicted positive): \" + str( ( true_positive_count / predicted_positive_count ) ) )\n",
    "print( \"==> Recall (true positive/ground truth positive): \" + str( ( true_positive_count / ground_truth_positive_count ) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Predicted positives: 2383 ( 2383 )\n",
      "==> Ground truth positives: 2378\n",
      "==> True positives: 2315\n",
      "==> False positives: 68\n",
      "==> Predicted negatives: 63 ( 63 )\n",
      "==> Ground truth negatives: 68\n",
      "==> True negatives: 0\n",
      "==> False negatives: 63\n",
      "==> Precision (true positive/predicted positive): 0.9714645404951742\n",
      "==> Recall (true positive/ground truth positive): 0.9735071488645921\n"
     ]
    }
   ],
   "source": [
    "# loop over lists to derive counts\n",
    "predicted_value = -1\n",
    "ground_truth_value = -1\n",
    "ground_truth_positive_count = 0\n",
    "predicted_positive_count = 0\n",
    "true_positive_count = 0\n",
    "false_positive_count = 0\n",
    "ground_truth_negative_count = 0\n",
    "predicted_negative_count = 0\n",
    "true_negative_count = 0\n",
    "false_negative_count = 0\n",
    "list_index = -1\n",
    "for predicted_value in predicted_list:\n",
    "\n",
    "    # increment index and get associated item from ground_truth_list\n",
    "    list_index += 1\n",
    "    ground_truth_value = ground_truth_list[ list_index ]\n",
    "    \n",
    "    # add to counts\n",
    "    \n",
    "    # ==> ground truth\n",
    "    if ( ground_truth_value == 0 ):\n",
    "        \n",
    "        # ground truth negative\n",
    "        ground_truth_negative_count += 1\n",
    "        \n",
    "    # not zero - so 1 (or supports other integer values)\n",
    "    else:\n",
    "\n",
    "        # ground truth positive\n",
    "        ground_truth_positive_count += 1\n",
    "                \n",
    "    #-- END check to see if positive or negative --# \n",
    "    \n",
    "    \n",
    "    if ( predicted_value == 0 ):\n",
    "        \n",
    "        # predicted negative\n",
    "        predicted_negative_count += 1\n",
    "        \n",
    "        # equal to ground_truth?\n",
    "        if ( predicted_value == ground_truth_value ):\n",
    "            \n",
    "            # true negative\n",
    "            true_negative_count += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # false negative\n",
    "            false_negative_count += 1\n",
    "            \n",
    "        #-- END check to see if true or false --#\n",
    "        \n",
    "    # not zero - so 1 (or supports other integer values)\n",
    "    else:\n",
    "\n",
    "        # predicted positive\n",
    "        predicted_positive_count += 1\n",
    "        \n",
    "        # equal to ground_truth?\n",
    "        if ( predicted_value == ground_truth_value ):\n",
    "            \n",
    "            # true positive\n",
    "            true_positive_count += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # false positive\n",
    "            false_positive_count += 1\n",
    "            \n",
    "        #-- END check to see if true or false --#\n",
    "        \n",
    "    #-- END check to see if positive or negative --# \n",
    "\n",
    "#-- END loop over list items. --#\n",
    "    \n",
    "print( \"==> Predicted positives: \" + str( predicted_positive_count ) + \" ( \" + str( ( true_positive_count + false_positive_count ) ) + \" )\" )\n",
    "print( \"==> Ground truth positives: \" + str( ground_truth_positive_count ) )\n",
    "print( \"==> True positives: \" + str( true_positive_count ) )\n",
    "print( \"==> False positives: \" + str( false_positive_count ) )\n",
    "print( \"==> Predicted negatives: \" + str( predicted_negative_count ) + \" ( \" + str( ( true_negative_count + false_negative_count ) ) + \" )\" )\n",
    "print( \"==> Ground truth negatives: \" + str( ground_truth_negative_count ) )\n",
    "print( \"==> True negatives: \" + str( true_negative_count ) )\n",
    "print( \"==> False negatives: \" + str( false_negative_count ) )\n",
    "print( \"==> Precision (true positive/predicted positive): \" + str( ( true_positive_count / predicted_positive_count ) ) )\n",
    "print( \"==> Recall (true positive/ground truth positive): \" + str( ( true_positive_count / ground_truth_positive_count ) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.        ,  0.97146454]),\n",
       " array([ 0.        ,  0.97350715]),\n",
       " array([ 0.        ,  0.97248477]),\n",
       " array([  68, 2378]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try scikit-learn: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
    "sklearn.metrics.precision_recall_fscore_support( ground_truth_list, predicted_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0   68]\n",
      " [  63 2315]]\n",
      "==> Predicted positives: 2383 ( 2383 )\n",
      "==> Ground truth positives: 2378\n",
      "==> True positives: 2315\n",
      "==> False positives: 68\n",
      "==> Predicted negatives: 63 ( 63 )\n",
      "==> Ground truth negatives: 68\n",
      "==> True negatives: 0\n",
      "==> False negatives: 63\n",
      "==> Precision (true positive/predicted positive): 0.971464540495\n",
      "==> Recall (true positive/ground truth positive): 0.973507148865\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn confusion matrix\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "conf_matrix = sklearn.metrics.confusion_matrix( ground_truth_list, predicted_list )\n",
    "print( str( conf_matrix ) )\n",
    "\n",
    "# get counts in variables\n",
    "true_positive_count = conf_matrix[ 1 ][ 1 ]\n",
    "false_positive_count = conf_matrix[ 0 ][ 1 ]\n",
    "true_negative_count = conf_matrix[ 0 ][ 0 ]\n",
    "false_negative_count = conf_matrix[ 1 ][ 0 ]\n",
    "\n",
    "print( \"==> Predicted positives: \" + str( predicted_positive_count ) + \" ( \" + str( ( true_positive_count + false_positive_count ) ) + \" )\" )\n",
    "print( \"==> Ground truth positives: \" + str( ground_truth_positive_count ) )\n",
    "print( \"==> True positives: \" + str( true_positive_count ) )\n",
    "print( \"==> False positives: \" + str( false_positive_count ) )\n",
    "print( \"==> Predicted negatives: \" + str( predicted_negative_count ) + \" ( \" + str( ( true_negative_count + false_negative_count ) ) + \" )\" )\n",
    "print( \"==> Ground truth negatives: \" + str( ground_truth_negative_count ) )\n",
    "print( \"==> True negatives: \" + str( true_negative_count ) )\n",
    "print( \"==> False negatives: \" + str( false_negative_count ) )\n",
    "print( \"==> Precision (true positive/predicted positive): \" + str( ( true_positive_count / predicted_positive_count ) ) )\n",
    "print( \"==> Recall (true positive/ground truth positive): \" + str( ( true_positive_count / ground_truth_positive_count ) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted   0     1\n",
      "Actual             \n",
      "0           0    68\n",
      "1          63  2315\n"
     ]
    }
   ],
   "source": [
    "# pandas\n",
    "# https://stackoverflow.com/questions/2148543/how-to-write-a-confusion-matrix-in-python\n",
    "y_actu = pandas.Series( ground_truth_list, name='Actual')\n",
    "y_pred = pandas.Series( predicted_list, name='Predicted')\n",
    "df_confusion = pandas.crosstab(y_actu, y_pred)\n",
    "print( str( df_confusion ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More to look into:\n",
    "\n",
    "- https://stackoverflow.com/questions/39626401/how-to-get-odds-ratios-and-other-related-features-with-scikit-learn#39711837"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person lookup\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For each person detected across the set of articles, look at whether the automated coder correctly looked up the person (so compare person IDs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person type\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For each person detected across the set of articles, look at whether the automated coder assigned the correct type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person type - Subjects\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For each person detected across the set of articles classified by Ground truth as a subject, look at whether the automated coder assigned the correct person type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person type - Sources\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For each person detected across the set of articles classified by Ground truth as a source, look at whether the automated coder assigned the correct person type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "TODO:\n",
    "\n",
    "- add field to article table for non-news or is_hard_news.\n",
    "- Article 22181 - Why is the incorrect person \"Christian Reformed Church\" tagged as being mentioned in paragraph 14 rather than 18 where that string is?\n",
    "- Want a way to limit to disagreements where quoted?  Might not - this is a start to assessing erroneous agreement.  If yes, 1 < coding time < 4 hours.\n",
    "\n",
    "    - problem - `Reliability_Names.person_type` only has three values - \"author\", \"subject\", \"source\" - might need a row-level measure of \"`has_mention`\", \"`has_quote`\" to more readily capture rows where disagreement is over quoted-or-not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - filter articles that are not news\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "TODO:\n",
    "\n",
    "- Article 22705 - Book roundup - probably should just remove from study, and see if meta-data about articles that could be used to automatically filter these type of articles out in the future.  Leaving in for now, but should flag these so I can do comparison of numbers with and without.\n",
    "- Use keywords for Lakeshore section stories to try to filter out sports stories (\"Basketball\").  Maybe try this for all articles in the month?\n",
    "- sports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Issues to debug:\n",
    "\n",
    "- TK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "DONE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sourcenet (Python 3)",
   "language": "python",
   "name": "sourcenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
