{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__prelim_month - confusion matrix__\n",
    "\n",
    "- original title: `2017.09.20 - work log - prelim_month - confusion matrix`\n",
    "- original file name: `2017.09.20-work_log-prelim_month-confusion_matrix.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup---Imports\" data-toc-modified-id=\"Setup---Imports-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Setup - Imports</a></span></li><li><span><a href=\"#Setup---Initialize-Django\" data-toc-modified-id=\"Setup---Initialize-Django-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Setup - Initialize Django</a></span></li><li><span><a href=\"#Setup---Tools\" data-toc-modified-id=\"Setup---Tools-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Setup - Tools</a></span></li></ul></li><li><span><a href=\"#Build-Confusion-Matrix-Data\" data-toc-modified-id=\"Build-Confusion-Matrix-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Build Confusion Matrix Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Person-detection\" data-toc-modified-id=\"Person-detection-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Person detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Person-detection---build-value-lists\" data-toc-modified-id=\"Person-detection---build-value-lists-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Person detection - build value lists</a></span></li><li><span><a href=\"#Person-detection---confusion-matrix\" data-toc-modified-id=\"Person-detection---confusion-matrix-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Person detection - confusion matrix</a></span></li></ul></li><li><span><a href=\"#Person-lookup\" data-toc-modified-id=\"Person-lookup-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Person lookup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Person-lookup---build-value-lists\" data-toc-modified-id=\"Person-lookup---build-value-lists-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Person lookup - build value lists</a></span></li><li><span><a href=\"#Person-lookup---confusion-matrix\" data-toc-modified-id=\"Person-lookup---confusion-matrix-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Person lookup - confusion matrix</a></span></li></ul></li><li><span><a href=\"#Person-Types\" data-toc-modified-id=\"Person-Types-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Person Types</a></span><ul class=\"toc-item\"><li><span><a href=\"#Function:-build-confusion-lists-for-a-given-categorical-value\" data-toc-modified-id=\"Function:-build-confusion-lists-for-a-given-categorical-value-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Function: build confusion lists for a given categorical value</a></span></li><li><span><a href=\"#Person-type---Authors\" data-toc-modified-id=\"Person-type---Authors-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Person type - Authors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Person-type---Authors---build-value-lists\" data-toc-modified-id=\"Person-type---Authors---build-value-lists-2.3.2.1\"><span class=\"toc-item-num\">2.3.2.1&nbsp;&nbsp;</span>Person type - Authors - build value lists</a></span></li><li><span><a href=\"#Person-type---Authors---confusion-matrix\" data-toc-modified-id=\"Person-type---Authors---confusion-matrix-2.3.2.2\"><span class=\"toc-item-num\">2.3.2.2&nbsp;&nbsp;</span>Person type - Authors - confusion matrix</a></span></li></ul></li><li><span><a href=\"#Person-type---Subjects\" data-toc-modified-id=\"Person-type---Subjects-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Person type - Subjects</a></span><ul class=\"toc-item\"><li><span><a href=\"#Person-type---Subjects---build-value-lists\" data-toc-modified-id=\"Person-type---Subjects---build-value-lists-2.3.3.1\"><span class=\"toc-item-num\">2.3.3.1&nbsp;&nbsp;</span>Person type - Subjects - build value lists</a></span></li><li><span><a href=\"#Person-type---Subjects---confusion-matrix\" data-toc-modified-id=\"Person-type---Subjects---confusion-matrix-2.3.3.2\"><span class=\"toc-item-num\">2.3.3.2&nbsp;&nbsp;</span>Person type - Subjects - confusion matrix</a></span></li></ul></li><li><span><a href=\"#Person-type---Sources\" data-toc-modified-id=\"Person-type---Sources-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>Person type - Sources</a></span><ul class=\"toc-item\"><li><span><a href=\"#Person-type---Sources---build-value-lists\" data-toc-modified-id=\"Person-type---Sources---build-value-lists-2.3.4.1\"><span class=\"toc-item-num\">2.3.4.1&nbsp;&nbsp;</span>Person type - Sources - build value lists</a></span></li><li><span><a href=\"#Person-type---Sources---confusion-matrix\" data-toc-modified-id=\"Person-type---Sources---confusion-matrix-2.3.4.2\"><span class=\"toc-item-num\">2.3.4.2&nbsp;&nbsp;</span>Person type - Sources - confusion matrix</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Notes</a></span></li><li><span><a href=\"#NEXT\" data-toc-modified-id=\"NEXT-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>NEXT</a></span></li><li><span><a href=\"#TODO\" data-toc-modified-id=\"TODO-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>TODO</a></span><ul class=\"toc-item\"><li><span><a href=\"#TODO---filter-articles-that-are-not-news\" data-toc-modified-id=\"TODO---filter-articles-that-are-not-news-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>TODO - filter articles that are not news</a></span></li><li><span><a href=\"#Debugging\" data-toc-modified-id=\"Debugging-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Debugging</a></span></li></ul></li><li><span><a href=\"#DONE\" data-toc-modified-id=\"DONE-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>DONE</a></span></li><li><span><a href=\"#Appendix---Build-confusion-matrix\" data-toc-modified-id=\"Appendix---Build-confusion-matrix-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Appendix - Build confusion matrix</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#create-confusion-matrix\" data-toc-modified-id=\"create-confusion-matrix-7.0.1\"><span class=\"toc-item-num\">7.0.1&nbsp;&nbsp;</span>create confusion matrix</a></span></li><li><span><a href=\"#derive-confusion-outputs\" data-toc-modified-id=\"derive-confusion-outputs-7.0.2\"><span class=\"toc-item-num\">7.0.2&nbsp;&nbsp;</span>derive confusion outputs</a></span><ul class=\"toc-item\"><li><span><a href=\"#precision---Positive-predictive-value-(PPV),-Precision\" data-toc-modified-id=\"precision---Positive-predictive-value-(PPV),-Precision-7.0.2.1\"><span class=\"toc-item-num\">7.0.2.1&nbsp;&nbsp;</span>precision - Positive predictive value (PPV), Precision</a></span></li><li><span><a href=\"#recall---True-positive-rate-(TPR),-Recall,-Sensitivity,-probability-of-detection\" data-toc-modified-id=\"recall---True-positive-rate-(TPR),-Recall,-Sensitivity,-probability-of-detection-7.0.2.2\"><span class=\"toc-item-num\">7.0.2.2&nbsp;&nbsp;</span>recall - True positive rate (TPR), Recall, Sensitivity, probability of detection</a></span></li><li><span><a href=\"#FNR---False-negative-rate-(FNR),-Miss-rate\" data-toc-modified-id=\"FNR---False-negative-rate-(FNR),-Miss-rate-7.0.2.3\"><span class=\"toc-item-num\">7.0.2.3&nbsp;&nbsp;</span>FNR - False negative rate (FNR), Miss rate</a></span></li><li><span><a href=\"#FPR---False-positive-rate-(FPR),-Fall-out\" data-toc-modified-id=\"FPR---False-positive-rate-(FPR),-Fall-out-7.0.2.4\"><span class=\"toc-item-num\">7.0.2.4&nbsp;&nbsp;</span>FPR - False positive rate (FPR), Fall-out</a></span></li><li><span><a href=\"#TNR---True-negative-rate-(TNR),-Specificity-(SPC)\" data-toc-modified-id=\"TNR---True-negative-rate-(TNR),-Specificity-(SPC)-7.0.2.5\"><span class=\"toc-item-num\">7.0.2.5&nbsp;&nbsp;</span>TNR - True negative rate (TNR), Specificity (SPC)</a></span></li><li><span><a href=\"#FOR---False-omission-rate-(FOR)\" data-toc-modified-id=\"FOR---False-omission-rate-(FOR)-7.0.2.6\"><span class=\"toc-item-num\">7.0.2.6&nbsp;&nbsp;</span>FOR - False omission rate (FOR)</a></span></li><li><span><a href=\"#LR+---Positive-likelihood-ratio-(LR+)\" data-toc-modified-id=\"LR+---Positive-likelihood-ratio-(LR+)-7.0.2.7\"><span class=\"toc-item-num\">7.0.2.7&nbsp;&nbsp;</span>LR+ - Positive likelihood ratio (LR+)</a></span></li><li><span><a href=\"#LR----Negative-likelihood-ratio-(LR-)\" data-toc-modified-id=\"LR----Negative-likelihood-ratio-(LR-)-7.0.2.8\"><span class=\"toc-item-num\">7.0.2.8&nbsp;&nbsp;</span>LR- - Negative likelihood ratio (LR-)</a></span></li><li><span><a href=\"#ACC---Accuracy-(ACC)\" data-toc-modified-id=\"ACC---Accuracy-(ACC)-7.0.2.9\"><span class=\"toc-item-num\">7.0.2.9&nbsp;&nbsp;</span>ACC - Accuracy (ACC)</a></span></li><li><span><a href=\"#FDR---False-discovery-rate-(FDR),-probability-of-false-alarm\" data-toc-modified-id=\"FDR---False-discovery-rate-(FDR),-probability-of-false-alarm-7.0.2.10\"><span class=\"toc-item-num\">7.0.2.10&nbsp;&nbsp;</span>FDR - False discovery rate (FDR), probability of false alarm</a></span></li><li><span><a href=\"#NPV---Negative-predictive-value-(NPV)\" data-toc-modified-id=\"NPV---Negative-predictive-value-(NPV)-7.0.2.11\"><span class=\"toc-item-num\">7.0.2.11&nbsp;&nbsp;</span>NPV - Negative predictive value (NPV)</a></span></li><li><span><a href=\"#DOR---Diagnostic-odds-ratio-(DOR)\" data-toc-modified-id=\"DOR---Diagnostic-odds-ratio-(DOR)-7.0.2.12\"><span class=\"toc-item-num\">7.0.2.12&nbsp;&nbsp;</span>DOR - Diagnostic odds ratio (DOR)</a></span></li><li><span><a href=\"#F1-Score\" data-toc-modified-id=\"F1-Score-7.0.2.13\"><span class=\"toc-item-num\">7.0.2.13&nbsp;&nbsp;</span>F1 Score</a></span></li><li><span><a href=\"#MCC---Matthews-correlation-coefficient-(MCC)\" data-toc-modified-id=\"MCC---Matthews-correlation-coefficient-(MCC)-7.0.2.14\"><span class=\"toc-item-num\">7.0.2.14&nbsp;&nbsp;</span>MCC - Matthews correlation coefficient (MCC)</a></span></li><li><span><a href=\"#Informedness-or-Bookmaker-Informedness-(BM)\" data-toc-modified-id=\"Informedness-or-Bookmaker-Informedness-(BM)-7.0.2.15\"><span class=\"toc-item-num\">7.0.2.15&nbsp;&nbsp;</span>Informedness or Bookmaker Informedness (BM)</a></span></li><li><span><a href=\"#Markedness-(MK)\" data-toc-modified-id=\"Markedness-(MK)-7.0.2.16\"><span class=\"toc-item-num\">7.0.2.16&nbsp;&nbsp;</span>Markedness (MK)</a></span></li></ul></li></ul></li><li><span><a href=\"#Create-unit-tests\" data-toc-modified-id=\"Create-unit-tests-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Create unit tests</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Imports\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathanmorgan/.virtualenvs/sourcenet/lib/python3.5/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages imported at 2017-10-06 22:34:28.889769\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import math\n",
    "import pandas\n",
    "import pandas_ml\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import six\n",
    "import statsmodels\n",
    "import statsmodels.api\n",
    "\n",
    "print( \"packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jonathanmorgan/work/sourcenet/django/research/work/msu_phd_work'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Initialize Django\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, initialize my dev django project, so I can run code in this notebook that references my django models and can talk to the database using my project's settings.\n",
    "\n",
    "You need to have installed your virtualenv with django as a kernel, then select that kernel for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "django initialized at 2017-10-07 02:34:31.889621\n"
     ]
    }
   ],
   "source": [
    "%run ../django_init.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import any `sourcenet` or `sourcenet_analysis` models or classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourcenet and sourcenet_analysis packages imported at 2017-10-07 02:34:33.266765\n"
     ]
    }
   ],
   "source": [
    "# python_utilities\n",
    "from python_utilities.analysis.statistics.confusion_matrix_helper import ConfusionMatrixHelper\n",
    "from python_utilities.analysis.statistics.stats_helper import StatsHelper\n",
    "from python_utilities.dictionaries.dict_helper import DictHelper\n",
    "\n",
    "# sourcenet_analysis models.\n",
    "from sourcenet_analysis.models import Reliability_Names\n",
    "\n",
    "print( \"sourcenet and sourcenet_analysis packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Tools\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Write functions here to do math, so that we can reuse said tools below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Confusion Matrix Data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "A basic confusion matrix ( [https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix) ) contains counts of true positives, true negatives, false positives, and false negatives for a given binary or boolean (yes/no) classification decision you are asking someone or something to make.\n",
    "\n",
    "To create a confusion matrix, you need two associated vectors containing classification decisions (0s and 1s), one that contains ground truth, and one that contains values predicted by whatever coder you are testing.  For each associated pair of values:\n",
    "\n",
    "- Start with the predicted value: positive (1) or negative (0).\n",
    "- Look at the corresponding ground truth value.  If they match, it is \"true\".  If not, it is \"false\".\n",
    "- So, predicted 1 and ground_truth 1 is a \"true positive\".\n",
    "- Add one to the counter for the class of prediction: \"true positive\", \"true negative\", \"false positive\", \"false negative\".\n",
    "\n",
    "Once you have your basic confusion matrix, the counts of true positives, true negatives, false positives, and false negatives can then be used to calculate a set of different scores and values one can use to assess the quality of predictive models.  These scores include \"precision and recall\", \"accuracy\", an \"F1 score\" (a harmonic mean), and a \"diagnostic odds ratio\", among many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person detection\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For each person detected across the set of articles, look at whether the automated coder correctly detected the person, independent of eventual lookup or person type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person detection - build value lists\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, build lists of ground truth and predicted values per person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2446 rows with label in ['prelim_month']\n",
      "==> population values count: 2446\n",
      "==> predicted values count: 2446\n",
      "==> percentage agreement = 0.946443172527\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "reliability_names_label = None\n",
    "label_in_list = []\n",
    "reliability_names_qs = None\n",
    "ground_truth_coder_index = 1\n",
    "predicted_coder_index = 2\n",
    "\n",
    "# processing\n",
    "column_name = \"\"\n",
    "predicted_value = -1\n",
    "predicted_list = []\n",
    "ground_truth_value = -1\n",
    "ground_truth_list = []\n",
    "reliability_names_instance = None\n",
    "\n",
    "# set label\n",
    "reliability_names_label = \"prelim_month\"\n",
    "\n",
    "# lookup Reliability_Names for selected label\n",
    "label_in_list.append( reliability_names_label )\n",
    "reliability_names_qs = Reliability_Names.objects.filter( label__in = label_in_list )\n",
    "\n",
    "print( \"Found \" + str( reliability_names_qs.count() ) + \" rows with label in \" + str( label_in_list ) )\n",
    "\n",
    "# loop over records\n",
    "predicted_value = -1\n",
    "predicted_list = []\n",
    "ground_truth_value = -1\n",
    "ground_truth_list = []\n",
    "ground_truth_positive_count = 0\n",
    "predicted_positive_count = 0\n",
    "true_positive_count = 0\n",
    "false_positive_count = 0\n",
    "ground_truth_negative_count = 0\n",
    "predicted_negative_count = 0\n",
    "true_negative_count = 0\n",
    "false_negative_count = 0\n",
    "for reliability_names_instance in reliability_names_qs:\n",
    "    \n",
    "    # get detected flag from ground truth and predicted columns and add them to list.\n",
    "    \n",
    "    # ==> ground truth\n",
    "    column_name = Reliability_Names.FIELD_NAME_PREFIX_CODER\n",
    "    column_name += str( ground_truth_coder_index )\n",
    "    column_name += \"_\" + Reliability_Names.FIELD_NAME_SUFFIX_DETECTED\n",
    "    ground_truth_value = getattr( reliability_names_instance, column_name )\n",
    "    ground_truth_list.append( ground_truth_value )\n",
    "    \n",
    "    # ==> predicted\n",
    "    column_name = Reliability_Names.FIELD_NAME_PREFIX_CODER\n",
    "    column_name += str( predicted_coder_index )\n",
    "    column_name += \"_\" + Reliability_Names.FIELD_NAME_SUFFIX_DETECTED\n",
    "    predicted_value = getattr( reliability_names_instance, column_name )\n",
    "    predicted_list.append( predicted_value )\n",
    "    \n",
    "#-- END loop over Reliability_Names instances. --#\n",
    "\n",
    "print( \"==> population values count: \" + str( len( ground_truth_list ) ) )\n",
    "print( \"==> predicted values count: \" + str( len( predicted_list ) ) )\n",
    "print( \"==> percentage agreement = \" + str( StatsHelper.percentage_agreement( ground_truth_list, predicted_list ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> population values: 2446\n",
      "ACTUAL_VALUE_LIST = [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ]\n",
      "==> predicted values count: 2446\n",
      "PREDICTED_VALUE_LIST = [ 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ]\n"
     ]
    }
   ],
   "source": [
    "print( \"==> population values: \" + str( len( ground_truth_list ) ) )\n",
    "list_name = \"ACTUAL_VALUE_LIST\"\n",
    "string_list = map( str, ground_truth_list )\n",
    "list_values = \", \".join( string_list )\n",
    "print( list_name + \" = [ \" + list_values + \" ]\" )\n",
    "\n",
    "print( \"==> predicted values count: \" + str( len( predicted_list ) ) )\n",
    "list_name = \"PREDICTED_VALUE_LIST\"\n",
    "string_list = map( str, predicted_list )\n",
    "list_values = \", \".join( string_list )\n",
    "print( list_name + \" = [ \" + list_values + \" ]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person detection - confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "Predicted   0     1  __all__\n",
      "Actual                      \n",
      "0           0    68       68\n",
      "1          63  2315     2378\n",
      "__all__    63  2383     2446\n",
      "population: 2446\n",
      "P: 2378\n",
      "N: 68\n",
      "PositiveTest: 2383\n",
      "NegativeTest: 63\n",
      "TP: 2315\n",
      "TN: 0\n",
      "FP: 68\n",
      "FN: 63\n",
      "TPR: 0.973507148865\n",
      "TNR: 0.0\n",
      "PPV: 0.971464540495\n",
      "NPV: 0.0\n",
      "FPR: 1.0\n",
      "FDR: 0.0285354595048\n",
      "FNR: 0.0264928511354\n",
      "ACC: 0.946443172527\n",
      "F1_score: 0.972484772107\n",
      "MCC: -0.0274951937753\n",
      "informedness: -0.0264928511354\n",
      "markedness: -0.0285354595048\n",
      "prevalence: 0.972199509403\n",
      "LRP: 0.973507148865\n",
      "LRN: inf\n",
      "DOR: 0.0\n",
      "FOR: 1.0\n",
      "OrderedDict([('population', 2446), ('P', 2378), ('N', 68), ('PositiveTest', 2383), ('NegativeTest', 63), ('TP', 2315), ('TN', 0), ('FP', 68), ('FN', 63), ('TPR', 0.97350714886459211), ('TNR', 0.0), ('PPV', 0.97146454049517417), ('NPV', 0.0), ('FPR', 1.0), ('FDR', 0.02853545950482585), ('FNR', 0.026492851135407905), ('ACC', 0.94644317252657395), ('F1_score', 0.97248477210670026), ('MCC', -0.027495193775309384), ('informedness', -0.026492851135407891), ('markedness', -0.028535459504825833), ('prevalence', 0.97219950940310706), ('LRP', 0.97350714886459211), ('LRN', inf), ('DOR', 0.0), ('FOR', 1.0)])\n",
      "0.973507148865\n",
      "==> Predicted positives: 2383 ( 2383 )\n",
      "==> Ground truth positives: 2378 ( 2378 )\n",
      "==> True positives: 2315\n",
      "==> False positives: 68\n",
      "==> Predicted negatives: 63 ( 63 )\n",
      "==> Ground truth negatives: 68 ( 68 )\n",
      "==> True negatives: 0\n",
      "==> False negatives: 63\n",
      "==> Precision (true positive/predicted positive): 0.971464540495\n",
      "==> Recall (true positive/ground truth positive): 0.973507148865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathanmorgan/.virtualenvs/sourcenet/lib/python3.5/site-packages/pandas_ml/confusion_matrix/bcm.py:339: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return(np.float64(self.FNR) / self.TNR)\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = pandas_ml.ConfusionMatrix( ground_truth_list, predicted_list )\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix)\n",
    "confusion_matrix.print_stats()\n",
    "stats_dict = confusion_matrix.stats()\n",
    "print( str( stats_dict ) )\n",
    "print( str( stats_dict[ 'TPR' ] ) )\n",
    "\n",
    "# get counts in variables\n",
    "true_positive_count = confusion_matrix.TP\n",
    "false_positive_count = confusion_matrix.FP\n",
    "true_negative_count = confusion_matrix.TN\n",
    "false_negative_count = confusion_matrix.FN\n",
    "\n",
    "# and derive population and predicted counts\n",
    "ground_truth_positive_count = true_positive_count + false_negative_count\n",
    "predicted_positive_count = true_positive_count + false_positive_count\n",
    "ground_truth_negative_count = true_negative_count + false_positive_count\n",
    "predicted_negative_count = true_negative_count + false_negative_count\n",
    "\n",
    "\n",
    "print( \"==> Predicted positives: \" + str( predicted_positive_count ) + \" ( \" + str( ( true_positive_count + false_positive_count ) ) + \" )\" )\n",
    "print( \"==> Ground truth positives: \" + str( ground_truth_positive_count )  + \" ( \" + str( ( true_positive_count + false_negative_count ) ) + \" )\" )\n",
    "print( \"==> True positives: \" + str( true_positive_count ) )\n",
    "print( \"==> False positives: \" + str( false_positive_count ) )\n",
    "print( \"==> Predicted negatives: \" + str( predicted_negative_count ) + \" ( \" + str( ( true_negative_count + false_negative_count ) ) + \" )\" )\n",
    "print( \"==> Ground truth negatives: \" + str( ground_truth_negative_count ) + \" ( \" + str( ( true_negative_count + false_positive_count ) ) + \" )\" )\n",
    "print( \"==> True negatives: \" + str( true_negative_count ) )\n",
    "print( \"==> False negatives: \" + str( false_negative_count ) )\n",
    "print( \"==> Precision (true positive/predicted positive): \" + str( ( true_positive_count / predicted_positive_count ) ) )\n",
    "print( \"==> Recall (true positive/ground truth positive): \" + str( ( true_positive_count / ground_truth_positive_count ) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfusionMatrixHelper --> \n",
      "DictHelper --> \n",
      "----> ACC : 0.946443172526574\n",
      "----> BM : -0.02649285113540789\n",
      "----> DOR : None\n",
      "----> FDR : 0.02853545950482585\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FOR : 1.0\n",
      "----> FPR : 1.0\n",
      "----> LR+ : 0.9735071488645921\n",
      "----> LR- : None\n",
      "----> MCC : -0.027495193775309384\n",
      "----> MK : -0.028535459504825833\n",
      "----> NPV : 0.0\n",
      "----> PPV : 0.9714645404951742\n",
      "----> SPC : 0.0\n",
      "----> TNR : 0.0\n",
      "----> TPR : 0.9735071488645921\n",
      "----> accuracy : 0.946443172526574\n",
      "----> diagnostic_odds_ratio : None\n",
      "----> f1_score : 0.9724847721067004\n",
      "----> false_discovery_rate : 0.02853545950482585\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_omission_rate : 1.0\n",
      "----> false_positive : 68\n",
      "----> false_positive_rate : 1.0\n",
      "----> informedness : -0.02649285113540789\n",
      "----> markedness : -0.028535459504825833\n",
      "----> matthews_correlation_coefficient : -0.027495193775309384\n",
      "----> negative_likelihood_ratio : None\n",
      "----> negative_predictive_value : 0.0\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> positive_likelihood_ratio : 0.9735071488645921\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> specificity : 0.0\n",
      "----> total_population : 2446\n",
      "----> true_negative : 0\n",
      "----> true_negative_rate : 0.0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "confusion_helper = ConfusionMatrixHelper.populate_confusion_matrix( ground_truth_list, predicted_list )\n",
    "print( str( confusion_helper ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person lookup\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For each person detected across the set of articles, look at whether the automated coder correctly looked up the person (so compare person IDs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person lookup - build value lists\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, build lists of ground truth and predicted values per person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2446 rows with label in ['prelim_month']\n",
      "==> population values count: 2446\n",
      "==> predicted values count: 2446\n",
      "==> percentage agreement = 0.94071954211\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "reliability_names_label = None\n",
    "label_in_list = []\n",
    "reliability_names_qs = None\n",
    "ground_truth_coder_index = 1\n",
    "predicted_coder_index = 2\n",
    "\n",
    "# processing\n",
    "column_name = \"\"\n",
    "predicted_value = -1\n",
    "predicted_list = []\n",
    "ground_truth_value = -1\n",
    "ground_truth_list = []\n",
    "reliability_names_instance = None\n",
    "\n",
    "# set label\n",
    "reliability_names_label = \"prelim_month\"\n",
    "\n",
    "# lookup Reliability_Names for selected label\n",
    "label_in_list.append( reliability_names_label )\n",
    "reliability_names_qs = Reliability_Names.objects.filter( label__in = label_in_list )\n",
    "\n",
    "print( \"Found \" + str( reliability_names_qs.count() ) + \" rows with label in \" + str( label_in_list ) )\n",
    "\n",
    "# loop over records\n",
    "predicted_value = -1\n",
    "predicted_list = []\n",
    "ground_truth_value = -1\n",
    "ground_truth_list = []\n",
    "ground_truth_positive_count = 0\n",
    "predicted_positive_count = 0\n",
    "true_positive_count = 0\n",
    "false_positive_count = 0\n",
    "ground_truth_negative_count = 0\n",
    "predicted_negative_count = 0\n",
    "true_negative_count = 0\n",
    "false_negative_count = 0\n",
    "for reliability_names_instance in reliability_names_qs:\n",
    "    \n",
    "    # get person_id from ground truth and predicted columns and add them to list.\n",
    "    \n",
    "    # ==> ground truth\n",
    "    column_name = Reliability_Names.FIELD_NAME_PREFIX_CODER\n",
    "    column_name += str( ground_truth_coder_index )\n",
    "    column_name += \"_\" + Reliability_Names.FIELD_NAME_SUFFIX_PERSON_ID\n",
    "    ground_truth_value = getattr( reliability_names_instance, column_name )\n",
    "    ground_truth_list.append( ground_truth_value )\n",
    "    \n",
    "    # ==> predicted\n",
    "    column_name = Reliability_Names.FIELD_NAME_PREFIX_CODER\n",
    "    column_name += str( predicted_coder_index )\n",
    "    column_name += \"_\" + Reliability_Names.FIELD_NAME_SUFFIX_PERSON_ID\n",
    "    predicted_value = getattr( reliability_names_instance, column_name )\n",
    "    predicted_list.append( predicted_value )\n",
    "    \n",
    "#-- END loop over Reliability_Names instances. --#\n",
    "\n",
    "print( \"==> population values count: \" + str( len( ground_truth_list ) ) )\n",
    "print( \"==> predicted values count: \" + str( len( predicted_list ) ) )\n",
    "print( \"==> percentage agreement = \" + str( StatsHelper.percentage_agreement( ground_truth_list, predicted_list ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"==> population values: \" + str( len( ground_truth_list ) ) )\n",
    "list_name = \"ACTUAL_VALUE_LIST\"\n",
    "string_list = map( str, ground_truth_list )\n",
    "list_values = \", \".join( string_list )\n",
    "print( list_name + \" = [ \" + list_values + \" ]\" )\n",
    "\n",
    "print( \"==> predicted values count: \" + str( len( predicted_list ) ) )\n",
    "list_name = \"PREDICTED_VALUE_LIST\"\n",
    "string_list = map( str, predicted_list )\n",
    "list_values = \", \".join( string_list )\n",
    "print( list_name + \" = [ \" + list_values + \" ]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person lookup - confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfusionMatrixHelper --> \n",
      "DictHelper --> \n",
      "----> ACC : 0.9407195421095667\n",
      "----> BM : -0.03238015138772077\n",
      "----> DOR : None\n",
      "----> FDR : 0.03441040704993705\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FOR : 1.0\n",
      "----> FPR : 1.2058823529411764\n",
      "----> LR+ : 0.802416459824817\n",
      "----> LR- : None\n",
      "----> MCC : -0.03028248031907182\n",
      "----> MK : -0.03441040704993703\n",
      "----> NPV : 0.0\n",
      "----> PPV : 0.965589592950063\n",
      "----> SPC : 0.0\n",
      "----> TNR : 0.0\n",
      "----> TPR : 0.9676198486122792\n",
      "----> accuracy : 0.9407195421095667\n",
      "----> diagnostic_odds_ratio : None\n",
      "----> f1_score : 0.9666036546943921\n",
      "----> false_discovery_rate : 0.03441040704993705\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_omission_rate : 1.0\n",
      "----> false_positive : 82\n",
      "----> false_positive_rate : 1.2058823529411764\n",
      "----> informedness : -0.03238015138772077\n",
      "----> markedness : -0.03441040704993703\n",
      "----> matthews_correlation_coefficient : -0.03028248031907182\n",
      "----> negative_likelihood_ratio : None\n",
      "----> negative_predictive_value : 0.0\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> positive_likelihood_ratio : 0.802416459824817\n",
      "----> precision : 0.965589592950063\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9676198486122792\n",
      "----> specificity : 0.0\n",
      "----> total_population : 2446\n",
      "----> true_negative : 0\n",
      "----> true_negative_rate : 0.0\n",
      "----> true_positive : 2301\n"
     ]
    }
   ],
   "source": [
    "confusion_helper = ConfusionMatrixHelper.populate_confusion_matrix( ground_truth_list, predicted_list )\n",
    "print( str( confusion_helper ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person Types\n",
    "\n",
    "For each person type, will build binary lists of yes or no where each person type value will in turn be the value of interest, and positive or negative is whether the coder found the current person to be of that type (positive/1) or any other type (negative/0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: build confusion lists for a given categorical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function build_confusion_lists() defined at 2017-10-07 03:42:44.881673\n"
     ]
    }
   ],
   "source": [
    "def build_confusion_lists( column_name_suffix_IN,\n",
    "                           desired_value_IN,\n",
    "                           label_list_IN = [ \"prelim_month\", ],\n",
    "                           ground_truth_coder_index_IN = 1,\n",
    "                           predicted_coder_index_IN = 2,\n",
    "                           debug_flag_IN = False ):\n",
    "\n",
    "    '''\n",
    "    Accepts suffix of column name of interest and desired value.  Also accepts optional labels\n",
    "        list, indexes of ground_truth and predicted coder users, and a debug flag.  Uses these\n",
    "        values to loop over records whose label matches the on in the list passed in.  For each,\n",
    "        in the specified column, checks to see if the ground_truth and predicted values match\n",
    "        the desired value.  If so, positive, so 1 is stored for the row.  If no, negative, so 0\n",
    "        is stored for the row.\n",
    "        \n",
    "    Returns dictionary with value lists inside, ground truth values list mapped to key\n",
    "        \"ground_truth\" and predicted values list mapped to key \"predicted\".\n",
    "    '''\n",
    "    \n",
    "    # return reference\n",
    "    lists_OUT = {}\n",
    "\n",
    "    # declare variables\n",
    "    reliability_names_label = None\n",
    "    label_in_list = []\n",
    "    reliability_names_qs = None\n",
    "    ground_truth_coder_index = -1\n",
    "    predicted_coder_index = -1\n",
    "\n",
    "    # processing\n",
    "    debug_flag = False\n",
    "    desired_column_suffix = None\n",
    "    desired_value = None\n",
    "    ground_truth_column_name = None\n",
    "    ground_truth_column_value = None\n",
    "    ground_truth_value = -1\n",
    "    ground_truth_list = []\n",
    "    predicted_column_name = None\n",
    "    predicted_column_value = None\n",
    "    predicted_value = -1\n",
    "    predicted_list = []\n",
    "    reliability_names_instance = None\n",
    "\n",
    "    # got required values?\n",
    "    \n",
    "    # column name suffix?\n",
    "    if ( column_name_suffix_IN is not None ):\n",
    "        \n",
    "        # desired value?\n",
    "        if ( desired_value_IN is not None ):\n",
    "    \n",
    "            # ==> initialize\n",
    "            desired_column_suffix = column_name_suffix_IN\n",
    "            desired_value = desired_value_IN\n",
    "            label_in_list = label_list_IN\n",
    "            ground_truth_coder_index = ground_truth_coder_index_IN\n",
    "            predicted_coder_index = predicted_coder_index_IN\n",
    "            debug_flag = debug_flag_IN\n",
    "            \n",
    "            # create ground truth column name\n",
    "            ground_truth_column_name = Reliability_Names.FIELD_NAME_PREFIX_CODER\n",
    "            ground_truth_column_name += str( ground_truth_coder_index )\n",
    "            ground_truth_column_name += \"_\" + desired_column_suffix\n",
    "\n",
    "            # create predicted column name.\n",
    "            predicted_column_name = Reliability_Names.FIELD_NAME_PREFIX_CODER\n",
    "            predicted_column_name += str( predicted_coder_index )\n",
    "            predicted_column_name += \"_\" + desired_column_suffix\n",
    "\n",
    "            # ==> processing\n",
    "            \n",
    "            # lookup Reliability_Names for selected label(s)\n",
    "            reliability_names_qs = Reliability_Names.objects.filter( label__in = label_in_list )\n",
    "\n",
    "            print( \"Found \" + str( reliability_names_qs.count() ) + \" rows with label in \" + str( label_in_list ) )\n",
    "\n",
    "            # reset all lists and values.\n",
    "            ground_truth_column_value = \"\"\n",
    "            ground_truth_value = -1\n",
    "            ground_truth_list = []\n",
    "            predicted_column_value = \"\"\n",
    "            predicted_value = -1\n",
    "            predicted_list = []\n",
    "            \n",
    "            # loop over records to build ground_truth and predicted value lists\n",
    "            #     where 1 = value matching desired value in multi-value categorical\n",
    "            #     variable and 0 = any value other than the desired value.\n",
    "            for reliability_names_instance in reliability_names_qs:\n",
    "\n",
    "                # get detected flag from ground truth and predicted columns and add them to list.\n",
    "\n",
    "                # ==> ground truth\n",
    "\n",
    "                # get column value.\n",
    "                ground_truth_column_value = getattr( reliability_names_instance, ground_truth_column_name )\n",
    "\n",
    "                # does it match desired value?\n",
    "                if ( ground_truth_column_value == desired_value ):\n",
    "\n",
    "                    # it does - True (or positive or 1!)!\n",
    "                    ground_truth_value = 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # it does not - False (or negative or 0!)!\n",
    "                    ground_truth_value = 0\n",
    "\n",
    "                #-- END check to see if current value matches desired value. --#\n",
    "\n",
    "                # add value to list.\n",
    "                ground_truth_list.append( ground_truth_value )\n",
    "\n",
    "                # ==> predicted\n",
    "\n",
    "                # get column value.\n",
    "                predicted_column_value = getattr( reliability_names_instance, predicted_column_name )\n",
    "\n",
    "                # does it match desired value?\n",
    "                if ( predicted_column_value == desired_value ):\n",
    "\n",
    "                    # it does - True (or positive or 1!)!\n",
    "                    predicted_value = 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # it does not - False (or negative or 0!)!\n",
    "                    predicted_value = 0\n",
    "\n",
    "                #-- END check to see if current value matches desired value. --#\n",
    "\n",
    "                # add to predicted list.\n",
    "                predicted_list.append( predicted_value )\n",
    "\n",
    "                if ( debug_flag == True ):        \n",
    "                    print( \"----> gt: \" + str( ground_truth_column_value ) + \" ( \" + str( ground_truth_value ) + \" ) - p: \" + str( predicted_column_value ) + \" ( \" + str( predicted_value ) + \" )\" )\n",
    "                #-- END DEBUG --#\n",
    "\n",
    "            #-- END loop over Reliability_Names instances. --#\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            print( \"ERROR - you must specify a desired value.\" )\n",
    "            \n",
    "        #-- END check to see if desired value passed in. --#\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print( \"ERROR - you must provide the suffix of the column you want to examine.\" )\n",
    "\n",
    "    #-- END check to see if column name suffix passed in. --#\n",
    "    \n",
    "    # package up and return lists.\n",
    "    lists_OUT[ \"ground_truth\" ] = ground_truth_list\n",
    "    lists_OUT[ \"predicted\" ] = predicted_list\n",
    "    \n",
    "    return lists_OUT\n",
    "    \n",
    "#-- END function build_confusion_lists() --#\n",
    "\n",
    "print( \"Function build_confusion_lists() defined at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person type - Authors\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For each person detected across the set of articles, look at whether the automated coder assigned the correct type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Person type - Authors - build value lists\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, build lists of ground truth and predicted values per person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2446 rows with label in ['prelim_month']\n",
      "==> population values count: 2446\n",
      "==> predicted values count: 2446\n",
      "==> percentage agreement = 0.999182338512\n"
     ]
    }
   ],
   "source": [
    "confusion_lists = build_confusion_lists( Reliability_Names.FIELD_NAME_SUFFIX_PERSON_TYPE,\n",
    "                                         Reliability_Names.PERSON_TYPE_AUTHOR )\n",
    "ground_truth_list = confusion_lists.get( \"ground_truth\", None )\n",
    "predicted_list = confusion_lists.get( \"predicted\", None )\n",
    "\n",
    "print( \"==> population values count: \" + str( len( ground_truth_list ) ) )\n",
    "print( \"==> predicted values count: \" + str( len( predicted_list ) ) )\n",
    "print( \"==> percentage agreement = \" + str( StatsHelper.percentage_agreement( ground_truth_list, predicted_list ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"==> population values: \" + str( len( ground_truth_list ) ) )\n",
    "list_name = \"ACTUAL_VALUE_LIST\"\n",
    "string_list = map( str, ground_truth_list )\n",
    "list_values = \", \".join( string_list )\n",
    "print( list_name + \" = [ \" + list_values + \" ]\" )\n",
    "\n",
    "print( \"==> predicted values count: \" + str( len( predicted_list ) ) )\n",
    "list_name = \"PREDICTED_VALUE_LIST\"\n",
    "string_list = map( str, predicted_list )\n",
    "list_values = \", \".join( string_list )\n",
    "print( list_name + \" = [ \" + list_values + \" ]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Person type - Authors - confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfusionMatrixHelper --> \n",
      "DictHelper --> \n",
      "----> ACC : 0.9991823385118561\n",
      "----> BM : 0.9956140350877192\n",
      "----> DOR : None\n",
      "----> FDR : 0.0\n",
      "----> FNR : 0.0043859649122807015\n",
      "----> FOR : 0.001004016064257028\n",
      "----> FPR : 0.0\n",
      "----> LR+ : None\n",
      "----> LR- : 0.0043859649122807015\n",
      "----> MCC : 0.9973035759500171\n",
      "----> MK : 0.998995983935743\n",
      "----> NPV : 0.998995983935743\n",
      "----> PPV : 1.0\n",
      "----> SPC : 1.0\n",
      "----> TNR : 1.0\n",
      "----> TPR : 0.9956140350877193\n",
      "----> accuracy : 0.9991823385118561\n",
      "----> diagnostic_odds_ratio : None\n",
      "----> f1_score : 0.9978021978021979\n",
      "----> false_discovery_rate : 0.0\n",
      "----> false_negative : 2\n",
      "----> false_negative_rate : 0.0043859649122807015\n",
      "----> false_omission_rate : 0.001004016064257028\n",
      "----> false_positive : 0\n",
      "----> false_positive_rate : 0.0\n",
      "----> informedness : 0.9956140350877192\n",
      "----> markedness : 0.998995983935743\n",
      "----> matthews_correlation_coefficient : 0.9973035759500171\n",
      "----> negative_likelihood_ratio : 0.0043859649122807015\n",
      "----> negative_predictive_value : 0.998995983935743\n",
      "----> population_negative : 1990\n",
      "----> population_positive : 456\n",
      "----> positive_likelihood_ratio : None\n",
      "----> precision : 1.0\n",
      "----> predicted_negative : 1992\n",
      "----> predicted_positive : 454\n",
      "----> recall : 0.9956140350877193\n",
      "----> specificity : 1.0\n",
      "----> total_population : 2446\n",
      "----> true_negative : 1990\n",
      "----> true_negative_rate : 1.0\n",
      "----> true_positive : 454\n"
     ]
    }
   ],
   "source": [
    "confusion_helper = ConfusionMatrixHelper.populate_confusion_matrix( ground_truth_list, predicted_list )\n",
    "print( str( confusion_helper ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person type - Subjects\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For each person detected across the set of articles classified by Ground truth as a subject, look at whether the automated coder assigned the correct person type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Person type - Subjects - build value lists\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, build lists of ground truth and predicted values per person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2446 rows with label in ['prelim_month']\n",
      "==> population values count: 2446\n",
      "==> predicted values count: 2446\n",
      "==> percentage agreement = 0.903924775143\n"
     ]
    }
   ],
   "source": [
    "# subjects = \"mentioned\"\n",
    "confusion_lists = build_confusion_lists( Reliability_Names.FIELD_NAME_SUFFIX_PERSON_TYPE,\n",
    "                                         Reliability_Names.SUBJECT_TYPE_MENTIONED )\n",
    "ground_truth_list = confusion_lists.get( \"ground_truth\", None )\n",
    "predicted_list = confusion_lists.get( \"predicted\", None )\n",
    "\n",
    "print( \"==> population values count: \" + str( len( ground_truth_list ) ) )\n",
    "print( \"==> predicted values count: \" + str( len( predicted_list ) ) )\n",
    "print( \"==> percentage agreement = \" + str( StatsHelper.percentage_agreement( ground_truth_list, predicted_list ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"==> population values: \" + str( len( ground_truth_list ) ) )\n",
    "list_name = \"ACTUAL_VALUE_LIST\"\n",
    "string_list = map( str, ground_truth_list )\n",
    "list_values = \", \".join( string_list )\n",
    "print( list_name + \" = [ \" + list_values + \" ]\" )\n",
    "\n",
    "print( \"==> predicted values count: \" + str( len( predicted_list ) ) )\n",
    "list_name = \"PREDICTED_VALUE_LIST\"\n",
    "string_list = map( str, predicted_list )\n",
    "list_values = \", \".join( string_list )\n",
    "print( list_name + \" = [ \" + list_values + \" ]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Person type - Subjects - confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfusionMatrixHelper --> \n",
      "DictHelper --> \n",
      "----> ACC : 0.9039247751430908\n",
      "----> BM : 0.7959936861580661\n",
      "----> DOR : 79.02504755865567\n",
      "----> FDR : 0.194125159642401\n",
      "----> FNR : 0.11624649859943978\n",
      "----> FOR : 0.04990980156343957\n",
      "----> FPR : 0.08775981524249422\n",
      "----> LR+ : 10.07013858174849\n",
      "----> LR- : 0.12742970605963905\n",
      "----> MCC : 0.7757212114132166\n",
      "----> MK : 0.7559650387941592\n",
      "----> NPV : 0.9500901984365604\n",
      "----> PPV : 0.8058748403575989\n",
      "----> SPC : 0.9122401847575058\n",
      "----> TNR : 0.9122401847575058\n",
      "----> TPR : 0.8837535014005602\n",
      "----> accuracy : 0.9039247751430908\n",
      "----> diagnostic_odds_ratio : 79.02504755865567\n",
      "----> f1_score : 0.8430193720774883\n",
      "----> false_discovery_rate : 0.194125159642401\n",
      "----> false_negative : 83\n",
      "----> false_negative_rate : 0.11624649859943978\n",
      "----> false_omission_rate : 0.04990980156343957\n",
      "----> false_positive : 152\n",
      "----> false_positive_rate : 0.08775981524249422\n",
      "----> informedness : 0.7959936861580661\n",
      "----> markedness : 0.7559650387941592\n",
      "----> matthews_correlation_coefficient : 0.7757212114132166\n",
      "----> negative_likelihood_ratio : 0.12742970605963905\n",
      "----> negative_predictive_value : 0.9500901984365604\n",
      "----> population_negative : 1732\n",
      "----> population_positive : 714\n",
      "----> positive_likelihood_ratio : 10.07013858174849\n",
      "----> precision : 0.8058748403575989\n",
      "----> predicted_negative : 1663\n",
      "----> predicted_positive : 783\n",
      "----> recall : 0.8837535014005602\n",
      "----> specificity : 0.9122401847575058\n",
      "----> total_population : 2446\n",
      "----> true_negative : 1580\n",
      "----> true_negative_rate : 0.9122401847575058\n",
      "----> true_positive : 631\n"
     ]
    }
   ],
   "source": [
    "confusion_helper = ConfusionMatrixHelper.populate_confusion_matrix( ground_truth_list, predicted_list )\n",
    "print( str( confusion_helper ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person type - Sources\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For each person detected across the set of articles classified by Ground truth as a source, look at whether the automated coder assigned the correct person type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Person type - Sources - build value lists\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, build lists of ground truth and predicted values per person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2446 rows with label in ['prelim_month']\n",
      "==> population values count: 2446\n",
      "==> predicted values count: 2446\n",
      "==> percentage agreement = 0.92068683565\n"
     ]
    }
   ],
   "source": [
    "# subjects = \"mentioned\"\n",
    "confusion_lists = build_confusion_lists( Reliability_Names.FIELD_NAME_SUFFIX_PERSON_TYPE,\n",
    "                                         Reliability_Names.SUBJECT_TYPE_QUOTED )\n",
    "ground_truth_list = confusion_lists.get( \"ground_truth\", None )\n",
    "predicted_list = confusion_lists.get( \"predicted\", None )\n",
    "\n",
    "print( \"==> population values count: \" + str( len( ground_truth_list ) ) )\n",
    "print( \"==> predicted values count: \" + str( len( predicted_list ) ) )\n",
    "print( \"==> percentage agreement = \" + str( StatsHelper.percentage_agreement( ground_truth_list, predicted_list ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"==> population values: \" + str( len( ground_truth_list ) ) )\n",
    "list_name = \"ACTUAL_VALUE_LIST\"\n",
    "string_list = map( str, ground_truth_list )\n",
    "list_values = \", \".join( string_list )\n",
    "print( list_name + \" = [ \" + list_values + \" ]\" )\n",
    "\n",
    "print( \"==> predicted values count: \" + str( len( predicted_list ) ) )\n",
    "list_name = \"PREDICTED_VALUE_LIST\"\n",
    "string_list = map( str, predicted_list )\n",
    "list_values = \", \".join( string_list )\n",
    "print( list_name + \" = [ \" + list_values + \" ]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Person type - Sources - confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfusionMatrixHelper --> \n",
      "DictHelper --> \n",
      "----> ACC : 0.92068683565\n",
      "----> BM : 0.840727941884\n",
      "----> DOR : 149.829545455\n",
      "----> FDR : 0.0575916230366\n",
      "----> FNR : 0.105960264901\n",
      "----> FOR : 0.0984615384615\n",
      "----> FPR : 0.0533117932149\n",
      "----> LR+ : 16.7700180614\n",
      "----> LR- : 0.111927310535\n",
      "----> MCC : 0.842335852611\n",
      "----> MK : 0.843946838502\n",
      "----> NPV : 0.901538461538\n",
      "----> PPV : 0.942408376963\n",
      "----> SPC : 0.946688206785\n",
      "----> TNR : 0.946688206785\n",
      "----> TPR : 0.894039735099\n",
      "----> accuracy : 0.92068683565\n",
      "----> diagnostic_odds_ratio : 149.829545455\n",
      "----> f1_score : 0.917587085811\n",
      "----> false_discovery_rate : 0.0575916230366\n",
      "----> false_negative : 128\n",
      "----> false_negative_rate : 0.105960264901\n",
      "----> false_omission_rate : 0.0984615384615\n",
      "----> false_positive : 66\n",
      "----> false_positive_rate : 0.0533117932149\n",
      "----> informedness : 0.840727941884\n",
      "----> markedness : 0.843946838502\n",
      "----> matthews_correlation_coefficient : 0.842335852611\n",
      "----> negative_likelihood_ratio : 0.111927310535\n",
      "----> negative_predictive_value : 0.901538461538\n",
      "----> population_negative : 1238\n",
      "----> population_positive : 1208\n",
      "----> positive_likelihood_ratio : 16.7700180614\n",
      "----> precision : 0.942408376963\n",
      "----> predicted_negative : 1300\n",
      "----> predicted_positive : 1146\n",
      "----> recall : 0.894039735099\n",
      "----> specificity : 0.946688206785\n",
      "----> total_population : 2446\n",
      "----> true_negative : 1172\n",
      "----> true_negative_rate : 0.946688206785\n",
      "----> true_positive : 1080\n"
     ]
    }
   ],
   "source": [
    "confusion_helper = ConfusionMatrixHelper.populate_confusion_matrix( ground_truth_list,\n",
    "                                                                    predicted_list,\n",
    "                                                                    calc_type_IN = ConfusionMatrixHelper.CALC_TYPE_PANDAS_ML )\n",
    "print( str( confusion_helper ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "TODO:\n",
    "\n",
    "- add field to article table for non-news or is_hard_news.\n",
    "- Article 22181 - Why is the incorrect person \"Christian Reformed Church\" tagged as being mentioned in paragraph 14 rather than 18 where that string is?\n",
    "- Want a way to limit to disagreements where quoted?  Might not - this is a start to assessing erroneous agreement.  If yes, 1 < coding time < 4 hours.\n",
    "\n",
    "    - problem - `Reliability_Names.person_type` only has three values - \"author\", \"subject\", \"source\" - might need a row-level measure of \"`has_mention`\", \"`has_quote`\" to more readily capture rows where disagreement is over quoted-or-not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - filter articles that are not news\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "TODO:\n",
    "\n",
    "- Article 22705 - Book roundup - probably should just remove from study, and see if meta-data about articles that could be used to automatically filter these type of articles out in the future.  Leaving in for now, but should flag these so I can do comparison of numbers with and without.\n",
    "- Use keywords for Lakeshore section stories to try to filter out sports stories (\"Basketball\").  Maybe try this for all articles in the month?\n",
    "- sports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Issues to debug:\n",
    "\n",
    "- TK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "DONE:\n",
    "\n",
    "- build ConfusionMatrixHelper based on logic in appendix at bottom of this page.\n",
    "- build test case for ConfusionMatrixHelper based on lists of values from this page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - Build confusion matrix\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create confusion matrix\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Use lists of population and predicted values to derive confusion matrix counts:\n",
    "\n",
    "- population positive\n",
    "- predicted positive\n",
    "- true positive\n",
    "- false positive\n",
    "- population negative\n",
    "- predicted negative\n",
    "- true negative\n",
    "- false negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Predicted positives: 2383 ( 2383 )\n",
      "==> Ground truth positives: 2378 ( 2378 )\n",
      "==> True positives: 2315\n",
      "==> False positives: 68\n",
      "==> Predicted negatives: 63 ( 63 )\n",
      "==> Ground truth negatives: 68 ( 68 )\n",
      "==> True negatives: 0\n",
      "==> False negatives: 63\n",
      "==> Precision (true positive/predicted positive): 0.9714645404951742\n",
      "==> Recall (true positive/ground truth positive): 0.9735071488645921\n"
     ]
    }
   ],
   "source": [
    "# loop over lists to derive counts\n",
    "predicted_value = -1\n",
    "ground_truth_value = -1\n",
    "ground_truth_positive_count = 0\n",
    "predicted_positive_count = 0\n",
    "true_positive_count = 0\n",
    "false_positive_count = 0\n",
    "ground_truth_negative_count = 0\n",
    "predicted_negative_count = 0\n",
    "true_negative_count = 0\n",
    "false_negative_count = 0\n",
    "list_index = -1\n",
    "for predicted_value in predicted_list:\n",
    "\n",
    "    # increment index and get associated item from ground_truth_list\n",
    "    list_index += 1\n",
    "    ground_truth_value = ground_truth_list[ list_index ]\n",
    "    \n",
    "    # add to counts\n",
    "    \n",
    "    # ==> ground truth\n",
    "    if ( ground_truth_value == 0 ):\n",
    "        \n",
    "        # ground truth negative\n",
    "        ground_truth_negative_count += 1\n",
    "        \n",
    "    # not zero - so 1 (or supports other integer values)\n",
    "    else:\n",
    "\n",
    "        # ground truth positive\n",
    "        ground_truth_positive_count += 1\n",
    "                \n",
    "    #-- END check to see if positive or negative --# \n",
    "    \n",
    "    \n",
    "    if ( predicted_value == 0 ):\n",
    "        \n",
    "        # predicted negative\n",
    "        predicted_negative_count += 1\n",
    "        \n",
    "        # equal to ground_truth?\n",
    "        if ( predicted_value == ground_truth_value ):\n",
    "            \n",
    "            # true negative\n",
    "            true_negative_count += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # false negative\n",
    "            false_negative_count += 1\n",
    "            \n",
    "        #-- END check to see if true or false --#\n",
    "        \n",
    "    # not zero - so 1 (or supports other integer values)\n",
    "    else:\n",
    "\n",
    "        # predicted positive\n",
    "        predicted_positive_count += 1\n",
    "        \n",
    "        # equal to ground_truth?\n",
    "        if ( predicted_value == ground_truth_value ):\n",
    "            \n",
    "            # true positive\n",
    "            true_positive_count += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # false positive\n",
    "            false_positive_count += 1\n",
    "            \n",
    "        #-- END check to see if true or false --#\n",
    "        \n",
    "    #-- END check to see if positive or negative --# \n",
    "\n",
    "#-- END loop over list items. --#\n",
    "    \n",
    "print( \"==> Predicted positives: \" + str( predicted_positive_count ) + \" ( \" + str( ( true_positive_count + false_positive_count ) ) + \" )\" )\n",
    "print( \"==> Ground truth positives: \" + str( ground_truth_positive_count )  + \" ( \" + str( ( true_positive_count + false_negative_count ) ) + \" )\" )\n",
    "print( \"==> True positives: \" + str( true_positive_count ) )\n",
    "print( \"==> False positives: \" + str( false_positive_count ) )\n",
    "print( \"==> Predicted negatives: \" + str( predicted_negative_count ) + \" ( \" + str( ( true_negative_count + false_negative_count ) ) + \" )\" )\n",
    "print( \"==> Ground truth negatives: \" + str( ground_truth_negative_count ) + \" ( \" + str( ( true_negative_count + false_positive_count ) ) + \" )\" )\n",
    "print( \"==> True negatives: \" + str( true_negative_count ) )\n",
    "print( \"==> False negatives: \" + str( false_negative_count ) )\n",
    "print( \"==> Precision (true positive/predicted positive): \" + str( ( true_positive_count / predicted_positive_count ) ) )\n",
    "print( \"==> Recall (true positive/ground truth positive): \" + str( ( true_positive_count / ground_truth_positive_count ) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.        ,  0.97146454]),\n",
       " array([ 0.        ,  0.97350715]),\n",
       " array([ 0.        ,  0.97248477]),\n",
       " array([  68, 2378]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try scikit-learn: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
    "sklearn.metrics.precision_recall_fscore_support( ground_truth_list, predicted_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn confusion matrix\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "conf_matrix = sklearn.metrics.confusion_matrix( ground_truth_list, predicted_list )\n",
    "print( str( conf_matrix ) )\n",
    "\n",
    "# get counts in variables\n",
    "true_positive_count = conf_matrix[ 1 ][ 1 ]\n",
    "false_positive_count = conf_matrix[ 0 ][ 1 ]\n",
    "true_negative_count = conf_matrix[ 0 ][ 0 ]\n",
    "false_negative_count = conf_matrix[ 1 ][ 0 ]\n",
    "\n",
    "# and derive population and predicted counts\n",
    "ground_truth_positive_count = true_positive_count + false_negative_count\n",
    "predicted_positive_count = true_positive_count + false_positive_count\n",
    "ground_truth_negative_count = true_negative_count + false_positive_count\n",
    "predicted_negative_count = true_negative_count + false_negative_count\n",
    "\n",
    "\n",
    "print( \"==> Predicted positives: \" + str( predicted_positive_count ) + \" ( \" + str( ( true_positive_count + false_positive_count ) ) + \" )\" )\n",
    "print( \"==> Ground truth positives: \" + str( ground_truth_positive_count )  + \" ( \" + str( ( true_positive_count + false_negative_count ) ) + \" )\" )\n",
    "print( \"==> True positives: \" + str( true_positive_count ) )\n",
    "print( \"==> False positives: \" + str( false_positive_count ) )\n",
    "print( \"==> Predicted negatives: \" + str( predicted_negative_count ) + \" ( \" + str( ( true_negative_count + false_negative_count ) ) + \" )\" )\n",
    "print( \"==> Ground truth negatives: \" + str( ground_truth_negative_count ) + \" ( \" + str( ( true_negative_count + false_positive_count ) ) + \" )\" )\n",
    "print( \"==> True negatives: \" + str( true_negative_count ) )\n",
    "print( \"==> False negatives: \" + str( false_negative_count ) )\n",
    "print( \"==> Precision (true positive/predicted positive): \" + str( ( true_positive_count / predicted_positive_count ) ) )\n",
    "print( \"==> Recall (true positive/ground truth positive): \" + str( ( true_positive_count / ground_truth_positive_count ) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "# https://stackoverflow.com/questions/2148543/how-to-write-a-confusion-matrix-in-python\n",
    "y_actu = pandas.Series( ground_truth_list, name='Actual')\n",
    "y_pred = pandas.Series( predicted_list, name='Predicted')\n",
    "df_confusion = pandas.crosstab(y_actu, y_pred)\n",
    "print( str( df_confusion ) )\n",
    "\n",
    "# get counts in variables\n",
    "true_positive_count = df_confusion[ 1 ][ 1 ]\n",
    "false_positive_count = df_confusion[ 1 ][ 0 ]\n",
    "true_negative_count = df_confusion[ 0 ][ 0 ]\n",
    "false_negative_count = df_confusion[ 0 ][ 1 ]\n",
    "\n",
    "# and derive population and predicted counts\n",
    "ground_truth_positive_count = true_positive_count + false_negative_count\n",
    "predicted_positive_count = true_positive_count + false_positive_count\n",
    "ground_truth_negative_count = true_negative_count + false_positive_count\n",
    "predicted_negative_count = true_negative_count + false_negative_count\n",
    "\n",
    "print( \"==> Predicted positives: \" + str( predicted_positive_count ) + \" ( \" + str( ( true_positive_count + false_positive_count ) ) + \" )\" )\n",
    "print( \"==> Ground truth positives: \" + str( ground_truth_positive_count )  + \" ( \" + str( ( true_positive_count + false_negative_count ) ) + \" )\" )\n",
    "print( \"==> True positives: \" + str( true_positive_count ) )\n",
    "print( \"==> False positives: \" + str( false_positive_count ) )\n",
    "print( \"==> Predicted negatives: \" + str( predicted_negative_count ) + \" ( \" + str( ( true_negative_count + false_negative_count ) ) + \" )\" )\n",
    "print( \"==> Ground truth negatives: \" + str( ground_truth_negative_count ) + \" ( \" + str( ( true_negative_count + false_positive_count ) ) + \" )\" )\n",
    "print( \"==> True negatives: \" + str( true_negative_count ) )\n",
    "print( \"==> False negatives: \" + str( false_negative_count ) )\n",
    "print( \"==> Precision (true positive/predicted positive): \" + str( ( true_positive_count / predicted_positive_count ) ) )\n",
    "print( \"==> Recall (true positive/ground truth positive): \" + str( ( true_positive_count / ground_truth_positive_count ) ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More to look into:\n",
    "\n",
    "- https://stackoverflow.com/questions/39626401/how-to-get-odds-ratios-and-other-related-features-with-scikit-learn#39711837"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### derive confusion outputs\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Use confusion matrix to derive related metrics:\n",
    "\n",
    "- Positive predictive value (PPV), **_Precision_**\n",
    "- True positive rate (TPR), **_Recall_**, Sensitivity, probability of detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Confusion outputs:\n",
      "----> false_negative : 63\n",
      "----> false_positive : 68\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> true_negative : 0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# build up confusion outputs from confusion matrix.\n",
    "\n",
    "# assume we have the following set one way or another above.\n",
    "#ground_truth_positive_count = 0\n",
    "#predicted_positive_count = 0\n",
    "#ground_truth_negative_count = 0\n",
    "#predicted_negative_count = 0\n",
    "#true_positive_count = 0\n",
    "#false_positive_count = 0\n",
    "#true_negative_count = 0\n",
    "#false_negative_count = 0\n",
    "\n",
    "# add base measures to confusion_outputs\n",
    "confusion_outputs[ \"population_positive\" ] = ground_truth_positive_count\n",
    "confusion_outputs[ \"predicted_positive\" ] = predicted_positive_count\n",
    "confusion_outputs[ \"population_negative\" ] = ground_truth_negative_count\n",
    "confusion_outputs[ \"predicted_negative\" ] = predicted_negative_count\n",
    "confusion_outputs[ \"true_positive\" ] = true_positive_count\n",
    "confusion_outputs[ \"false_positive\" ] = false_positive_count\n",
    "confusion_outputs[ \"true_negative\" ] = true_negative_count\n",
    "confusion_outputs[ \"false_negative\" ] = false_negative_count\n",
    "\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### precision - Positive predictive value (PPV), Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Positive predictive value (PPV), Precision = 0.9714645404951742\n",
      "==> Confusion outputs:\n",
      "----> PPV : 0.9714645404951742\n",
      "----> false_negative : 63\n",
      "----> false_positive : 68\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> true_negative : 0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "precision = None\n",
    "\n",
    "# ==> Positive predictive value (PPV), Precision\n",
    "try:\n",
    "    \n",
    "    precision = ( true_positive_count / predicted_positive_count )\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    precision = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"precision\" ] = precision\n",
    "confusion_outputs[ \"PPV\" ] = precision\n",
    "\n",
    "print( \"==> Positive predictive value (PPV), Precision = \" + str( precision ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recall - True positive rate (TPR), Recall, Sensitivity, probability of detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> True positive rate (TPR), Recall = 0.9735071488645921\n",
      "==> Confusion outputs:\n",
      "----> PPV : 0.9714645404951742\n",
      "----> TPR : 0.9735071488645921\n",
      "----> false_negative : 63\n",
      "----> false_positive : 68\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> true_negative : 0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "recall = None\n",
    "\n",
    "# ==> True positive rate (TPR), Recall, Sensitivity, probability of detection\n",
    "try:\n",
    "    \n",
    "    recall = ( true_positive_count / ground_truth_positive_count )\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    recall = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"recall\" ] = recall\n",
    "confusion_outputs[ \"TPR\" ] = recall\n",
    "\n",
    "print( \"==> True positive rate (TPR), Recall = \" + str( recall ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FNR - False negative rate (FNR), Miss rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> false negative rate (FNR) = 0.026492851135407905\n",
      "==> Confusion outputs:\n",
      "----> FNR : 0.026492851135407905\n",
      "----> PPV : 0.9714645404951742\n",
      "----> TPR : 0.9735071488645921\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_positive : 68\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> true_negative : 0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "false_negative_rate = None\n",
    "\n",
    "# ==> False negative rate (FNR), Miss rate\n",
    "try:\n",
    "    \n",
    "    false_negative_rate = ( false_negative_count / ground_truth_positive_count )\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    false_negative_rate = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"false_negative_rate\" ] = false_negative_rate\n",
    "confusion_outputs[ \"FNR\" ] = false_negative_rate\n",
    "\n",
    "print( \"==> false negative rate (FNR) = \" + str( false_negative_rate ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FPR - False positive rate (FPR), Fall-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> False positive rate (FPR), Fall-out = 1.0\n",
      "==> Confusion outputs:\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FPR : 1.0\n",
      "----> PPV : 0.9714645404951742\n",
      "----> TPR : 0.9735071488645921\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_positive : 68\n",
      "----> false_positive_rate : 1.0\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> true_negative : 0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "false_positive_rate = None\n",
    "\n",
    "# ==> False positive rate (FPR), Fall-out\n",
    "try:\n",
    "    \n",
    "    false_positive_rate = ( false_positive_count / ground_truth_negative_count )\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    false_positive_rate = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"false_positive_rate\" ] = false_positive_rate\n",
    "confusion_outputs[ \"FPR\" ] = false_positive_rate\n",
    "\n",
    "print( \"==> False positive rate (FPR), Fall-out = \" + str( false_positive_rate ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TNR - True negative rate (TNR), Specificity (SPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> True negative rate (TNR), Specificity (SPC) = 0.0\n",
      "==> Confusion outputs:\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FPR : 1.0\n",
      "----> PPV : 0.9714645404951742\n",
      "----> SPC : 0.0\n",
      "----> TNR : 0.0\n",
      "----> TPR : 0.9735071488645921\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_positive : 68\n",
      "----> false_positive_rate : 1.0\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> specificity : 0.0\n",
      "----> true_negative : 0\n",
      "----> true_negative_rate : 0.0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "true_negative_rate = None\n",
    "\n",
    "# ==> True negative rate (TNR), Specificity (SPC)\n",
    "try:\n",
    "    \n",
    "    true_negative_rate = ( true_negative_count / ground_truth_negative_count )\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    true_negative_rate = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"true_negative_rate\" ] = true_negative_rate\n",
    "confusion_outputs[ \"TNR\" ] = true_negative_rate\n",
    "confusion_outputs[ \"specificity\" ] = true_negative_rate\n",
    "confusion_outputs[ \"SPC\" ] = true_negative_rate\n",
    "\n",
    "print( \"==> True negative rate (TNR), Specificity (SPC) = \" + str( true_negative_rate ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FOR - False omission rate (FOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> False omission rate (FOR) = 1.0\n",
      "==> Confusion outputs:\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FOR : 1.0\n",
      "----> FPR : 1.0\n",
      "----> PPV : 0.9714645404951742\n",
      "----> SPC : 0.0\n",
      "----> TNR : 0.0\n",
      "----> TPR : 0.9735071488645921\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_omission_rate : 1.0\n",
      "----> false_positive : 68\n",
      "----> false_positive_rate : 1.0\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> specificity : 0.0\n",
      "----> true_negative : 0\n",
      "----> true_negative_rate : 0.0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "false_omission_rate = None\n",
    "\n",
    "# ==> False omission rate (FOR) = Σ False negative/Σ Predicted condition negative\n",
    "try:\n",
    "    \n",
    "    false_omission_rate = ( false_negative_count / predicted_negative_count )\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    false_omission_rate = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"false_omission_rate\" ] = false_omission_rate\n",
    "confusion_outputs[ \"FOR\" ] = false_omission_rate\n",
    "\n",
    "print( \"==> False omission rate (FOR) = \" + str( false_omission_rate ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR+ - Positive likelihood ratio (LR+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Positive likelihood ratio (LR+) = 0.9735071488645921\n",
      "==> Confusion outputs:\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FOR : 1.0\n",
      "----> FPR : 1.0\n",
      "----> LR+ : 0.9735071488645921\n",
      "----> PPV : 0.9714645404951742\n",
      "----> SPC : 0.0\n",
      "----> TNR : 0.0\n",
      "----> TPR : 0.9735071488645921\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_omission_rate : 1.0\n",
      "----> false_positive : 68\n",
      "----> false_positive_rate : 1.0\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> positive_likelihood_ratio : 0.9735071488645921\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> specificity : 0.0\n",
      "----> true_negative : 0\n",
      "----> true_negative_rate : 0.0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "positive_likelihood_ratio = None\n",
    "tpr = None\n",
    "fpr = None\n",
    "\n",
    "# ==> Positive likelihood ratio (LR+) = TPR/FPR\n",
    "tpr = confusion_outputs.get( \"TPR\", None )\n",
    "fpr = confusion_outputs.get( \"FPR\", None )\n",
    "\n",
    "try:\n",
    "    \n",
    "    positive_likelihood_ratio = ( tpr / fpr )\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    positive_likelihood_ratio = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"positive_likelihood_ratio\" ] = positive_likelihood_ratio\n",
    "confusion_outputs[ \"LR+\" ] = positive_likelihood_ratio\n",
    "\n",
    "print( \"==> Positive likelihood ratio (LR+) = \" + str( positive_likelihood_ratio ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR- - Negative likelihood ratio (LR-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Negative likelihood ratio (LR-) = None\n",
      "==> Confusion outputs:\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FOR : 1.0\n",
      "----> FPR : 1.0\n",
      "----> LR+ : 0.9735071488645921\n",
      "----> LR- : None\n",
      "----> PPV : 0.9714645404951742\n",
      "----> SPC : 0.0\n",
      "----> TNR : 0.0\n",
      "----> TPR : 0.9735071488645921\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_omission_rate : 1.0\n",
      "----> false_positive : 68\n",
      "----> false_positive_rate : 1.0\n",
      "----> negative_likelihood_ratio : None\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> positive_likelihood_ratio : 0.9735071488645921\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> specificity : 0.0\n",
      "----> true_negative : 0\n",
      "----> true_negative_rate : 0.0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "negative_likelihood_ratio = None\n",
    "fnr = None\n",
    "tnr = None\n",
    "\n",
    "# ==> Negative likelihood ratio (LR-) = FNR/TNR\n",
    "fnr = confusion_outputs.get( \"FNR\", None )\n",
    "tnr = confusion_outputs.get( \"TNR\", None )\n",
    "\n",
    "try:\n",
    "    \n",
    "    negative_likelihood_ratio = ( fnr / tnr )\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    negative_likelihood_ratio = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"negative_likelihood_ratio\" ] = negative_likelihood_ratio\n",
    "confusion_outputs[ \"LR-\" ] = negative_likelihood_ratio\n",
    "\n",
    "print( \"==> Negative likelihood ratio (LR-) = \" + str( negative_likelihood_ratio ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACC - Accuracy (ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Accuracy (ACC) = 0.946443172526574\n",
      "==> Confusion outputs:\n",
      "----> ACC : 0.946443172526574\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FOR : 1.0\n",
      "----> FPR : 1.0\n",
      "----> LR+ : 0.9735071488645921\n",
      "----> LR- : None\n",
      "----> PPV : 0.9714645404951742\n",
      "----> SPC : 0.0\n",
      "----> TNR : 0.0\n",
      "----> TPR : 0.9735071488645921\n",
      "----> accuracy : 0.946443172526574\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_omission_rate : 1.0\n",
      "----> false_positive : 68\n",
      "----> false_positive_rate : 1.0\n",
      "----> negative_likelihood_ratio : None\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> positive_likelihood_ratio : 0.9735071488645921\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> specificity : 0.0\n",
      "----> total_population : 2446\n",
      "----> true_negative : 0\n",
      "----> true_negative_rate : 0.0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "accuracy = None\n",
    "total_population = None\n",
    "\n",
    "# ==> Accuracy (ACC) = Σ True positive + Σ True negative/Σ Total population\n",
    "total_population = true_positive_count + true_negative_count + false_positive_count + false_negative_count\n",
    "\n",
    "try:\n",
    "    \n",
    "    accuracy = ( ( true_positive_count + true_negative_count ) / total_population )\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    accuracy = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"accuracy\" ] = accuracy\n",
    "confusion_outputs[ \"ACC\" ] = accuracy\n",
    "confusion_outputs[ \"total_population\" ] = total_population\n",
    "\n",
    "print( \"==> Accuracy (ACC) = \" + str( accuracy ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FDR - False discovery rate (FDR), probability of false alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> False discovery rate (FDR), probability of false alarm = 0.02853545950482585\n",
      "==> Confusion outputs:\n",
      "----> ACC : 0.946443172526574\n",
      "----> FDR : 0.02853545950482585\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FOR : 1.0\n",
      "----> FPR : 1.0\n",
      "----> LR+ : 0.9735071488645921\n",
      "----> LR- : None\n",
      "----> PPV : 0.9714645404951742\n",
      "----> SPC : 0.0\n",
      "----> TNR : 0.0\n",
      "----> TPR : 0.9735071488645921\n",
      "----> accuracy : 0.946443172526574\n",
      "----> false_discovery_rate : 0.02853545950482585\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_omission_rate : 1.0\n",
      "----> false_positive : 68\n",
      "----> false_positive_rate : 1.0\n",
      "----> negative_likelihood_ratio : None\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> positive_likelihood_ratio : 0.9735071488645921\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> specificity : 0.0\n",
      "----> total_population : 2446\n",
      "----> true_negative : 0\n",
      "----> true_negative_rate : 0.0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "false_discovery_rate = None\n",
    "\n",
    "# ==> False discovery rate (FDR), probability of false alarm = Σ False positive/Σ Predicted condition positive\n",
    "try:\n",
    "    \n",
    "    false_discovery_rate = ( false_positive_count / predicted_positive_count )\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    false_discovery_rate = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"false_discovery_rate\" ] = false_discovery_rate\n",
    "confusion_outputs[ \"FDR\" ] = false_discovery_rate\n",
    "\n",
    "print( \"==> False discovery rate (FDR), probability of false alarm = \" + str( false_discovery_rate ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NPV - Negative predictive value (NPV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Negative predictive value (NPV) = 0.0\n",
      "==> Confusion outputs:\n",
      "----> ACC : 0.946443172526574\n",
      "----> FDR : 0.02853545950482585\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FOR : 1.0\n",
      "----> FPR : 1.0\n",
      "----> LR+ : 0.9735071488645921\n",
      "----> LR- : None\n",
      "----> NPV : 0.0\n",
      "----> PPV : 0.9714645404951742\n",
      "----> SPC : 0.0\n",
      "----> TNR : 0.0\n",
      "----> TPR : 0.9735071488645921\n",
      "----> accuracy : 0.946443172526574\n",
      "----> false_discovery_rate : 0.02853545950482585\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_omission_rate : 1.0\n",
      "----> false_positive : 68\n",
      "----> false_positive_rate : 1.0\n",
      "----> negative_likelihood_ratio : None\n",
      "----> negative_predictive_value : 0.0\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> positive_likelihood_ratio : 0.9735071488645921\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> specificity : 0.0\n",
      "----> total_population : 2446\n",
      "----> true_negative : 0\n",
      "----> true_negative_rate : 0.0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "negative_predictive_value = None\n",
    "\n",
    "# ==> Negative predictive value (NPV) = Σ True negative/Σ Predicted condition negative\n",
    "try:\n",
    "    \n",
    "    negative_predictive_value = ( true_negative_count / predicted_negative_count )\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    negative_predictive_value = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"negative_predictive_value\" ] = negative_predictive_value\n",
    "confusion_outputs[ \"NPV\" ] = negative_predictive_value\n",
    "\n",
    "print( \"==> Negative predictive value (NPV) = \" + str( negative_predictive_value ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOR - Diagnostic odds ratio (DOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Diagnostic odds ratio (DOR) = None\n",
      "==> Confusion outputs:\n",
      "----> ACC : 0.946443172526574\n",
      "----> DOR : None\n",
      "----> FDR : 0.02853545950482585\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FOR : 1.0\n",
      "----> FPR : 1.0\n",
      "----> LR+ : 0.9735071488645921\n",
      "----> LR- : None\n",
      "----> NPV : 0.0\n",
      "----> PPV : 0.9714645404951742\n",
      "----> SPC : 0.0\n",
      "----> TNR : 0.0\n",
      "----> TPR : 0.9735071488645921\n",
      "----> accuracy : 0.946443172526574\n",
      "----> diagnostic_odds_ratio : None\n",
      "----> false_discovery_rate : 0.02853545950482585\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_omission_rate : 1.0\n",
      "----> false_positive : 68\n",
      "----> false_positive_rate : 1.0\n",
      "----> negative_likelihood_ratio : None\n",
      "----> negative_predictive_value : 0.0\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> positive_likelihood_ratio : 0.9735071488645921\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> specificity : 0.0\n",
      "----> total_population : 2446\n",
      "----> true_negative : 0\n",
      "----> true_negative_rate : 0.0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "diagnostic_odds_ratio = None\n",
    "lr_plus = None\n",
    "lr_minus = None\n",
    "\n",
    "# ==> Diagnostic odds ratio (DOR) = LR+/LR−\n",
    "lr_plus = confusion_outputs.get( \"LR+\", None )\n",
    "lr_minus = confusion_outputs.get( \"LR-\", None )\n",
    "\n",
    "try:\n",
    "    \n",
    "    diagnostic_odds_ratio = ( lr_plus / lr_minus )\n",
    "    \n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    diagnostic_odds_ratio = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "    \n",
    "confusion_outputs[ \"diagnostic_odds_ratio\" ] = diagnostic_odds_ratio\n",
    "confusion_outputs[ \"DOR\" ] = diagnostic_odds_ratio\n",
    "\n",
    "print( \"==> Diagnostic odds ratio (DOR) = \" + str( diagnostic_odds_ratio ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> F1 score = 0.9724847721067004\n",
      "==> Confusion outputs:\n",
      "----> ACC : 0.946443172526574\n",
      "----> DOR : None\n",
      "----> FDR : 0.02853545950482585\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FOR : 1.0\n",
      "----> FPR : 1.0\n",
      "----> LR+ : 0.9735071488645921\n",
      "----> LR- : None\n",
      "----> NPV : 0.0\n",
      "----> PPV : 0.9714645404951742\n",
      "----> SPC : 0.0\n",
      "----> TNR : 0.0\n",
      "----> TPR : 0.9735071488645921\n",
      "----> accuracy : 0.946443172526574\n",
      "----> diagnostic_odds_ratio : None\n",
      "----> f1_score : 0.9724847721067004\n",
      "----> false_discovery_rate : 0.02853545950482585\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_omission_rate : 1.0\n",
      "----> false_positive : 68\n",
      "----> false_positive_rate : 1.0\n",
      "----> negative_likelihood_ratio : None\n",
      "----> negative_predictive_value : 0.0\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> positive_likelihood_ratio : 0.9735071488645921\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> specificity : 0.0\n",
      "----> total_population : 2446\n",
      "----> true_negative : 0\n",
      "----> true_negative_rate : 0.0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "f1_score = None\n",
    "recall = None\n",
    "precision = None\n",
    "\n",
    "# ==> F1 score = 2 / ( ( 1 / Recall ) + ( 1 / Precision ) )\n",
    "recall = confusion_outputs.get( \"recall\", None )\n",
    "precision = confusion_outputs.get( \"precision\", None )\n",
    "try:\n",
    "    \n",
    "    f1_score = ( 2 / ( ( 1 / recall ) + ( 1 / precision ) ) )\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    f1_score = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"f1_score\" ] = f1_score\n",
    "\n",
    "print( \"==> F1 score = \" + str( f1_score ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCC - Matthews correlation coefficient (MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Matthews correlation coefficient (MCC) = -0.027495193775309384\n",
      "==> Confusion outputs:\n",
      "----> ACC : 0.946443172526574\n",
      "----> DOR : None\n",
      "----> FDR : 0.02853545950482585\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FOR : 1.0\n",
      "----> FPR : 1.0\n",
      "----> LR+ : 0.9735071488645921\n",
      "----> LR- : None\n",
      "----> MCC : -0.027495193775309384\n",
      "----> NPV : 0.0\n",
      "----> PPV : 0.9714645404951742\n",
      "----> SPC : 0.0\n",
      "----> TNR : 0.0\n",
      "----> TPR : 0.9735071488645921\n",
      "----> accuracy : 0.946443172526574\n",
      "----> diagnostic_odds_ratio : None\n",
      "----> f1_score : 0.9724847721067004\n",
      "----> false_discovery_rate : 0.02853545950482585\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_omission_rate : 1.0\n",
      "----> false_positive : 68\n",
      "----> false_positive_rate : 1.0\n",
      "----> matthews_correlation_coefficient : -0.027495193775309384\n",
      "----> negative_likelihood_ratio : None\n",
      "----> negative_predictive_value : 0.0\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> positive_likelihood_ratio : 0.9735071488645921\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> specificity : 0.0\n",
      "----> total_population : 2446\n",
      "----> true_negative : 0\n",
      "----> true_negative_rate : 0.0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "matthews_correlation_coefficient = None\n",
    "numerator = None\n",
    "temp_math = None\n",
    "denominator = None\n",
    "\n",
    "# ==> Matthews correlation coefficient (MCC) = ( ( T P × T N ) − ( F P × F N ) ) / sqrt( ( T P + F P ) * ( T P + F N ) * ( T N + F P ) * ( T N + F N ) )\n",
    "numerator = ( ( true_positive_count * true_negative_count ) - ( false_positive_count * false_negative_count ) )\n",
    "temp_math = ( ( true_positive_count + false_positive_count ) * ( true_positive_count + false_negative_count ) * ( true_negative_count + false_positive_count ) * ( true_negative_count + false_negative_count ) )\n",
    "denominator = math.sqrt( temp_math )\n",
    "\n",
    "try:\n",
    "    \n",
    "    matthews_correlation_coefficient = numerator / denominator\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    matthews_correlation_coefficient = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"matthews_correlation_coefficient\" ] = matthews_correlation_coefficient\n",
    "confusion_outputs[ \"MCC\" ] = matthews_correlation_coefficient\n",
    "\n",
    "print( \"==> Matthews correlation coefficient (MCC) = \" + str( matthews_correlation_coefficient ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Informedness or Bookmaker Informedness (BM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Informedness or Bookmaker Informedness (BM) = -0.02649285113540789\n",
      "==> Confusion outputs:\n",
      "----> ACC : 0.946443172526574\n",
      "----> BM : -0.02649285113540789\n",
      "----> DOR : None\n",
      "----> FDR : 0.02853545950482585\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FOR : 1.0\n",
      "----> FPR : 1.0\n",
      "----> LR+ : 0.9735071488645921\n",
      "----> LR- : None\n",
      "----> MCC : -0.027495193775309384\n",
      "----> NPV : 0.0\n",
      "----> PPV : 0.9714645404951742\n",
      "----> SPC : 0.0\n",
      "----> TNR : 0.0\n",
      "----> TPR : 0.9735071488645921\n",
      "----> accuracy : 0.946443172526574\n",
      "----> diagnostic_odds_ratio : None\n",
      "----> f1_score : 0.9724847721067004\n",
      "----> false_discovery_rate : 0.02853545950482585\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_omission_rate : 1.0\n",
      "----> false_positive : 68\n",
      "----> false_positive_rate : 1.0\n",
      "----> informedness : -0.02649285113540789\n",
      "----> matthews_correlation_coefficient : -0.027495193775309384\n",
      "----> negative_likelihood_ratio : None\n",
      "----> negative_predictive_value : 0.0\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> positive_likelihood_ratio : 0.9735071488645921\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> specificity : 0.0\n",
      "----> total_population : 2446\n",
      "----> true_negative : 0\n",
      "----> true_negative_rate : 0.0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "informedness = None\n",
    "tpr = None\n",
    "tnr = None\n",
    "\n",
    "# ==> Informedness or Bookmaker Informedness (BM) = TPR + TNR − 1\n",
    "tpr = confusion_outputs.get( \"TPR\", None )\n",
    "tnr = confusion_outputs.get( \"TNR\", None )\n",
    "\n",
    "try:\n",
    "    \n",
    "    informedness = tpr + tnr - 1\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    informedness = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"informedness\" ] = informedness\n",
    "confusion_outputs[ \"BM\" ] = informedness\n",
    "\n",
    "print( \"==> Informedness or Bookmaker Informedness (BM) = \" + str( informedness ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markedness (MK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Markedness (MK) = -0.028535459504825833\n",
      "==> Confusion outputs:\n",
      "EXPECTED_OUTPUT_MAP[ \"ACC\" ] = 0.946443172526574\n",
      "EXPECTED_OUTPUT_MAP[ \"BM\" ] = -0.02649285113540789\n",
      "EXPECTED_OUTPUT_MAP[ \"DOR\" ] = None\n",
      "EXPECTED_OUTPUT_MAP[ \"FDR\" ] = 0.02853545950482585\n",
      "EXPECTED_OUTPUT_MAP[ \"FNR\" ] = 0.026492851135407905\n",
      "EXPECTED_OUTPUT_MAP[ \"FOR\" ] = 1.0\n",
      "EXPECTED_OUTPUT_MAP[ \"FPR\" ] = 1.0\n",
      "EXPECTED_OUTPUT_MAP[ \"LR+\" ] = 0.9735071488645921\n",
      "EXPECTED_OUTPUT_MAP[ \"LR-\" ] = None\n",
      "EXPECTED_OUTPUT_MAP[ \"MCC\" ] = -0.027495193775309384\n",
      "EXPECTED_OUTPUT_MAP[ \"MK\" ] = -0.028535459504825833\n",
      "EXPECTED_OUTPUT_MAP[ \"NPV\" ] = 0.0\n",
      "EXPECTED_OUTPUT_MAP[ \"PPV\" ] = 0.9714645404951742\n",
      "EXPECTED_OUTPUT_MAP[ \"SPC\" ] = 0.0\n",
      "EXPECTED_OUTPUT_MAP[ \"TNR\" ] = 0.0\n",
      "EXPECTED_OUTPUT_MAP[ \"TPR\" ] = 0.9735071488645921\n",
      "EXPECTED_OUTPUT_MAP[ \"accuracy\" ] = 0.946443172526574\n",
      "EXPECTED_OUTPUT_MAP[ \"diagnostic_odds_ratio\" ] = None\n",
      "EXPECTED_OUTPUT_MAP[ \"f1_score\" ] = 0.9724847721067004\n",
      "EXPECTED_OUTPUT_MAP[ \"false_discovery_rate\" ] = 0.02853545950482585\n",
      "EXPECTED_OUTPUT_MAP[ \"false_negative\" ] = 63\n",
      "EXPECTED_OUTPUT_MAP[ \"false_negative_rate\" ] = 0.026492851135407905\n",
      "EXPECTED_OUTPUT_MAP[ \"false_omission_rate\" ] = 1.0\n",
      "EXPECTED_OUTPUT_MAP[ \"false_positive\" ] = 68\n",
      "EXPECTED_OUTPUT_MAP[ \"false_positive_rate\" ] = 1.0\n",
      "EXPECTED_OUTPUT_MAP[ \"informedness\" ] = -0.02649285113540789\n",
      "EXPECTED_OUTPUT_MAP[ \"markedness\" ] = -0.028535459504825833\n",
      "EXPECTED_OUTPUT_MAP[ \"matthews_correlation_coefficient\" ] = -0.027495193775309384\n",
      "EXPECTED_OUTPUT_MAP[ \"negative_likelihood_ratio\" ] = None\n",
      "EXPECTED_OUTPUT_MAP[ \"negative_predictive_value\" ] = 0.0\n",
      "EXPECTED_OUTPUT_MAP[ \"population_negative\" ] = 68\n",
      "EXPECTED_OUTPUT_MAP[ \"population_positive\" ] = 2378\n",
      "EXPECTED_OUTPUT_MAP[ \"positive_likelihood_ratio\" ] = 0.9735071488645921\n",
      "EXPECTED_OUTPUT_MAP[ \"precision\" ] = 0.9714645404951742\n",
      "EXPECTED_OUTPUT_MAP[ \"predicted_negative\" ] = 63\n",
      "EXPECTED_OUTPUT_MAP[ \"predicted_positive\" ] = 2383\n",
      "EXPECTED_OUTPUT_MAP[ \"recall\" ] = 0.9735071488645921\n",
      "EXPECTED_OUTPUT_MAP[ \"specificity\" ] = 0.0\n",
      "EXPECTED_OUTPUT_MAP[ \"total_population\" ] = 2446\n",
      "EXPECTED_OUTPUT_MAP[ \"true_negative\" ] = 0\n",
      "EXPECTED_OUTPUT_MAP[ \"true_negative_rate\" ] = 0.0\n",
      "EXPECTED_OUTPUT_MAP[ \"true_positive\" ] = 2315\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "markedness = None\n",
    "ppv = None\n",
    "npv = None\n",
    "\n",
    "# ==> Markedness (MK) = PPV + NPV − 1 \n",
    "ppv = confusion_outputs.get( \"PPV\", None )\n",
    "npv = confusion_outputs.get( \"NPV\", None )\n",
    "\n",
    "try:\n",
    "    \n",
    "    markedness = ppv + npv - 1\n",
    "\n",
    "except:\n",
    "    \n",
    "    # error - None\n",
    "    markedness = None\n",
    "    \n",
    "#-- END check to see if Exception. --#\n",
    "\n",
    "confusion_outputs[ \"markedness\" ] = markedness\n",
    "confusion_outputs[ \"MK\" ] = markedness\n",
    "\n",
    "print( \"==> Markedness (MK) = \" + str( markedness ) )\n",
    "print( \"==> Confusion outputs:\" )\n",
    "DictHelper.print_dict( confusion_outputs,\n",
    "                       prefix_IN = \"EXPECTED_OUTPUT_MAP[ \\\"\",\n",
    "                       separator_IN = \"\\\" ] = \",\n",
    "                       suffix_IN = None )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> population values: 2446\n",
      "ACTUAL_VALUE_LIST = [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ]\n",
      "==> predicted values count: 2446\n",
      "PREDICTED_VALUE_LIST = [ 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ]\n"
     ]
    }
   ],
   "source": [
    "print( \"==> population values: \" + str( len( ground_truth_list ) ) )\n",
    "list_name = \"ACTUAL_VALUE_LIST\"\n",
    "string_list = map( str, ground_truth_list )\n",
    "list_values = \", \".join( string_list )\n",
    "print( list_name + \" = [ \" + list_values + \" ]\" )\n",
    "\n",
    "print( \"==> predicted values count: \" + str( len( predicted_list ) ) )\n",
    "list_name = \"PREDICTED_VALUE_LIST\"\n",
    "string_list = map( str, predicted_list )\n",
    "list_values = \", \".join( string_list )\n",
    "print( list_name + \" = [ \" + list_values + \" ]\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfusionMatrixHelper --> \n",
      "DictHelper --> \n",
      "----> ACC : 0.946443172526574\n",
      "----> BM : -0.02649285113540789\n",
      "----> DOR : None\n",
      "----> FDR : 0.02853545950482585\n",
      "----> FNR : 0.026492851135407905\n",
      "----> FOR : 1.0\n",
      "----> FPR : 1.0\n",
      "----> LR+ : 0.9735071488645921\n",
      "----> LR- : None\n",
      "----> MCC : -0.027495193775309384\n",
      "----> MK : -0.028535459504825833\n",
      "----> NPV : 0.0\n",
      "----> PPV : 0.9714645404951742\n",
      "----> SPC : 0.0\n",
      "----> TNR : 0.0\n",
      "----> TPR : 0.9735071488645921\n",
      "----> accuracy : 0.946443172526574\n",
      "----> diagnostic_odds_ratio : None\n",
      "----> f1_score : 0.9724847721067004\n",
      "----> false_discovery_rate : 0.02853545950482585\n",
      "----> false_negative : 63\n",
      "----> false_negative_rate : 0.026492851135407905\n",
      "----> false_omission_rate : 1.0\n",
      "----> false_positive : 68\n",
      "----> false_positive_rate : 1.0\n",
      "----> informedness : -0.02649285113540789\n",
      "----> markedness : -0.028535459504825833\n",
      "----> matthews_correlation_coefficient : -0.027495193775309384\n",
      "----> negative_likelihood_ratio : None\n",
      "----> negative_predictive_value : 0.0\n",
      "----> population_negative : 68\n",
      "----> population_positive : 2378\n",
      "----> positive_likelihood_ratio : 0.9735071488645921\n",
      "----> precision : 0.9714645404951742\n",
      "----> predicted_negative : 63\n",
      "----> predicted_positive : 2383\n",
      "----> recall : 0.9735071488645921\n",
      "----> specificity : 0.0\n",
      "----> total_population : 2446\n",
      "----> true_negative : 0\n",
      "----> true_negative_rate : 0.0\n",
      "----> true_positive : 2315\n"
     ]
    }
   ],
   "source": [
    "confusion_helper = ConfusionMatrixHelper.populate_confusion_matrix( ground_truth_list, predicted_list )\n",
    "print( str( confusion_helper ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sourcenet (Python 3)",
   "language": "python",
   "name": "sourcenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "717px",
    "left": "0px",
    "right": "1036.8px",
    "top": "111px",
    "width": "403px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
