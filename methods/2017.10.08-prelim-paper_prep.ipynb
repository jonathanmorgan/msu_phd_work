{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2017.10.08 - prelim - paper prep**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Reliability\" data-toc-modified-id=\"Reliability-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Reliability</a></span></li><li><span><a href=\"#Additional-analysis\" data-toc-modified-id=\"Additional-analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Additional analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Human-Precision-and-Recall\" data-toc-modified-id=\"Human-Precision-and-Recall-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Human Precision and Recall</a></span></li><li><span><a href=\"#Calculate-reliability-numbers-for-prelim_month...\" data-toc-modified-id=\"Calculate-reliability-numbers-for-prelim_month...-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Calculate reliability numbers for prelim_month...</a></span></li><li><span><a href=\"#...-and-calculate-reliability-numbers-for-prelim_month_human\" data-toc-modified-id=\"...-and-calculate-reliability-numbers-for-prelim_month_human-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>... and calculate reliability numbers for prelim_month_human</a></span></li><li><span><a href=\"#Network-Analysis\" data-toc-modified-id=\"Network-Analysis-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Network Analysis</a></span></li><li><span><a href=\"#Combine-results-into-spreadsheet\" data-toc-modified-id=\"Combine-results-into-spreadsheet-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Combine results into spreadsheet</a></span></li></ul></li><li><span><a href=\"#Preparation-for-automated-assessment\" data-toc-modified-id=\"Preparation-for-automated-assessment-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preparation for automated assessment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Single-name-removal\" data-toc-modified-id=\"Single-name-removal-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Single name removal</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Notes</a></span></li><li><span><a href=\"#Error-detail\" data-toc-modified-id=\"Error-detail-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Error detail</a></span></li></ul></li></ul></li><li><span><a href=\"#CURRENT\" data-toc-modified-id=\"CURRENT-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>CURRENT</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluating-disagreements\" data-toc-modified-id=\"Evaluating-disagreements-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Evaluating disagreements</a></span><ul class=\"toc-item\"><li><span><a href=\"#Disagreement-tracking-process\" data-toc-modified-id=\"Disagreement-tracking-process-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Disagreement tracking process</a></span></li><li><span><a href=\"#Data-in-Reliability_Names_Evaluation\" data-toc-modified-id=\"Data-in-Reliability_Names_Evaluation-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Data in <code>Reliability_Names_Evaluation</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Overall-disagreement-log\" data-toc-modified-id=\"Overall-disagreement-log-4.1.2.1\"><span class=\"toc-item-num\">4.1.2.1&nbsp;&nbsp;</span>Overall disagreement log</a></span></li><li><span><a href=\"#Ground-truth-coding-fixed\" data-toc-modified-id=\"Ground-truth-coding-fixed-4.1.2.2\"><span class=\"toc-item-num\">4.1.2.2&nbsp;&nbsp;</span>Ground truth coding fixed</a></span></li><li><span><a href=\"#Reliability_Names-records-merged\" data-toc-modified-id=\"Reliability_Names-records-merged-4.1.2.3\"><span class=\"toc-item-num\">4.1.2.3&nbsp;&nbsp;</span>Reliability_Names records merged</a></span></li><li><span><a href=\"#Deleted-Reliability_Names-records\" data-toc-modified-id=\"Deleted-Reliability_Names-records-4.1.2.4\"><span class=\"toc-item-num\">4.1.2.4&nbsp;&nbsp;</span>Deleted Reliability_Names records</a></span></li></ul></li><li><span><a href=\"#Evaluating-Disagreements---Human-Error\" data-toc-modified-id=\"Evaluating-Disagreements---Human-Error-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Evaluating Disagreements - Human Error</a></span></li><li><span><a href=\"#Disagreements---Computer-Error\" data-toc-modified-id=\"Disagreements---Computer-Error-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Disagreements - Computer Error</a></span></li></ul></li></ul></li><li><span><a href=\"#Paper-Edits\" data-toc-modified-id=\"Paper-Edits-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Paper Edits</a></span></li><li><span><a href=\"#TODO\" data-toc-modified-id=\"TODO-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>TODO</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliability\n",
    "\n",
    "- back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, need to make sure I understand which results are the final results.\n",
    "\n",
    "To view results: [https://research.local/sourcenet/sourcenet/analysis/reliability/names/results/view](https://research.local/sourcenet/sourcenet/analysis/reliability/names/results/view)\n",
    "\n",
    "I am pretty sure that the human-only results (the ones I will write about) are results with labels:\n",
    "\n",
    "- \"`prelim_reliability_combined_human_final`\"\n",
    "- and \"`prelim_reliability_combined_human`\" (this is old code, versus \"final\" calculated with rewritten code - numbers should be identical).\n",
    "\n",
    "Path to Dropbox folder that holds PDF and Excel file output of reliability numbers:\n",
    "\n",
    "- Dropbox/academia/MSU/program_stuff/prelim_paper/analysis/reliability/2016-data\n",
    "\n",
    "Code to actually calculate reliability numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start to support python 3:\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "\n",
    "#==============================================================================#\n",
    "# ! imports\n",
    "#==============================================================================#\n",
    "\n",
    "# grouped by functional area, then alphabetical order by package, then\n",
    "#     alphabetical order by name of thing being imported.\n",
    "\n",
    "# sourcenet_analysis imports\n",
    "from sourcenet_analysis.reliability.reliability_names_analyzer import ReliabilityNamesAnalyzer\n",
    "\n",
    "#==============================================================================#\n",
    "# ! logic\n",
    "#==============================================================================#\n",
    "\n",
    "# declare variables\n",
    "my_analysis_instance = None\n",
    "label = \"\"\n",
    "indices_to_process = -1\n",
    "result_status = \"\"\n",
    "\n",
    "# make reliability instance\n",
    "my_analysis_instance = ReliabilityNamesAnalyzer()\n",
    "\n",
    "# database connection information - 2 options...  Enter it here:\n",
    "#my_analysis_instance.db_username = \"\"\n",
    "#my_analysis_instance.db_password = \"\"\n",
    "#my_analysis_instance.db_host = \"localhost\"\n",
    "#my_analysis_instance.db_name = \"sourcenet\"\n",
    "\n",
    "# Or set up the following properties in Django_Config, inside the django admins.\n",
    "#     All have application of: \"sourcenet-db-admin\":\n",
    "#     - db_username\n",
    "#     - db_password\n",
    "#     - db_host\n",
    "#     - db_port\n",
    "#     - db_name\n",
    "\n",
    "# run the analyze method, see what happens.\n",
    "#label = \"prelim_reliability_test\"\n",
    "#indices_to_process = 3\n",
    "#label = \"prelim_reliability_combined_human\"\n",
    "#indices_to_process = 3\n",
    "#label = \"name_data_test_combined_human\"\n",
    "#indices_to_process = 3\n",
    "#label = \"prelim_reliability_combined_human_final\"\n",
    "#indices_to_process = 3\n",
    "#label = \"prelim_reliability_combined_all\"\n",
    "#indices_to_process = 4\n",
    "#label = \"prelim_reliability_combined_all_final\"\n",
    "#indices_to_process = 4\n",
    "#label = \"prelim_reliability_test_human\"\n",
    "#indices_to_process = 3\n",
    "#label = \"prelim_reliability_test_all\"\n",
    "#indices_to_process = 4\n",
    "label = \"prelim_month\"\n",
    "indices_to_process = 2\n",
    "result_status = my_analysis_instance.analyze_reliability_names( label, indices_to_process )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional analysis\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Precision and Recall\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Calculate precision and recall for humans versus ground truth - set it up so that coder 1 is as it was for computer (ground_truth having precedence) and then set up coder 2 up the same way, but without ground_truth...\n",
    "\n",
    "- Jupyter notebook: [https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.20-work_log-prelim_month-create_human_Reliability_Names.ipynb](https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.20-work_log-prelim_month-create_human_Reliability_Names.ipynb)\n",
    "- results are in `Dropbox/academia/MSU/program_stuff/prelim_paper/analysis/precision_and_recall/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate reliability numbers for prelim_month...\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Run the reliability calculations for prelim_month just to get lookup assessment (since it is not classification, precision and recall make no sense).\n",
    "\n",
    "- Jupyter notebook: [https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.25-work_log-prelim_month-Reliability_Names_reliability.ipynb](https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.25-work_log-prelim_month-Reliability_Names_reliability.ipynb)\n",
    "- results are in `Dropbox/academia/MSU/program_stuff/prelim_paper/analysis/reliability/2016-data/prelim_month-reliability_results.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... and calculate reliability numbers for prelim_month_human\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Run the reliability calculations for prelim_month_human just to get lookup assessment (since it is not classification, precision and recall make no sense).\n",
    "\n",
    "- Jupyter notebook: [https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.25-work_log-prelim_month_human-Reliability_Names_reliability.ipynb](https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/2017.10.25-work_log-prelim_month_human-Reliability_Names_reliability.ipynb)\n",
    "- results are in `Dropbox/academia/MSU/program_stuff/prelim_paper/analysis/reliability/2016-data/prelim_month_human-reliability_results.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Generate some basic network statistics from the ground truth and automated attribution data, characterize and compare using QAP (including explaining substantial limitations of this given sparseness of networks).\n",
    "\n",
    "Details: [2017.11.14-work_log-prelim-network_analysis.ipynb](2017.11.14-work_log-prelim-network_analysis.ipynb)\n",
    "\n",
    "- find code used to derive network information last time. - Evernote Network Analysis Notes: [https://www.evernote.com/shard/s101/nl/11379781/ef9db83f-5fd3-4bd2-bdd7-1407e2c01f9c/](https://www.evernote.com/shard/s101/nl/11379781/ef9db83f-5fd3-4bd2-bdd7-1407e2c01f9c/)\n",
    "- run it again on full month of data, rather than just a week.\n",
    "- examine traits of ground_truth and automated networks\n",
    "\n",
    "    - compare with QAP.\n",
    "    - look at average degree of reporters\n",
    "    - look at average degree, density, etc.\n",
    "\n",
    "Notebooks:\n",
    "\n",
    "- [2017.11.14-work_log-prelim-network_analysis.ipynb](2017.11.14-work_log-prelim-network_analysis.ipynb) - original Python network analysis, run for original coders, then week and month of new data - per-author source, shared, and article counts; and means of each across all authors.\n",
    "- [2017.12.02-work_log-prelim-R-igraph-grp_month.ipynb](2017.12.02-work_log-prelim-R-igraph-grp_month.ipynb) - basic network analysis of new month and week using igraph.\n",
    "- [2017.12.02-work_log-prelim-R-statnet-grp_month.ipynb](2017.12.02-work_log-prelim-R-statnet-grp_month.ipynb) - basic network analysis of new month and week using statnet.\n",
    "- [2017.12.07-work_log-prelim-R-grp_month-sna-author_info.r.ipynb](2017.12.07-work_log-prelim-R-grp_month-sna-author_info.r.ipynb) - R-based author info (similar to Python-based above) for full month of data.\n",
    "- [2017.12.07-work_log-prelim-R-statnet-grp_month-01.ipynb](2017.12.07-work_log-prelim-R-statnet-grp_month-01.ipynb) - statnet analysis when network is converted to either be 0 or 1 weight, where all weights greater than 1 are converted to 1.  No real difference here, so ignoring in paper.\n",
    "- [2017.12.09-work_log-prelim-R-grp_week-sna-author_info.r.ipynb](2017.12.09-work_log-prelim-R-grp_week-sna-author_info.r.ipynb) - R-based author info (similar to Python-based above) for single week of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T05:07:59.731310Z",
     "start_time": "2017-10-31T05:07:59.720302Z"
    }
   },
   "source": [
    "DONE:\n",
    "\n",
    "- updated forms `ArticleSelectForm` and `PersonSelectForm` to include field for \"`coder_id_priority_list`\"/\"`person_coder_id_priority_list`\".\n",
    "- created method NetworkOutput.get_coder_id_list() that:\n",
    "\n",
    "    - knows about the two places where coder IDs can be set.\n",
    "    - if prioritzed list is present:\n",
    "    \n",
    "        - starts with the priotized list\n",
    "        - appends coders from other field who aren't already in the list to the end of it.\n",
    "        - stores the list in an instance variable inside the object so it can be retrieved easily.\n",
    "        \n",
    "- updated NetworkOutput.create_query_set() to use get_coder_id_list() method.\n",
    "- need to update NetworkOutput.remove_duplicate_article_data() - it is where we choose which Article_Data to omit per article where there are duplicates.  Need to go with order of list.  Might already do this...  Nope.\n",
    "\n",
    "    - get prioritized list.\n",
    "    - for first instance of Article_Data for article, store it (related by id, or unique_identifier?)\n",
    "    - on subsequent Article_Data for article, get index of coder for existing and new.\n",
    "    - Whichever has lower index you keep.\n",
    "    - **_Need to test_**\n",
    "\n",
    "        - person-coded articles:\n",
    "        \n",
    "            - 1) output networks from initial prelim (12/6/2009-12/13/2009).  Make sure they are the same now as they were then.\n",
    "            - 2) keep article specs the same, but change person lookup to use ordered ID list.  See if this is the same as the files in 1 (might not be).  If not, count rows, find and compare rows for some users to see how different they are.  Hopefully same contents, different order...?\n",
    "            - 3) then, switch the article specs to use ordered list and put person specs back to old way, see how this file compares to the others.\n",
    "            - look for differences in:\n",
    "            \n",
    "                - number of rows\n",
    "                - contents of rows\n",
    "                - IDs of those included\n",
    "            \n",
    "        - automated coder:\n",
    "\n",
    "            - 1) output networks from initial prelim (12/6/2009-12/13/2009).  Make sure they are the same now as they were then.\n",
    "            - 2) keep article specs the same, but change person lookup to use ordered ID list.  See if this is the same as the files in 1 (might not be).  If not, count rows, find and compare rows for some users to see how different they are.  Hopefully same contents, different order.\n",
    "            - 3) then, switch the article specs to use ordered list, see how this file compares to the others.\n",
    "\n",
    "        - as long as the tests above check out, then try out the whole month, with prioritized coder list.\n",
    "\n",
    "- need to update NetworkDataOutput and children?  Looks like no - all comes down to the remove_duplicate_article_data().\n",
    "- figure out how to run `sourcenet/R/sna/sna_author_info.r`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine results into spreadsheet\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Next step is to pull analysis together in an Excel spreadsheet like I did last time.\n",
    "\n",
    "For old results and more detailed notes on implementation and interpretation, see `Dropbox/academia/MSU/program_stuff/prelim_paper/analysis/archive/prelim_v1-2015/analysis_summary.xlsx`.\n",
    "\n",
    "New analysis file: `Dropbox/academia/MSU/program_stuff/prelim_paper/analysis/analysis_summary-2017.12.24.xlsx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- In general, revised procedure:\n",
    "\n",
    "    - content analysis protocol to create testing data.\n",
    "    \n",
    "        - create and test protocol.\n",
    "        - report reliability.\n",
    "        - use protocol to create testing data.\n",
    "\n",
    "    - have automated tool code same articles.\n",
    "    - for all disagreements, evaluate manually, correct the testing data when it has an error.\n",
    "    - derive confusion matrix data by comparing automated coding to testing data, assess quality of automated coding using precision and recall (etc.).\n",
    "    \n",
    "- so, won't look as much at comparing humans to computer in terms of agreement for content analysis:\n",
    "\n",
    "    - describe protocol development and reliability assessment\n",
    "    - describe process of turning the resulting data into testing data (don't use \"ground truth\").  Some discussion here of ratio of human error to machine error, proportion of human to machine errors, overall number of errors compared to all decisions, etc.\n",
    "    - outline precision and recall and evaluate.\n",
    "\n",
    "- Removed tabs:\n",
    "\n",
    "    - `agree-prelim_reliability` - old reliability coding between 2 human coders.\n",
    "    - `agree-prelim_network-mentions` - agreement between traits of network data derived from human and computer code - tie weights.\n",
    "    - `values-detect_names` - survey of name detection descriptives - counts across all names of how many were detected and not per coder.  Will see if we need to derive this again for new coders.  Probably won't.\n",
    "    - `values-count_ties` - descriptives and comparison of ties weights between human and computer, to look at something like precision and recall (confusion matrix), but just comparing human and computer, not treating human as ground truth.  No need for this with precision and recall stats.\n",
    "    - `counts_per_person` - not sure what this is...\n",
    "    - `disagreements` - similar to `values-count_ties`, but higher-level analysis.  Will have to create new disagreement information from results of disagreement analysis in creating evaluation data.\n",
    "\n",
    "Updated spreadsheet:\n",
    "\n",
    "- New analysis file: `Dropbox/academia/MSU/program_stuff/prelim_paper/analysis/analysis_summary-2017.12.24.xlsx`\n",
    "- Agreement results:\n",
    "\n",
    "    - tabs \"`CA-reliability-author`\" and \"`CA-reliability-subject`\" are derived from work labeled \"`prelim_reliability_combined`\" = articles from both Grand Rapids Press and Detroit News, to minimally test cross-paper use of protocol.\n",
    "    \n",
    "        - pulled from spreadsheet: `Dropbox/academia/MSU/program_stuff/prelim_paper/analysis/reliability/2016-data/2016.08.27-reliability-prelim_reliability_combined_human.xlsx`\n",
    "    \n",
    "    - old results have \"mentions\".  Mentions are weights from network data back when I derived it and stored it in a table of my own design (\"`sourcenet_analysis_reliability_ties`\"), rather than outputting in formats readable by SNA packages.  Omitting this in favor of precision and recall and network statistics.\n",
    "\n",
    "- Network results:\n",
    "\n",
    "    - Main sources:\n",
    "    \n",
    "        - [2017.12.02-work_log-prelim-R-statnet-grp_month.ipynb](2017.12.02-work_log-prelim-R-statnet-grp_month.ipynb) - basic network analysis of new month and week using statnet.\n",
    "        - [2017.12.07-work_log-prelim-R-grp_month-sna-author_info.r.ipynb](2017.12.07-work_log-prelim-R-grp_month-sna-author_info.r.ipynb) - R-based author info (similar to Python-based above) for full month of data.\n",
    "        - [2017.12.09-work_log-prelim-R-grp_week-sna-author_info.r.ipynb](2017.12.09-work_log-prelim-R-grp_week-sna-author_info.r.ipynb) - R-based author info (similar to Python-based above) for single week of data.\n",
    "        - [2017.12.02-work_log-prelim-R-igraph-grp_month.ipynb](2017.12.02-work_log-prelim-R-igraph-grp_month.ipynb) - basic network analysis of new month and week using igraph.\n",
    "            \n",
    "            - for mean transitivity of nodes, look at igraph notebook.\n",
    "\n",
    "    - Other:\n",
    "    \n",
    "        - [2017.11.14-work_log-prelim-network_analysis.ipynb](2017.11.14-work_log-prelim-network_analysis.ipynb) - original Python network analysis, run for original coders, then week and month of new data - per-author source, shared, and article counts; and means of each across all authors.\n",
    "        - [2017.12.07-work_log-prelim-R-statnet-grp_month-01.ipynb](2017.12.07-work_log-prelim-R-statnet-grp_month-01.ipynb) - statnet analysis when network is converted to either be 0 or 1 weight, where all weights greater than 1 are converted to 1.  No real difference here, so ignoring in paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for automated assessment\n",
    "\n",
    "- Bacl to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In preparation for using the human coding as a standard against which data created with automated tool is assessed, I performed a couple of cleaning steps:\n",
    "\n",
    "- **[Single name removal](#Single-name-removal)** - In CA protocol, we ignored people who were referred to only with a single name part, to avoid potential for ambiguity when assigning a last name.  When preparing for assessment, I first went through and removed all people who were captured with only a single name by the automated tool.\n",
    "- **[Evaluating disagreements](#Evaluating-disagreements)** - In order to make the human-created data as good a reflection of correct data as possible, I then reviewed each disagreement between the human and computer manually.  I assessed the disagreement based on the content analysis protocol and on having been a news writer and editor.  If the human coding was in error, I fixed the human coding (and the content analysis protocol, if it turned out to have been a problem with the protocol)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single name removal\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In CA protocol, we ignored people who were referred to only with a single name part, to avoid potential for ambiguity when assigning a last name.  Removed all instances where person was only ever referenced using a single-word (really single-part - only first name, mostly) name, to remove potential source of ambiguity.  \n",
    "\n",
    "Example:  \"Joe Smith's wife Sandy\" - could assume her name is Sandy Smith, but it could be something else.  For this study, removing that potential ambiguity by discarding instances where a given person's full name is never used.\n",
    "\n",
    "- Notebook with aggregated information on what was removed, and notes: [2017.06.01-work_log-prelim_month-remove_single_names.ipynb](https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/methods/2017.06.01-work_log-prelim_month-remove_single_names.ipynb)\n",
    "- Exceptions:\n",
    "\n",
    "    - If the single-name was an error on the part of either computer or human, where one party detected the full name but the other incorrectly detected just a single name, the Reliability_Names records for the two were merged so the same person being detected was captured, and then the error is subsumed in the agreement and precision-recall analysis.\n",
    "    - Since, post-reliability, the human coding is being used as the standard against which the quality of the automated coding is compared, if the human made an error, the coding was corrected to prepare for assessing the automated coding (see non-destructive method for correcting erroneous human coding below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- removed single names because of potential for ambiguity\n",
    "- Usually means you lose spouses and children of subjects who are not subjects themselves.\n",
    "- 143 instances of single name references removed from coding data.\n",
    "- 3 instances were automated errors that needed to be merged with a more accurate human-coded record.\n",
    "- 1 instance was human error, and was corrected.\n",
    "- Types of single-named entities:\n",
    "\n",
    "    - most were family members of subjects not relevant to the story (spouses, children, parents, grandparents, etc.).\n",
    "    - biblical characters (Mary, Jesus)\n",
    "    - famous people (Obama)\n",
    "    - one homeless man who wouldn't give last name (article [23982](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23982)).\n",
    "    - a few were misspellings (so a wrong spelling of a last name only used once, without first name).\n",
    "    - pets\n",
    "    - out and out automated errors - place name (Example: Saigon - Article [23921](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23921)), parts of song titles (\"Twinkle\" - Article [23491](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23491)), planet names (\"Saturn\" -  Article [23559](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23559)), or a part of a business name were detected as a person name.\n",
    "\n",
    "- Of 143 single names removed from analysis data, 15 instances were out-and-out errors (89.5% correct):\n",
    "\n",
    "    - 3 partial-name detects that had to be merged.\n",
    "    - 3 references to named pet chickens (Article [23065](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23065) - Betty, Mabel, and Violet).\n",
    "    - 9 errors where a place name (Example: Saigon - Article [23921](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23921)), parts of song titles (\"Twinkle\" - Article [23491](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23491)), planet names (\"Saturn\" -  Article [23559](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23559)), or a part of a business name were detected as a person name.\n",
    "\n",
    "- Only noted one instance where the single-name person was quoted (\"Linda\") - Article [23223](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23223) | Article_Data 3212 | 12096 (AS) - Linda ( id = 2911; capture_method = OpenCalais_REST_API_v2 ) (quoted; individual) ==> name: Linda |\n",
    "\n",
    "    - Actually was quoted, but just a one-word name, no explicit mention of last name. Need to keep track of relationship to others in story (\"wife of X\").\n",
    "\n",
    "- Assessment - OpenCalais is actually quite good at identifying single name-part references to people, for the most part.  It even sometimes tacked on a last name based on the context in the article.  But, most of the time it did not.  Appears to be built to know of this potential, but tuned to only take action when it is certain.  Not built to assume name relationships implied by things like \"survived by\" or \"Smith's children X, Y, and Z\".  This is something that could be leveraged in a post-processing step if single names were left in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error detail\n",
    "\n",
    "Errors:\n",
    "\n",
    "- Article 21116\n",
    "\n",
    "    - RANDOM - \"More...\"\n",
    "    - Paragraph 12: More than 600 works of art were added to the museum's collection under her leadership, most notably Ellsworth Kelly's \"Blue White,\" a 25-foot- tall wall sculpture that was commissioned in 2006 for the museum's entry pavilion.\n",
    "    - User: 2 - automated (OpenCalais_REST_API_v2)\n",
    "    - Article [21116](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=21116) | 11288 (AS) - More ( id = 2817; capture_method = OpenCalais_REST_API_v2 ) (mentioned; individual) ==> name: More\n",
    "    \n",
    "- Article 22765\n",
    "\n",
    "    - PLACE NAME\n",
    "    - Paragraph 8: Gavin Orchards has started selling farm-direct apples to Grand Rapids and Fruitport schools. The biggest challenge is the time it takes to deliver low-volume orders, said Mike Gavin, who runs the 240-acre farm near Coopersville with his brother, Dave. \n",
    "    - User: 2 - automated (OpenCalais_REST_API_v2)\n",
    "    - Article [22765](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=22765) | 11806 (AS) - Coopersville ( id = 2869; capture_method = OpenCalais_REST_API_v2 ) (mentioned; individual) ==> name: Coopersville\n",
    "    \n",
    "- Article 23055\n",
    "\n",
    "    - PLACE NAME\n",
    "    - Paragraph 2: While they are not disputing the state DHS' recent decision to reassign longtime Kent County DHS Director Andy Zylstra from Grand Rapids to Lansing, legislators are asking state officials to improve their communications with local workers, state Rep. Robert Dean said.\n",
    "    - User: 2 - automated (OpenCalais_REST_API_v2)\n",
    "    - Article [23055](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23055) | 12014 (AS) - Lansing ( id = 2902; capture_method = OpenCalais_REST_API_v2 ) (mentioned; individual) ==> name: Lansing\n",
    "    \n",
    "- Article 23491\n",
    "\n",
    "    - SONG LYRIC\n",
    "    - Paragraph 39: \"As the program was wrapping up and the kids were leaving the stage, one of the 2-year-olds ran up to the microphone and started singing 'Twinkle, twinkle Christmas star ...' to the tune of 'Twinkle, Twinkle Little Star.' It was so funny and cute.\"\n",
    "    - User: 2 - automated (OpenCalais_REST_API_v2)\n",
    "    - Portion of Song title: | 10448 | Article [23491](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23491) | Article_Data [3249](http://research.local/sourcenet/sourcenet/article/article_data/view/?article_id=23491&article_data_id_select=3249) | 12299 (AS) - Twinkle ( id = 2938; capture_method = OpenCalais_REST_API_v2 ) (mentioned; individual) ==> name: Twinkle |\n",
    "    \n",
    "- Article 23559\n",
    "\n",
    "    - PLANET NAME\n",
    "    - Paragraph 10: \"Three appear: Saturn joins Mars and Venus in March so, through spring and most of summer, there will be three naked eye planets in the evening sky. They will be joined briefly by elusive Mercury in April.\"\n",
    "    - User: 2 - automated (OpenCalais_REST_API_v2)\n",
    "    - \"Saturn joins...\" - | 7961 | Article [23559](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23559) | Article_Data [3254](http://research.local/sourcenet/sourcenet/article/article_data/view/?article_id=23559&article_data_id_select=3254) | 12315 (AS) - Saturn ( id = 2940; capture_method = OpenCalais_REST_API_v2 ) (mentioned; individual) ==> name: Saturn |\n",
    "\n",
    "- Article 23631\n",
    "\n",
    "    - SCHOOL NAME - \"Madonna\"\n",
    "    - Paragraph 6: \"The school is planning a tribute during halftime of the first night's Hope game Tuesday against Madonna. There will also be other activities open to former players and family members connected to DeVette.\"\n",
    "    - User: 2 - automated (OpenCalais_REST_API_v2)\n",
    "    - | 8120 | Article [23631](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23631) | Article_Data [3274](http://research.local/sourcenet/sourcenet/article/article_data/view/?article_id=23631&article_data_id_select=3274) | 12404 (AS) - Madonna ( id = 2946; capture_method = OpenCalais_REST_API_v2 ) (mentioned; individual) ==> name: Madonna |\n",
    "\n",
    "- Article 23631\n",
    "\n",
    "    - SCHOOL NAME\n",
    "    - Paragraph 7: \"We have a dinner scheduled in his honor and memory during the first game of the tournament (between Davenport and Grace Bible),\" Van Wieren said. \"We had people that had a hard time getting to the funeral, so this will be a way that people attending can share memories of Russ.\" \n",
    "    - User: 2 - automated (OpenCalais_REST_API_v2)\n",
    "    - | 8119 | Article [23631](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23631) | Article_Data [3274](http://research.local/sourcenet/sourcenet/article/article_data/view/?article_id=23631&article_data_id_select=3274) | 12405 (AS) - Davenport ( id = 2947; capture_method = OpenCalais_REST_API_v2 ) (mentioned; individual) ==> name: Davenport |\n",
    "\n",
    "- Article 23921\n",
    "\n",
    "    - PLACE\n",
    "    - Paragraph 6: It was 1975 when he fled his native Saigon as it fell to the North Vietnamese Army.\n",
    "    - User: 2 - automated (OpenCalais_REST_API_v2)\n",
    "    - | 8981 | Article [23921](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23921) | Article_Data [3283](http://research.local/sourcenet/sourcenet/article/article_data/view/?article_id=23921&article_data_id_select=3283) | 12444 (AS) - Saigon ( id = 2952; capture_method = OpenCalais_REST_API_v2 ) (mentioned; individual) ==> name: Saigon |\n",
    "    \n",
    "- Article 23974\n",
    "\n",
    "    - BUSINESS NAME - Detected part of business name as person.\n",
    "    - Paragraph 14: Trevor Ditmar, a two-year employee at Smitty's Specialty Beverage, 1489 Lake Drive SE, said customers are vowing to quit in increasing numbers due to the product change.\n",
    "    - User: 2 - automated (OpenCalais_REST_API_v2)\n",
    "    - | 8185 | Article [23974](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23974) | Article_Data [3292](http://research.local/sourcenet/sourcenet/article/article_data/view/?article_id=23974&article_data_id_select=3292) | 12492 (AS) - Smitty ( id = 2789; capture_method = OpenCalais_REST_API_v2 ) (mentioned; individual) ==> name: Smitty |\n",
    "    \n",
    "- Article 21080\n",
    "\n",
    "    - MISSPELLING\n",
    "    - Paragraph 21: \"Ben was in middle school when his father was in Desert Storm, and we'd watch the developments on TV,\" Patti Vab Syzkle said. \"He'd say, 'It's OK, Mom. It's just a skirmish.'\"\n",
    "    - User: 2 - automated (OpenCalais_REST_API_v2)\n",
    "    - Made new person: 11246 (AS) - Syzkle, Patti ( id = 2813; capture_method = OpenCalais_REST_API_v2 ) (quoted; individual) ==> name: Patti Vab Syzkle\n",
    "    - Should have mapped to: 11248 (AS) - Van Syzkle, Patti ( id = 1750; capture_method = OpenCalais_REST_API_v2 ) (mentioned; individual) ==> name: Patti Van Syzkle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CURRENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating disagreements\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Look at stats for disagreements and evaluation, including human and computer errors.\n",
    "\n",
    "- Notebook with work details: [2017.07.01-work_log-prelim_month-evaluate_disagreements.ipynb](https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/methods/2017.07.01-work_log-prelim_month-evaluate_disagreements.ipynb)\n",
    "\n",
    "Process: Look at each instance where there is a disagreement and make sure the human coding is correct.\n",
    "\n",
    "Most are probably instances where the computer screwed up, but since we are calling this human coding \"ground truth\", want to winnow out as much human error as possible.\n",
    "\n",
    "For each disagreement, to check for coder error (like just capturing a name part for a person whose full name was in the story), click the \"Article ID\" in the column that has a link to article ID. It will take you to a view of the article where all the people who coded the article are included, with each detection of a mention or quotation displayed next to the paragraph where the person was originally first detected.\n",
    "\n",
    "If not human error, remove TODO tag, filling in details on the diagreement in the record in `Reliability_Names_Evaluation` for the removal of the tag (details: [Disagreement tracking process](#Disagreement-tracking-process)).\n",
    "\n",
    "If human error:\n",
    "\n",
    "- 1) look at all the disagreements for the article.\n",
    "- 2) remove all TODO tags from all disagreements, and fill in details for each.\n",
    "- 3) follow steps below to create ground_truth copy and fix it.\n",
    "- 4) rebuild Reliability_Names for article and cleanup.\n",
    "- 5) then, do any deletes or merges you need to do, so you only do them once.\n",
    "\n",
    "Pull together some numbers and analysis from disagreement work:\n",
    "\n",
    "- counts of disagreements.\n",
    "- which were human and computer error.\n",
    "- ratio of human error to machine error.\n",
    "- proportion of human and machine errors.\n",
    "- overall number of disagreements compared to all decisions.\n",
    "- characterisation of potential for systematic issues (not as bad as I feared).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disagreement tracking process\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "From [2017.07.01-work_log-prelim_month-evaluate_disagreements.ipynb - Disagreement resolution](https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/methods/2017.07.01-work_log-prelim_month-evaluate_disagreements.ipynb#Disagreement-resolution):\n",
    "\n",
    "For each disagreement, click on the article ID link in the row to go to the article and check to see if the human coding for the disagreement in question is correct ( [http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/) ).\n",
    "\n",
    "Once you've evaluated and verified the human coding, remove the \"`TODO`\" tag from the current record (either from the single-article view above if you've removed all disagreements, or from the disagreement view if not):\n",
    "\n",
    "- Click the checkbox in the \"**select**\" column next to the record whose evaluation is complete.\n",
    "- In the \"**Reliability names action:**\" field, select \"_Remove tag(s) from selected_\".\n",
    "- In the \"**Tag(s) - (comma-delimited):**\" field, enter \"_`TODO`_\" (without the quotes).\n",
    "- Click the \"**Do Action**\" button.\n",
    "- This will also place information on the `Reliability_Names` record into a `Reliability_Names_Evaluation` record in the database.  The message that results from this action completing will include a link to the record (the first number in the output).  Click the link to open the record and update it with additional details.  Specifically:\n",
    "\n",
    "    - status - status of human coder's coding:\n",
    "        \n",
    "        - If the human coder got it right, status is \"CORRECT\", even if OpenCalais had an egregious error.\n",
    "        - If this is because the coding screen couldn't capture compound names initially, set status to \"INCOMPLETE\", set the status message to \"SKIPPED because screen couldn't deal with compound names\", put the compound name string in notes, and then add tag \"compound_names\".\n",
    "        - if the OC coder had an issue because we had to smoosh all paragraphs together because it didn't deal well with HTML markup in the body of text it processed, set status to \"CORRECT\", set status message to \"OC ERROR because of formatting\", and then explain the problem in the notes.  If the article is a list or column with odd formatting, consider flagging the article for removal from the study.\n",
    "        - if you have to update ground truth, set \"`status`\" to \"ERROR\".\n",
    "        - else, use your best judgment.\n",
    "            \n",
    "    - if problems caused by automated coder error, click the \"`is_automated_error`\" checkbox.\n",
    "    - update the \"`status_message`\" so it contains a brief description of what exactly happened (should have been mentioned, should have been quoted, missed the person entirely, etc.).\n",
    "    - update \"`Notes`\" with more details.\n",
    "    \n",
    "        - If should have been quoted or mentioned, note the graf # and paragraph text of the paragraph that indicates this.\n",
    "    \n",
    "    - add \"`Tags`\" if appropriate (for sports articles, for example, add \"sports\" tag).\n",
    "\n",
    "_**NOTE: Always remove TODO tag first, so you have a record of status for each Reliability Names record.  Then, once you've done that, you can merge, delete, etc.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data in `Reliability_Names_Evaluation`\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall disagreement log\n",
    "\n",
    "- Track each Reliability_Names record with disagreement that we evaluate (All \"remove tags\" events with label \"prelim_month\"):\n",
    "- Moved to `Reliability_Names_Evaluation` table in django: [http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?label=prelim_month&o=-1.7.8.3.5](http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?label=prelim_month&o=-1.7.8.3.5)\n",
    "- [All \"remove tags\" events with label \"prelim_month\"](https://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=remove_tags&label=prelim_month&o=-1.7.8.3.5)\n",
    "- 428 total records.\n",
    "- of those, 13 are same person and article, but different `Reliability_Names` record, so disagreements that had to be corrected twice because of rebuilding `Reliability_Names` for the article (either human error, or something weird).  SQL:\n",
    "\n",
    "        SELECT sarne1.person_name,\n",
    "            sarne1.id,\n",
    "            sarne1.status,\n",
    "            sarne1.original_reliability_names_id,\n",
    "            sarne2.id,\n",
    "            sarne2.status,\n",
    "            sarne2.original_reliability_names_id\n",
    "        FROM sourcenet_analysis_reliability_names_evaluation AS sarne1,\n",
    "            sourcenet_analysis_reliability_names_evaluation AS sarne2\n",
    "        WHERE sarne1.id != sarne2.id\n",
    "            AND sarne1.label = 'prelim_month'\n",
    "            AND sarne2.label = 'prelim_month'\n",
    "            AND sarne1.event_type = 'remove_tags'\n",
    "            AND sarne2.event_type = 'remove_tags'\n",
    "            AND sarne1.article_id = sarne2.article_id\n",
    "            AND sarne1.person_name = sarne2.person_name\n",
    "            AND sarne1.original_reliability_names_id != sarne2.original_reliability_names_id\n",
    "            AND sarne2.original_reliability_names_id > sarne1.original_reliability_names_id\n",
    "        ORDER BY sarne1.id ASC;\n",
    "\n",
    "- So, 428 - 13 = 415 unique disagreements.\n",
    "- Could regenerate Reliability_Names without `ground_truth` to look at original counts?  Should be able to...  Just need to make sure I remember all steps...\n",
    "\n",
    "    - would need to clear single names, then I'd be left with disagreements.  Not worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground truth coding fixed\n",
    "\n",
    "- For some, the error will be on the part of the human coder.  For human error, we create a new \"`ground_truth`\" record that we will correct, so we preserve original coding (and evidence of errors) in case we want or need that information later.  Below, we have a table of the articles where we had to fix ground truth.  To find the original coding, click the Article link.\n",
    "- Denoted by records with \"`is_ground_truth_fixed`\" set to True in the `Reliability_Names_Evaluation` table in django:  [http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?is_ground_truth_fixed__exact=1&label=prelim_month&o=-1.7.8.3.5](http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?is_ground_truth_fixed__exact=1&label=prelim_month&o=-1.7.8.3.5)\n",
    "- 130 total (130/415 = 31.3% - this is a lot - is this right?)\n",
    "\n",
    "    - 4 duplicates, so call it 126.\n",
    "    - based on \"`prelim_month_human`\" `Reliability_Names` tag, 135 disagreements between original and corrected coding.  Probably some merging needed here?\n",
    "    - of those, 4 are same person and article, but different `Reliability_Names` record, so disagreements that had to be corrected twice because of rebuilding `Reliability_Names` for the article (either human error, or something else weird).  SQL:\n",
    "\n",
    "            SELECT sarne1.person_name,\n",
    "                sarne1.id,\n",
    "                sarne1.status,\n",
    "                sarne1.original_reliability_names_id,\n",
    "                sarne1.article_id\n",
    "                sarne2.id,\n",
    "                sarne2.status,\n",
    "                sarne2.original_reliability_names_id,\n",
    "                sarne2.article_id\n",
    "            FROM sourcenet_analysis_reliability_names_evaluation AS sarne1,\n",
    "                sourcenet_analysis_reliability_names_evaluation AS sarne2\n",
    "            WHERE sarne1.id != sarne2.id\n",
    "                AND sarne1.label = 'prelim_month'\n",
    "                AND sarne2.label = 'prelim_month'\n",
    "                AND sarne1.event_type = 'remove_tags'\n",
    "                AND sarne2.event_type = 'remove_tags'\n",
    "                AND sarne1.is_ground_truth_fixed = TRUE\n",
    "                AND sarne2.is_ground_truth_fixed = TRUE\n",
    "                AND sarne1.article_id = sarne2.article_id\n",
    "                AND sarne1.person_name = sarne2.person_name\n",
    "                AND sarne1.original_reliability_names_id != sarne2.original_reliability_names_id\n",
    "                AND sarne2.original_reliability_names_id > sarne1.original_reliability_names_id\n",
    "            ORDER BY sarne1.id ASC;\n",
    "\n",
    "        Results (looks like these are ones that had to be merged, so ... minimze - when ambiguity, assume error in creating data, treat as duplicates so reduce count by number of duplicates):\n",
    "\n",
    "| person_name | id | status | original_reliability_names_id | article_id | id | status | original_reliability_names_id | article_id |\n",
    "|--|--|--|--|--|--|--|--|--|\n",
    "| Jeff Hawkins | 33 | ERROR | 9408 | [21007](https://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=21007) | 34 | ERROR | 9414 | 21007 |\n",
    "| Fritz Wahlfield | 405 | ERROR | 10330 | [22415](https://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=22415) | 407 | CORRECT | 10997 | 22415 |\n",
    "| John Agar | 610 | ERROR | 8917 | [23904](https://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23904) | 611 | ERROR | 8918 | 23904 |\n",
    "| Rachael Recker | 620 | ERROR | 8968 | [23920](https://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23920) | 619 | ERROR | 8976 | 23920 |\n",
    "\n",
    "- number of affected articles?\n",
    "- characterization of the problems:\n",
    "\n",
    "    - is_missed (a person): \n",
    "    - is_missed (limitation of coding application):\n",
    "    - is_author_shb_source:\n",
    "    - is_source_shb_author:\n",
    "    - is_quoted_shb_mentioned: \n",
    "    - is_mentioned_shb_quoted:\n",
    "    - is_wrong_text_captured:\n",
    "    - is_duplicate:\n",
    "    \n",
    "- TODO:\n",
    "\n",
    "    - set all with \"is_ground_truth_fixed\" = True so \"is_todo\" = True.\n",
    "    - go through all \"is_todo\" and update the metadata booleans.\n",
    "    - update counts of characterizations above.\n",
    "    - figure out number of affected articles (should just be count of Article_Data by ground_truth user)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reliability_Names records merged\n",
    "\n",
    "- For some, need to merge a single-name detection by Calais with full-name detection by ground_truth (an OpenCalais error - did not detect full name - combined with lookup error - didn't lookup the right person since missed part of his or her name).  Will still have subsequently deleted one or more duplicate rows.\n",
    "- Denoted by records with \"`event_type`\" set to \"merge\" in the Reliability_Names_Evaluation table in django: [http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=merge&label=prelim_month&o=-1.7.8.3.5](http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=merge&label=prelim_month&o=-1.7.8.3.5)\n",
    "- 18 total\n",
    "- TODO: need to check for the repeat thing SQL like in remove tags.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleted Reliability_Names records\n",
    "\n",
    "- Some records need to be deleted:\n",
    "\n",
    "    - are just broken, need to be deleted.\n",
    "    - cleanup after a merge (one stays, one goes).\n",
    "    - cleanup after rebuilding `Reliability_Names` (single names removed once again, etc.).\n",
    "    - ?\n",
    "\n",
    "- Denoted by records with \"`event_type`\" set to \"deleted\" in the `Reliability_Names_Evaluation` table in django: [http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=delete&label=prelim_month&o=-1.7.8.3.5](http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=delete&label=prelim_month&o=-1.7.8.3.5)\n",
    "- 181 total\n",
    "- TODO: need to provide examples.\n",
    "- TODO: need to check for the repeat thing SQL like in remove tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "\n",
    "- after correction:\n",
    "\n",
    "    - 2,446 overall coding decisions on people in \"`prelim_month`\".\n",
    "\n",
    "        - https://research.local/sourcenet/sourcenet/analysis/reliability/names/disagreement/view\n",
    "        - label: \"`prelim_month`\"\n",
    "        - coders to compare (1 ==>): 2\n",
    "        - Filter Type: \"Lookup\" (default)\n",
    "\n",
    "    - 294 disagreements in \"`prelim_month`\".\n",
    "    \n",
    "        - https://research.local/sourcenet/sourcenet/analysis/reliability/names/disagreement/view\n",
    "        - label: \"`prelim_month`\"\n",
    "        - coders to compare (1 ==>): 2\n",
    "        - Filter Type: \"Disagree\"\n",
    "\n",
    "    - 135 disagreements between humans and corrected.\n",
    "    \n",
    "        - https://research.local/sourcenet/sourcenet/analysis/reliability/names/disagreement/view\n",
    "        - label: \"`prelim_month_human`\"\n",
    "        - coders to compare (1 ==>): 2\n",
    "        - Filter Type: \"Disagree\"\n",
    "        - to more readily explore distribution of problems, added tag \"`prelim_month_human_disagree`\"to disagreements in \"`prelim_month_human`\":\n",
    "        \n",
    "            - all disagreements:\n",
    "            - label: \"`prelim_month_human`\"\n",
    "            - coders to compare (1 ==>): 2\n",
    "            - Filter Type: \"Lookup\"\n",
    "            - Reliability_Names tags: \"`prelim_month_human_disagree`\"\n",
    "            \n",
    "- TODO:\n",
    "\n",
    "    - look over human errors (disagreement between humans and corrected data).\n",
    "    \n",
    "        - Figure out types of error, counts of each type.  In SQL, filter on tag, then on other traits.\n",
    "        \n",
    "            - human miss: if coder 2 empty, 1 not, then human missed a person.\n",
    "            - human false detect: if coder 1 empty, 2 not, human erroneously detected person.\n",
    "            - \n",
    "            - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Disagreements - Human Error\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Human error - Per article (how many have ground truth?) and per decision?  How many errors, compared to total number of decisions, and what kind of errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disagreements - Computer Error\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Computer error - look over classes of error for trends (systemic error) and interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Edits\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "TODO:\n",
    "\n",
    "- path to paper: `Dropbox/academia/MSU/program_stuff/prelim_paper/paper/2017.10.30/Morgan-Prelim.docx`\n",
    "- cut the shit out of lit. review.\n",
    "- update methods\n",
    "\n",
    "    - generate content analysis data.\n",
    "    - assess reliability.\n",
    "    - generate attribution data using OpenCalais API.\n",
    "    - evaluate disagreements to establish ground truth (fix human errors).\n",
    "    - calculate precision and recall.\n",
    "    - examine resulting networks.\n",
    "\n",
    "- update results\n",
    "- update discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- make sure the network code can deal with multiple coders, and can prioritize in an order I specify."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sourcenet (Python 3)",
   "language": "python",
   "name": "sourcenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "689px",
    "left": "0px",
    "right": "1118px",
    "top": "111px",
    "width": "322px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
