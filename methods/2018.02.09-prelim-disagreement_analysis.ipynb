{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2018.02.09 - prelim - disagreement analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup---Imports\" data-toc-modified-id=\"Setup---Imports-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Setup - Imports</a></span></li><li><span><a href=\"#Setup---virtualenv-jupyter-kernel\" data-toc-modified-id=\"Setup---virtualenv-jupyter-kernel-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Setup - virtualenv jupyter kernel</a></span></li><li><span><a href=\"#Setup---Initialize-Django\" data-toc-modified-id=\"Setup---Initialize-Django-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Setup - Initialize Django</a></span></li></ul></li><li><span><a href=\"#Evaluating-disagreements\" data-toc-modified-id=\"Evaluating-disagreements-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Evaluating disagreements</a></span><ul class=\"toc-item\"><li><span><a href=\"#Disagreement-tracking-process\" data-toc-modified-id=\"Disagreement-tracking-process-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Disagreement tracking process</a></span></li><li><span><a href=\"#Data-in-Reliability_Names_Evaluation\" data-toc-modified-id=\"Data-in-Reliability_Names_Evaluation-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Data in <code>Reliability_Names_Evaluation</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Overall-disagreement-log\" data-toc-modified-id=\"Overall-disagreement-log-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Overall disagreement log</a></span><ul class=\"toc-item\"><li><span><a href=\"#flag-duplicates\" data-toc-modified-id=\"flag-duplicates-2.2.1.1\"><span class=\"toc-item-num\">2.2.1.1&nbsp;&nbsp;</span>flag duplicates</a></span></li><li><span><a href=\"#review-tags\" data-toc-modified-id=\"review-tags-2.2.1.2\"><span class=\"toc-item-num\">2.2.1.2&nbsp;&nbsp;</span>review tags</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#CURRENT\" data-toc-modified-id=\"CURRENT-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>CURRENT</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Ground-truth-coding-fixed\" data-toc-modified-id=\"Ground-truth-coding-fixed-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Ground truth coding fixed</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mark-all-ground-truth-updates-as-TODO\" data-toc-modified-id=\"Mark-all-ground-truth-updates-as-TODO-3.0.1.1\"><span class=\"toc-item-num\">3.0.1.1&nbsp;&nbsp;</span>Mark all ground truth updates as TODO</a></span></li><li><span><a href=\"#Mark-all-ground-truth-updates-as-human-error\" data-toc-modified-id=\"Mark-all-ground-truth-updates-as-human-error-3.0.1.2\"><span class=\"toc-item-num\">3.0.1.2&nbsp;&nbsp;</span>Mark all ground truth updates as human error</a></span></li><li><span><a href=\"#Count-ground-truth-updates\" data-toc-modified-id=\"Count-ground-truth-updates-3.0.1.3\"><span class=\"toc-item-num\">3.0.1.3&nbsp;&nbsp;</span>Count ground truth updates</a></span></li></ul></li><li><span><a href=\"#Reliability_Names-records-merged\" data-toc-modified-id=\"Reliability_Names-records-merged-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Reliability_Names records merged</a></span></li><li><span><a href=\"#Deleted-Reliability_Names-records\" data-toc-modified-id=\"Deleted-Reliability_Names-records-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Deleted Reliability_Names records</a></span></li></ul></li><li><span><a href=\"#Evaluating-Disagreements---Human-Error\" data-toc-modified-id=\"Evaluating-Disagreements---Human-Error-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Evaluating Disagreements - Human Error</a></span></li><li><span><a href=\"#Disagreements---Computer-Error\" data-toc-modified-id=\"Disagreements---Computer-Error-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Disagreements - Computer Error</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Imports\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T19:37:50.665567Z",
     "start_time": "2018-02-20T19:37:50.660229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages imported at 2018-02-20 14:37:50.662686\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import six\n",
    "\n",
    "print( \"packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - virtualenv jupyter kernel\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "If you are using a virtualenv, make sure that you:\n",
    "\n",
    "- have installed your virtualenv as a kernel.\n",
    "- choose the kernel for your virtualenv as the kernel for your notebook (Kernel --> Change kernel).\n",
    "\n",
    "Since I use a virtualenv, need to get that activated somehow inside this notebook.  One option is to run `../dev/wsgi.py` in this notebook, to configure the python environment manually as if you had activated the `sourcenet` virtualenv.  To do this, you'd make a code cell that contains:\n",
    "\n",
    "    %run ../dev/wsgi.py\n",
    "    \n",
    "This is sketchy, however, because of the changes it makes to your Python environment within the context of whatever your current kernel is.  I'd worry about collisions with the actual Python 3 kernel.  Better, one can install their virtualenv as a separate kernel.  Steps:\n",
    "\n",
    "- activate your virtualenv:\n",
    "\n",
    "        workon sourcenet\n",
    "\n",
    "- in your virtualenv, install the package `ipykernel`.\n",
    "\n",
    "        pip install ipykernel\n",
    "\n",
    "- use the ipykernel python program to install the current environment as a kernel:\n",
    "\n",
    "        python -m ipykernel install --user --name <env_name> --display-name \"<display_name>\"\n",
    "        \n",
    "    `sourcenet` example:\n",
    "    \n",
    "        python -m ipykernel install --user --name sourcenet --display-name \"sourcenet (Python 3)\"\n",
    "        \n",
    "More details: [http://ipython.readthedocs.io/en/stable/install/kernel_install.html](http://ipython.readthedocs.io/en/stable/install/kernel_install.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T19:37:53.678684Z",
     "start_time": "2018-02-20T19:37:53.665094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jonathanmorgan/work/sourcenet/django/research/work/msu_phd_work/methods'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Initialize Django\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, initialize my dev django project, so I can run code in this notebook that references my django models and can talk to the database using my project's settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T19:37:58.203490Z",
     "start_time": "2018-02-20T19:37:56.318432Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathanmorgan/.virtualenvs/sourcenet/lib/python3.5/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "django initialized at 2018-02-20 19:37:58.201133\n"
     ]
    }
   ],
   "source": [
    "%run django_init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T19:37:59.760925Z",
     "start_time": "2018-02-20T19:37:59.756355Z"
    }
   },
   "outputs": [],
   "source": [
    "# django imports\n",
    "from sourcenet_analysis.models import Reliability_Names_Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating disagreements\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Look at stats for disagreements and evaluation, including human and computer errors.\n",
    "\n",
    "- Notebook with work details: [2017.07.01-work_log-prelim_month-evaluate_disagreements.ipynb](https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/methods/2017.07.01-work_log-prelim_month-evaluate_disagreements.ipynb)\n",
    "\n",
    "Process: Look at each instance where there is a disagreement and make sure the human coding is correct.\n",
    "\n",
    "Most are probably instances where the computer screwed up, but since we are calling this human coding \"ground truth\", want to winnow out as much human error as possible.\n",
    "\n",
    "For each disagreement, to check for coder error (like just capturing a name part for a person whose full name was in the story), click the \"Article ID\" in the column that has a link to article ID. It will take you to a view of the article where all the people who coded the article are included, with each detection of a mention or quotation displayed next to the paragraph where the person was originally first detected.\n",
    "\n",
    "If not human error, remove TODO tag, filling in details on the diagreement in the record in `Reliability_Names_Evaluation` for the removal of the tag (details: [Disagreement tracking process](#Disagreement-tracking-process)).\n",
    "\n",
    "If human error:\n",
    "\n",
    "- 1) look at all the disagreements for the article.\n",
    "- 2) remove all TODO tags from all disagreements, and fill in details for each.\n",
    "- 3) follow steps below to create ground_truth copy and fix it.\n",
    "- 4) rebuild Reliability_Names for article and cleanup.\n",
    "- 5) then, do any deletes or merges you need to do, so you only do them once.\n",
    "\n",
    "Pull together some numbers and analysis from disagreement work:\n",
    "\n",
    "- counts of disagreements.\n",
    "- which were human and computer error.\n",
    "- ratio of human error to machine error.\n",
    "- proportion of human and machine errors.\n",
    "- overall number of disagreements compared to all decisions.\n",
    "- characterisation of potential for systematic issues (not as bad as I feared).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disagreement tracking process\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "From [2017.07.01-work_log-prelim_month-evaluate_disagreements.ipynb - Disagreement resolution](https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/methods/2017.07.01-work_log-prelim_month-evaluate_disagreements.ipynb#Disagreement-resolution):\n",
    "\n",
    "For each disagreement, click on the article ID link in the row to go to the article and check to see if the human coding for the disagreement in question is correct ( [http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/) ).\n",
    "\n",
    "Once you've evaluated and verified the human coding, remove the \"`TODO`\" tag from the current record (either from the single-article view above if you've removed all disagreements, or from the disagreement view if not):\n",
    "\n",
    "- Click the checkbox in the \"**select**\" column next to the record whose evaluation is complete.\n",
    "- In the \"**Reliability names action:**\" field, select \"_Remove tag(s) from selected_\".\n",
    "- In the \"**Tag(s) - (comma-delimited):**\" field, enter \"_`TODO`_\" (without the quotes).\n",
    "- Click the \"**Do Action**\" button.\n",
    "- This will also place information on the `Reliability_Names` record into a `Reliability_Names_Evaluation` record in the database.  The message that results from this action completing will include a link to the record (the first number in the output).  Click the link to open the record and update it with additional details.  Specifically:\n",
    "\n",
    "    - status - status of human coder's coding:\n",
    "        \n",
    "        - If the human coder got it right, status is \"CORRECT\", even if OpenCalais had an egregious error.\n",
    "        - If this is because the coding screen couldn't capture compound names initially, set status to \"INCOMPLETE\", set the status message to \"SKIPPED because screen couldn't deal with compound names\", put the compound name string in notes, and then add tag \"compound_names\".\n",
    "        - if the OC coder had an issue because we had to smoosh all paragraphs together because it didn't deal well with HTML markup in the body of text it processed, set status to \"CORRECT\", set status message to \"OC ERROR because of formatting\", and then explain the problem in the notes.  If the article is a list or column with odd formatting, consider flagging the article for removal from the study.\n",
    "        - if you have to update ground truth, set \"`status`\" to \"ERROR\".\n",
    "        - else, use your best judgment.\n",
    "            \n",
    "    - if problems caused by automated coder error, click the \"`is_automated_error`\" checkbox.\n",
    "    - update the \"`status_message`\" so it contains a brief description of what exactly happened (should have been mentioned, should have been quoted, missed the person entirely, etc.).\n",
    "    - update \"`Notes`\" with more details.\n",
    "    \n",
    "        - If should have been quoted or mentioned, note the graf # and paragraph text of the paragraph that indicates this.\n",
    "    \n",
    "    - add \"`Tags`\" if appropriate (for sports articles, for example, add \"sports\" tag).\n",
    "\n",
    "_**NOTE: Always remove TODO tag first, so you have a record of status for each Reliability Names record.  Then, once you've done that, you can merge, delete, etc.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data in `Reliability_Names_Evaluation`\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall disagreement log\n",
    "\n",
    "- Track each Reliability_Names record with disagreement that we evaluate (All \"remove tags\" events with label \"prelim_month\"):\n",
    "- Moved to `Reliability_Names_Evaluation` table in django: [http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?label=prelim_month&o=-1.7.8.3.5](http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?label=prelim_month&o=-1.7.8.3.5)\n",
    "- [All \"remove tags\" events with label \"prelim_month\"](https://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=remove_tags&label=prelim_month&o=-1.7.8.3.5)\n",
    "- 428 total records.\n",
    "- of those, 13 are same person and article, but different `Reliability_Names` record, so disagreements that had to be corrected twice because of rebuilding `Reliability_Names` for the article (either human error, or something weird).  SQL:\n",
    "\n",
    "        SELECT sarne1.person_name,\n",
    "            sarne1.id,\n",
    "            sarne1.status,\n",
    "            sarne1.original_reliability_names_id,\n",
    "            sarne1.is_duplicate,\n",
    "            sarne2.is_duplicate,\n",
    "            sarne2.id,\n",
    "            sarne2.status,\n",
    "            sarne2.original_reliability_names_id\n",
    "        FROM sourcenet_analysis_reliability_names_evaluation AS sarne1,\n",
    "            sourcenet_analysis_reliability_names_evaluation AS sarne2\n",
    "        WHERE sarne1.id != sarne2.id\n",
    "            AND sarne1.label = 'prelim_month'\n",
    "            AND sarne2.label = 'prelim_month'\n",
    "            AND sarne1.event_type = 'remove_tags'\n",
    "            AND sarne2.event_type = 'remove_tags'\n",
    "            AND sarne1.article_id = sarne2.article_id\n",
    "            AND sarne1.person_name = sarne2.person_name\n",
    "            AND sarne1.original_reliability_names_id != sarne2.original_reliability_names_id\n",
    "            AND sarne2.original_reliability_names_id > sarne1.original_reliability_names_id\n",
    "        ORDER BY sarne1.id ASC;\n",
    "\n",
    "- So, 428 - 13 = 415 unique disagreements.\n",
    "- Could regenerate Reliability_Names without `ground_truth` to look at original counts?  Should be able to...  Just need to make sure I remember all steps...\n",
    "\n",
    "    - would need to clear single names, then I'd be left with disagreements.  Not worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flag duplicates\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "TODO:\n",
    "\n",
    "- duplicates\n",
    "\n",
    "    - Make sure that for each pair of duplicates, one has \"is_duplicate\" checked.\n",
    "    - to start, mark all that have duplicates as \"is_to_do\" = True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: get IDs of records with duplicates**\n",
    "\n",
    "    SELECT DISTINCT ( sarne1.id )\n",
    "    FROM sourcenet_analysis_reliability_names_evaluation AS sarne1,\n",
    "        sourcenet_analysis_reliability_names_evaluation AS sarne2\n",
    "    WHERE sarne1.id != sarne2.id\n",
    "        AND sarne1.label = 'prelim_month'\n",
    "        AND sarne2.label = 'prelim_month'\n",
    "        AND sarne1.event_type = 'remove_tags'\n",
    "        AND sarne2.event_type = 'remove_tags'\n",
    "        AND sarne1.article_id = sarne2.article_id\n",
    "        AND sarne1.person_name = sarne2.person_name\n",
    "        AND sarne1.original_reliability_names_id != sarne2.original_reliability_names_id\n",
    "        AND sarne2.original_reliability_names_id > sarne1.original_reliability_names_id\n",
    "    ORDER BY sarne1.id ASC;\n",
    "\n",
    "and\n",
    "\n",
    "    SELECT DISTINCT ( sarne2.id )\n",
    "    FROM sourcenet_analysis_reliability_names_evaluation AS sarne1,\n",
    "        sourcenet_analysis_reliability_names_evaluation AS sarne2\n",
    "    WHERE sarne1.id != sarne2.id\n",
    "        AND sarne1.label = 'prelim_month'\n",
    "        AND sarne2.label = 'prelim_month'\n",
    "        AND sarne1.event_type = 'remove_tags'\n",
    "        AND sarne2.event_type = 'remove_tags'\n",
    "        AND sarne1.article_id = sarne2.article_id\n",
    "        AND sarne1.person_name = sarne2.person_name\n",
    "        AND sarne1.original_reliability_names_id != sarne2.original_reliability_names_id\n",
    "        AND sarne2.original_reliability_names_id > sarne1.original_reliability_names_id\n",
    "    ORDER BY sarne2.id ASC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T19:21:30.434131Z",
     "start_time": "2018-02-09T19:21:30.381468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Got IDs that contain duplicates, now tag them as todo\n",
    "ids_to_process = []\n",
    "ids_to_process.append( 15 )\n",
    "ids_to_process.append( 33 )\n",
    "ids_to_process.append( 75 )\n",
    "ids_to_process.append( 405 )\n",
    "ids_to_process.append( 435 )\n",
    "ids_to_process.append( 512 )\n",
    "ids_to_process.append( 556 )\n",
    "ids_to_process.append( 586 )\n",
    "ids_to_process.append( 610 )\n",
    "ids_to_process.append( 620 )\n",
    "ids_to_process.append( 635 )\n",
    "ids_to_process.append( 646 )\n",
    "ids_to_process.append( 16 )\n",
    "ids_to_process.append( 34 )\n",
    "ids_to_process.append( 76 )\n",
    "ids_to_process.append( 407 )\n",
    "ids_to_process.append( 432 )\n",
    "ids_to_process.append( 513 )\n",
    "ids_to_process.append( 517 )\n",
    "ids_to_process.append( 558 )\n",
    "ids_to_process.append( 596 )\n",
    "ids_to_process.append( 611 )\n",
    "ids_to_process.append( 619 )\n",
    "ids_to_process.append( 637 )\n",
    "ids_to_process.append( 651 )\n",
    "do_save = False\n",
    "\n",
    "# retrieve model instances.\n",
    "\n",
    "# get all evaluation records with label = \"prelim_month\" and IDs in our list.\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( pk__in = ids_to_process )\n",
    "\n",
    "# count?\n",
    "eval_count = evaluation_qs.count()\n",
    "print( \"record count: {}\".format( str( eval_count ) ) )\n",
    "\n",
    "# loop, setting \"is_to_do\" to True on each and saving.\n",
    "for current_eval in evaluation_qs:\n",
    "    \n",
    "    # set is_to_do to True and set work_status to \"metadata_review\".\n",
    "    current_eval.is_to_do = True\n",
    "    current_eval.work_status = \"duplicate_processing\"\n",
    "    \n",
    "    # save?\n",
    "    if ( do_save == True ):\n",
    "        \n",
    "        # save! save! save!\n",
    "        current_eval.save()\n",
    "        \n",
    "    #-- END check to see if we save(). --#\n",
    "    \n",
    "#-- END loop over QuerySet. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out, many of these were a missed person by human coder, that, once fixed, revealed a problem with the automated coding.  So, many were actually not duplicates, they were two separate issues with the same person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### review tags\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "TODO:\n",
    "\n",
    "- get all tags.\n",
    "- make boolean columns for each tag.\n",
    "- for each record, set flags based on tags.\n",
    "- Not going to flag all as todo and \"review_disagreements\".\n",
    "- Just use the tags I set, imperfect as they are, make basic counts and pick good ones for writing about disagreements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T19:16:14.441604Z",
     "start_time": "2018-02-19T19:16:13.320830Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "django-taggit documentation: https://github.com/alex/django-taggit\n",
    "\n",
    "Adding tags to a model:\n",
    "\n",
    "    from django.db import models\n",
    "    \n",
    "    from taggit.managers import TaggableManager\n",
    "    \n",
    "    class Food(models.Model):\n",
    "        # ... fields here\n",
    "    \n",
    "        tags = TaggableManager()\n",
    "\n",
    "Interacting with a model that has tags:\n",
    "\n",
    "    >>> apple = Food.objects.create(name=\"apple\")\n",
    "    >>> apple.tags.add(\"red\", \"green\", \"delicious\")\n",
    "    >>> apple.tags.all()\n",
    "    [<Tag: red>, <Tag: green>, <Tag: delicious>]\n",
    "    >>> apple.tags.remove(\"green\")\n",
    "    >>> apple.tags.all()\n",
    "    [<Tag: red>, <Tag: delicious>]\n",
    "    >>> Food.objects.filter(tags__name__in=[\"red\"])\n",
    "    [<Food: apple>, <Food: cherry>]\n",
    "    \n",
    "    # include only those with certain tags.\n",
    "    #tags_in_list = [ \"prelim_unit_test_001\", \"prelim_unit_test_002\", \"prelim_unit_test_003\", \"prelim_unit_test_004\", \"prelim_unit_test_005\", \"prelim_unit_test_006\", \"prelim_unit_test_007\" ]\n",
    "    tags_in_list = [ \"grp_month\", ]\n",
    "    if ( len( tags_in_list ) > 0 ):\n",
    "    \n",
    "        # filter\n",
    "        print( \"filtering to just articles with tags: \" + str( tags_in_list ) )\n",
    "        grp_article_qs = grp_article_qs.filter( tags__name__in = tags_in_list )\n",
    "        \n",
    "    #-- END check to see if we have a specific list of tags we want to include --#\n",
    "\n",
    "'''\n",
    "\n",
    "# imports\n",
    "from sourcenet_analysis.models import Reliability_Names_Evaluation\n",
    "\n",
    "# declare variables\n",
    "evaluation_qs = None\n",
    "record_count = -1\n",
    "record_counter = -1\n",
    "current_record = None\n",
    "tag_to_count_map = {}\n",
    "tag_qs = None\n",
    "tag_list = None\n",
    "current_tag = \"\"\n",
    "cleaned_tag = \"\"\n",
    "current_count = -1\n",
    "tag_count = -1\n",
    "no_tags_list = []\n",
    "\n",
    "# get all evaluation records with label = \"prelim_month\" and event_type = \"remove_tags\".\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( event_type = \"remove_tags\" )\n",
    "\n",
    "# first, just make sure that worked.\n",
    "record_count = evaluation_qs.count()\n",
    "\n",
    "# Check count of articles retrieved.\n",
    "print( \"Got \" + str( record_count ) + \" evaluations records.\" )\n",
    "\n",
    "# loop over evaluations.\n",
    "no_tags_count = 0\n",
    "for current_record in evaluation_qs:\n",
    "\n",
    "    # get tags.\n",
    "    # current_article.tags.add( tag_value )\n",
    "    tag_qs = current_record.tags.all()\n",
    "    \n",
    "    # output the tags.\n",
    "    #print( \"- Tags for record \" + str( current_record.id ) + \" : \" + str( tag_qs ) )\n",
    "    \n",
    "    # count tags\n",
    "    tag_count = tag_qs.count()\n",
    "    \n",
    "    # got tags?\n",
    "    if ( tag_count > 0 ):\n",
    "    \n",
    "        # loop over tags.\n",
    "        for current_tag in tag_qs:\n",
    "\n",
    "            # standardize\n",
    "            cleaned_tag = str( current_tag )\n",
    "\n",
    "            # to lower case\n",
    "            cleaned_tag = cleaned_tag.lower()\n",
    "\n",
    "            # strip()\n",
    "            cleaned_tag = cleaned_tag.strip()\n",
    "\n",
    "            # in map?  Get current count.\n",
    "            current_count = 0\n",
    "            if ( cleaned_tag in tag_to_count_map ): \n",
    "\n",
    "                # It is in map - get counter for it.\n",
    "                current_count = tag_to_count_map.get( cleaned_tag, None )\n",
    "\n",
    "            #-- END check to see if tag in map --#\n",
    "\n",
    "            # increment count and store.\n",
    "            current_count += 1\n",
    "            tag_to_count_map[ cleaned_tag ] = current_count\n",
    "\n",
    "        #-- END loop over tags --#\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # increment no_tag_counter bby 1.\n",
    "        no_tags_list.append( current_record )\n",
    "        \n",
    "    #-- END check to see if tags or not --#\n",
    "    \n",
    "#-- END loop over records --#\n",
    "\n",
    "# output number of tagless evaluations\n",
    "no_tags_count = len( no_tags_list )\n",
    "print( \"--> Count of articles with no tags: {}\".format( str( no_tags_count ) ) )\n",
    "\n",
    "# output tags\n",
    "key_view = six.viewkeys( tag_to_count_map )\n",
    "tag_list = list( key_view )\n",
    "tag_list.sort()\n",
    "for tag_string in tag_list:\n",
    "\n",
    "    # print each tag and its count.\n",
    "    current_count = tag_to_count_map.get( tag_string, -1 )\n",
    "    print( \"- {} ( {} )\".format( str( tag_string ), str( current_count ) ) )\n",
    "    \n",
    "#-- END print tags. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "--> Count of articles with no tags: 188\n",
    "\n",
    "Got 428 evaluations records.\n",
    "\n",
    "- ambiguity ( 2 )\n",
    "- ambiguous ( 10 )\n",
    "- complex ( 23 )\n",
    "- complex_title ( 1 )\n",
    "- complex_titles ( 25 )\n",
    "- compound_attribution ( 1 )\n",
    "- compound_names ( 9 )\n",
    "- contributed_to ( 2 )\n",
    "- dictionary_error ( 20 )\n",
    "- disambiguation ( 4 )\n",
    "- editing_error ( 3 )\n",
    "- error ( 204 )\n",
    "- follow_on_attribution ( 24 )\n",
    "- foreign_names ( 1 )\n",
    "- gender_confusion ( 2 )\n",
    "- initials ( 5 )\n",
    "- interesting ( 198 )\n",
    "- layout_or_design ( 3 )\n",
    "- list ( 14 )\n",
    "- lists ( 1 )\n",
    "- lookup ( 5 )\n",
    "- no_html ( 5 )\n",
    "- non_news ( 12 )\n",
    "- possessive ( 1 )\n",
    "- pronoun_attribution ( 2 )\n",
    "- pronouns ( 4 )\n",
    "- proper_noun ( 2 )\n",
    "- proper_nouns ( 38 )\n",
    "- quote_distance ( 12 )\n",
    "- said_verb ( 29 )\n",
    "- second_hand ( 10 )\n",
    "- short_n-gram ( 5 )\n",
    "- software_error ( 1 )\n",
    "- spanish ( 1 )\n",
    "- sports ( 6 )\n",
    "- straightforward ( 74 )\n",
    "- title_prefix ( 8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tags to create:\n",
    "\n",
    "- is_ambiguous = ambiguous, ambiguity\n",
    "- --> is_attribution_compound = compound_attribution\n",
    "- --> is_attribution_follow_on = follow_on_attribution\n",
    "- --> is_attribution_pronoun = pronoun_attribution\n",
    "- --> is_attribution_second_hand = second_hand\n",
    "- is_complex = complex\n",
    "- --> is_compound_names = compound_names\n",
    "- --> is_contributed_to (and is_subject_shb_author) = contributed_to\n",
    "- --> is_dictionary_error = dictionary_error\n",
    "- --> is_disambiguation = disambiguation\n",
    "- --> is_editing_error = editing_error\n",
    "- is_error = error\n",
    "- --> is_foreign_names = foreign_names\n",
    "- --> is_gender_confusion = gender_confusion\n",
    "- --> is_initials_error = initials\n",
    "- is_interesting = interesting\n",
    "- --> is_layout_or_design = layout_or_design\n",
    "- is_list = list, lists\n",
    "- --> is_lookup_error = lookup\n",
    "- is_no_html = no_html\n",
    "- is_not_hard_news = non_news\n",
    "- --> is_possessive = possessive\n",
    "- --> is_pronouns = pronouns\n",
    "- --> is_proper_noun (and is_not_a_person) = proper_noun, proper_nouns\n",
    "- --> is_quote_distance = quote_distance\n",
    "- --> is_said_verb = said_verb\n",
    "- --> is_short_n_gram = short_n-gram\n",
    "- --> is_software_error = software_error\n",
    "- --> is_spanish = spanish\n",
    "- is_sports = sports\n",
    "- is_straightforward = straightforward\n",
    "- is_title = complex_title, complex_titles, title_prefix\n",
    "- is_title_complex = complex_title, complex_titles\n",
    "- is_title_prefix = title_prefix\n",
    "\n",
    "Remember:\n",
    "\n",
    "- If error (\"-->\"), make sure to set \"is_error\", as well.\n",
    "- And, \"is_automated_error\" if automated error.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- verified that all fields are in model.\n",
    "- admin:\n",
    "\n",
    "    - make sure that all fields are in admin.\n",
    "    - reorganizing to make it a little less crazy.\n",
    "    - make sure all are in limit list.\n",
    "    \n",
    "- Need to automatically set the flags based on the tag values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up meta-data on fields, tags, and how they relate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T19:38:12.315780Z",
     "start_time": "2018-02-20T19:38:12.286761Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONSTANTS-ish - names of boolean model fields.\n",
    "FIELD_NAME_IS_AMBIGUOUS = \"is_ambiguous\"\n",
    "FIELD_NAME_IS_ATTRIBUTION_COMPOUND = \"is_attribution_compound\"\n",
    "FIELD_NAME_IS_ATTRIBUTION_FOLLOW_ON = \"is_attribution_follow_on\"\n",
    "FIELD_NAME_IS_ATTRIBUTION_PRONOUN = \"is_attribution_pronoun\"\n",
    "FIELD_NAME_IS_ATTRIBUTION_SECOND_HAND = \"is_attribution_second_hand\"\n",
    "FIELD_NAME_IS_COMPLEX = \"is_complex\"\n",
    "FIELD_NAME_IS_COMPOUND_NAMES = \"is_compound_names\"\n",
    "FIELD_NAME_IS_CONTRIBUTED_TO = \"is_contributed_to\"\n",
    "FIELD_NAME_IS_DICTIONARY_ERROR = \"is_dictionary_error\"\n",
    "FIELD_NAME_IS_DISAMBIGUATION = \"is_disambiguation\"\n",
    "FIELD_NAME_IS_EDITING_ERROR = \"is_editing_error\"\n",
    "FIELD_NAME_IS_ERROR = \"is_error\"\n",
    "FIELD_NAME_IS_FOREIGN_NAMES = \"is_foreign_names\"\n",
    "FIELD_NAME_IS_GENDER_CONFUSION = \"is_gender_confusion\"\n",
    "FIELD_NAME_IS_INITIALS_ERROR = \"is_initials_error\"\n",
    "FIELD_NAME_IS_INTERESTING = \"is_interesting\"\n",
    "FIELD_NAME_IS_LAYOUT_OR_DESIGN = \"is_layout_or_design\"\n",
    "FIELD_NAME_IS_LIST = \"is_list\"\n",
    "FIELD_NAME_IS_LOOKUP_ERROR = \"is_lookup_error\"\n",
    "FIELD_NAME_IS_NO_HTML = \"is_no_html\"\n",
    "FIELD_NAME_IS_NOT_HARD_NEWS = \"is_not_hard_news\"\n",
    "FIELD_NAME_IS_POSSESSIVE = \"is_possessive\"\n",
    "FIELD_NAME_IS_PRONOUNS = \"is_pronouns\"\n",
    "FIELD_NAME_IS_PROPER_NOUN = \"is_proper_noun\"\n",
    "FIELD_NAME_IS_QUOTE_DISTANCE = \"is_quote_distance\"\n",
    "FIELD_NAME_IS_SAID_VERB = \"is_said_verb\"\n",
    "FIELD_NAME_IS_SHORT_N_GRAM = \"is_short_n_gram\"\n",
    "FIELD_NAME_IS_SOFTWARE_ERROR = \"is_software_error\"\n",
    "FIELD_NAME_IS_SPANISH = \"is_spanish\"\n",
    "FIELD_NAME_IS_SPORTS = \"is_sports\"\n",
    "FIELD_NAME_IS_STRAIGHTFORWARD = \"is_straightforward\"\n",
    "FIELD_NAME_IS_TITLE = \"is_title\"\n",
    "FIELD_NAME_IS_TITLE_COMPLEX = \"is_title_complex\"\n",
    "FIELD_NAME_IS_TITLE_PREFIX = \"is_title_prefix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T19:38:15.748381Z",
     "start_time": "2018-02-20T19:38:15.743281Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CONSTANTS-ish - other related field names.\n",
    "FIELD_NAME_IS_SUBJECT_SHB_AUTHOR = \"is_subject_shb_author\"\n",
    "FIELD_NAME_IS_NOT_A_PERSON = \"is_not_a_person\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T20:30:47.203984Z",
     "start_time": "2018-02-20T20:30:47.196303Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONSTANTS-ish - names of properties per field.\n",
    "PROP_NAME = \"name\"\n",
    "PROP_TAG_LIST = \"tag_list\"\n",
    "PROP_IS_ERROR = \"is_error\"\n",
    "PROP_ASSOCIATED_FIELDS = \"associated_fields\"\n",
    "\n",
    "# CONSTANTS-ish - map of field names to field traits.\n",
    "FIELD_NAME_TO_TRAITS_MAP = {}\n",
    "\n",
    "# CONSTANTS-ish - map tag values to field names.\n",
    "TAG_TO_FIELD_NAME_MAP = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T20:30:50.151381Z",
     "start_time": "2018-02-20T20:30:48.906901Z"
    }
   },
   "outputs": [],
   "source": [
    "# set up mapping of field names to traits.\n",
    "temp_traits_map = {}\n",
    "\n",
    "# FIELD_NAME_IS_AMBIGUOUS = \"is_ambiguous\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_AMBIGUOUS\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'ambiguous', 'ambiguity' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = []\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_ATTRIBUTION_COMPOUND = \"is_attribution_compound\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_ATTRIBUTION_COMPOUND\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'compound_attribution' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_ATTRIBUTION_FOLLOW_ON = \"is_attribution_follow_on\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_ATTRIBUTION_FOLLOW_ON\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'follow_on_attribution' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_ATTRIBUTION_PRONOUN = \"is_attribution_pronoun\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_ATTRIBUTION_PRONOUN\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'pronoun_attribution' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_ATTRIBUTION_SECOND_HAND = \"is_attribution_second_hand\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_ATTRIBUTION_SECOND_HAND\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'second_hand' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_COMPLEX = \"is_complex\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_COMPLEX\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'complex' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = []\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_COMPOUND_NAMES = \"is_compound_names\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_COMPOUND_NAMES\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'compound_names' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_CONTRIBUTED_TO = \"is_contributed_to\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_CONTRIBUTED_TO\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'contributed_to' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR, FIELD_NAME_IS_SUBJECT_SHB_AUTHOR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_DICTIONARY_ERROR = \"is_dictionary_error\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_DICTIONARY_ERROR\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'dictionary_error' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_DISAMBIGUATION = \"is_disambiguation\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_DISAMBIGUATION\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'disambiguation' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_EDITING_ERROR = \"is_editing_error\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_EDITING_ERROR\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'editing_error' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_ERROR = \"is_error\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_ERROR\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'error' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = []\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_FOREIGN_NAMES = \"is_foreign_names\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_FOREIGN_NAMES\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'foreign_names' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_GENDER_CONFUSION = \"is_gender_confusion\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_GENDER_CONFUSION\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'gender_confusion' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_INITIALS_ERROR = \"is_initials_error\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_INITIALS_ERROR\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'initials' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_INTERESTING = \"is_interesting\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_INTERESTING\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'interesting' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = []\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_LAYOUT_OR_DESIGN = \"is_layout_or_design\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_LAYOUT_OR_DESIGN\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'layout_or_design' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_LIST = \"is_list\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_LIST\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'list', 'lists' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = []\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_LOOKUP_ERROR = \"is_lookup_error\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_LOOKUP_ERROR\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'lookup' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_NO_HTML = \"is_no_html\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_NO_HTML\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'no_html' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = []\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_NOT_HARD_NEWS = \"is_not_hard_news\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_NOT_HARD_NEWS\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'non_news' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = []\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_POSSESSIVE = \"is_possessive\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_POSSESSIVE\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'possessive' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_PRONOUNS = \"is_pronouns\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_PRONOUNS\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'pronouns' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_PROPER_NOUN = \"is_proper_noun\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_PROPER_NOUN\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'proper_noun', 'proper_nouns' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR, FIELD_NAME_IS_NOT_A_PERSON ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_QUOTE_DISTANCE = \"is_quote_distance\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_QUOTE_DISTANCE\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'quote_distance' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_SAID_VERB = \"is_said_verb\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_SAID_VERB\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'said_verb' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_SHORT_N_GRAM = \"is_short_n_gram\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_SHORT_N_GRAM\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'short_n-gram' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_SOFTWARE_ERROR = \"is_software_error\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_SOFTWARE_ERROR\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'software_error' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_SPANISH = \"is_spanish\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_SPANISH\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'spanish' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_SPORTS = \"is_sports\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_SPORTS\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'sports' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = []\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_STRAIGHTFORWARD = \"is_straightforward\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_STRAIGHTFORWARD\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'straightforward' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = []\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_TITLE = \"is_title\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_TITLE\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'complex_title', 'complex_titles', 'title_prefix' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_TITLE_COMPLEX = \"is_title_complex\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_TITLE_COMPLEX\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'complex_title', 'complex_titles' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map\n",
    "\n",
    "# FIELD_NAME_IS_TITLE_PREFIX = \"is_title_prefix\"\n",
    "temp_traits_map = {}\n",
    "field_name = FIELD_NAME_IS_TITLE_PREFIX\n",
    "temp_traits_map[ PROP_NAME ] = field_name\n",
    "temp_traits_map[ PROP_TAG_LIST ] = [ 'title_prefix' ]\n",
    "temp_traits_map[ PROP_ASSOCIATED_FIELDS ] = [ FIELD_NAME_IS_ERROR ]\n",
    "FIELD_NAME_TO_TRAITS_MAP[ field_name ] = temp_traits_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T20:30:55.682601Z",
     "start_time": "2018-02-20T20:30:55.660015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map of tags to field names in TAG_TO_FIELD_NAME_MAP: {'pronouns': ['is_pronouns'], 'title_prefix': ['is_title_prefix', 'is_title'], 'spanish': ['is_spanish'], 'proper_noun': ['is_proper_noun'], 'complex_title': ['is_title_complex', 'is_title'], 'lookup': ['is_lookup_error'], 'ambiguity': ['is_ambiguous'], 'straightforward': ['is_straightforward'], 'short_n-gram': ['is_short_n_gram'], 'layout_or_design': ['is_layout_or_design'], 'sports': ['is_sports'], 'pronoun_attribution': ['is_attribution_pronoun'], 'contributed_to': ['is_contributed_to'], 'proper_nouns': ['is_proper_noun'], 'quote_distance': ['is_quote_distance'], 'possessive': ['is_possessive'], 'said_verb': ['is_said_verb'], 'compound_names': ['is_compound_names'], 'compound_attribution': ['is_attribution_compound'], 'follow_on_attribution': ['is_attribution_follow_on'], 'complex': ['is_complex'], 'non_news': ['is_not_hard_news'], 'list': ['is_list'], 'interesting': ['is_interesting'], 'ambiguous': ['is_ambiguous'], 'foreign_names': ['is_foreign_names'], 'lists': ['is_list'], 'gender_confusion': ['is_gender_confusion'], 'no_html': ['is_no_html'], 'software_error': ['is_software_error'], 'error': ['is_error'], 'disambiguation': ['is_disambiguation'], 'initials': ['is_initials_error'], 'dictionary_error': ['is_dictionary_error'], 'complex_titles': ['is_title_complex', 'is_title'], 'second_hand': ['is_attribution_second_hand'], 'editing_error': ['is_editing_error']}\n"
     ]
    }
   ],
   "source": [
    "# set up mapping of tag values to field names in TAG_TO_FIELD_NAME_MAP.\n",
    "\n",
    "# declare variables\n",
    "current_field_name = None\n",
    "current_traits = None\n",
    "tag_list = None\n",
    "current_tag = None\n",
    "\n",
    "# loop over things in FIELD_NAME_TO_TRAITS_MAP.\n",
    "for current_field_name in six.iterkeys( FIELD_NAME_TO_TRAITS_MAP ):\n",
    "    \n",
    "    # get traits dictionary for field name.\n",
    "    current_traits = FIELD_NAME_TO_TRAITS_MAP.get( current_field_name, None )\n",
    "    \n",
    "    # retrieve tag list for field.\n",
    "    tag_list = current_traits.get( PROP_TAG_LIST )\n",
    "    for current_tag in tag_list:\n",
    "        \n",
    "        # get list of fields for tag\n",
    "        tag_field_list = TAG_TO_FIELD_NAME_MAP.get( current_tag, [] )\n",
    "        \n",
    "        # append current field.\n",
    "        tag_field_list.append( current_field_name )\n",
    "        \n",
    "        # put list back.\n",
    "        TAG_TO_FIELD_NAME_MAP[ current_tag ] = tag_field_list\n",
    "        \n",
    "    #-- END loop over tags for a given field --#\n",
    "    \n",
    "#-- END loop over field names. --#\n",
    "\n",
    "print( \"Map of tags to field names in TAG_TO_FIELD_NAME_MAP: {}\".format( str( TAG_TO_FIELD_NAME_MAP ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set booleans based on tags - first, see if tag is mapped to a field, then, if so, look up field traits to figure out what booleans to set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CURRENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T20:33:18.038720Z",
     "start_time": "2018-02-20T20:33:16.375946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 428 evaluations records.\n",
      "Completed at 2018-02-20 20:33:18.034532\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "django-taggit documentation: https://github.com/alex/django-taggit\n",
    "\n",
    "Adding tags to a model:\n",
    "\n",
    "    from django.db import models\n",
    "    \n",
    "    from taggit.managers import TaggableManager\n",
    "    \n",
    "    class Food(models.Model):\n",
    "        # ... fields here\n",
    "    \n",
    "        tags = TaggableManager()\n",
    "\n",
    "Interacting with a model that has tags:\n",
    "\n",
    "    >>> apple = Food.objects.create(name=\"apple\")\n",
    "    >>> apple.tags.add(\"red\", \"green\", \"delicious\")\n",
    "    >>> apple.tags.all()\n",
    "    [<Tag: red>, <Tag: green>, <Tag: delicious>]\n",
    "    >>> apple.tags.remove(\"green\")\n",
    "    >>> apple.tags.all()\n",
    "    [<Tag: red>, <Tag: delicious>]\n",
    "    >>> Food.objects.filter(tags__name__in=[\"red\"])\n",
    "    [<Food: apple>, <Food: cherry>]\n",
    "    \n",
    "    # include only those with certain tags.\n",
    "    #tags_in_list = [ \"prelim_unit_test_001\", \"prelim_unit_test_002\", \"prelim_unit_test_003\", \"prelim_unit_test_004\", \"prelim_unit_test_005\", \"prelim_unit_test_006\", \"prelim_unit_test_007\" ]\n",
    "    tags_in_list = [ \"grp_month\", ]\n",
    "    if ( len( tags_in_list ) > 0 ):\n",
    "    \n",
    "        # filter\n",
    "        print( \"filtering to just articles with tags: \" + str( tags_in_list ) )\n",
    "        grp_article_qs = grp_article_qs.filter( tags__name__in = tags_in_list )\n",
    "        \n",
    "    #-- END check to see if we have a specific list of tags we want to include --#\n",
    "\n",
    "'''\n",
    "\n",
    "# imports\n",
    "from sourcenet_analysis.models import Reliability_Names_Evaluation\n",
    "\n",
    "# declare variables\n",
    "evaluation_qs = None\n",
    "record_count = -1\n",
    "record_counter = -1\n",
    "current_record = None\n",
    "tag_set = set()\n",
    "tag_qs = None\n",
    "tag_list = None\n",
    "current_tag = \"\"\n",
    "cleaned_tag = \"\"\n",
    "field_name_list = None\n",
    "current_field_name = None\n",
    "current_traits = None\n",
    "related_field_name_list = None\n",
    "related_field_name = None\n",
    "do_save = True\n",
    "\n",
    "# get all evaluation records with label = \"prelim_month\" and event_type = \"remove_tags\".\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( event_type = \"remove_tags\" )\n",
    "\n",
    "# first, just make sure that worked.\n",
    "record_count = evaluation_qs.count()\n",
    "\n",
    "# Check count of articles retrieved.\n",
    "print( \"Got \" + str( record_count ) + \" evaluations records.\" )\n",
    "\n",
    "# loop over evaluations.\n",
    "for current_record in evaluation_qs:\n",
    "\n",
    "    # get tags.\n",
    "    # current_article.tags.add( tag_value )\n",
    "    tag_qs = current_record.tags.all()\n",
    "    \n",
    "    # output the tags.\n",
    "    #print( \"- Tags for record \" + str( current_record.id ) + \" : \" + str( tag_qs ) )\n",
    "    \n",
    "    # loop over tags.\n",
    "    for current_tag in tag_qs:\n",
    "        \n",
    "        # standardize\n",
    "        cleaned_tag = str( current_tag )\n",
    "\n",
    "        # to lower case\n",
    "        cleaned_tag = cleaned_tag.lower()\n",
    "        \n",
    "        # strip()\n",
    "        cleaned_tag = cleaned_tag.strip()\n",
    "        \n",
    "        # First, try to retrieve field name for current tag.\n",
    "        field_name_list = TAG_TO_FIELD_NAME_MAP.get( cleaned_tag, None )\n",
    "        \n",
    "        # got a field name?\n",
    "        if ( field_name_list is not None ):\n",
    "            \n",
    "            # loop over items in list.\n",
    "            for current_field_name in field_name_list:\n",
    "\n",
    "                # set field to True.\n",
    "                setattr( current_record, current_field_name, True )\n",
    "\n",
    "                # retrieve field's traits.\n",
    "                current_traits = FIELD_NAME_TO_TRAITS_MAP.get( current_field_name, None )\n",
    "\n",
    "                # get list of related fields\n",
    "                related_field_name_list = current_traits.get( PROP_ASSOCIATED_FIELDS, None )\n",
    "\n",
    "                # got anything?\n",
    "                if ( ( related_field_name_list is not None ) and ( len( related_field_name_list ) > 0 ) ):\n",
    "\n",
    "                    # yes - set related fields to True, also.\n",
    "                    for related_field_name in related_field_name_list:\n",
    "\n",
    "                        # set field to True.\n",
    "                        setattr( current_record, related_field_name, True )\n",
    "\n",
    "                    #-- END loop over related field names. --#\n",
    "\n",
    "                #-- END check to see if any related fields. --#\n",
    "                \n",
    "            #-- END loop over related fields. --#\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Unknown tag!\n",
    "            print( \"!!!! Unknown tag: {}\".format( cleaned_tag ) )\n",
    "            \n",
    "        #-- END check to see what tag we are processing. --#\n",
    "        \n",
    "    #-- END loop over tags --#\n",
    "    \n",
    "    # are we saving the results of this grand endeavour?\n",
    "    if ( do_save == True ):\n",
    "        \n",
    "        # do save.\n",
    "        current_record.save()\n",
    "        \n",
    "    #-- END check to see if saving. --#\n",
    "    \n",
    "#-- END loop over records --#\n",
    "\n",
    "# output\n",
    "print( \"Completed at {}\".format( str( datetime.datetime.now() ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T20:33:22.683633Z",
     "start_time": "2018-02-20T20:33:22.598409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- field is_ambiguous count: 67\n",
      "- field is_attribution_compound count: 1\n",
      "- field is_attribution_follow_on count: 24\n",
      "- field is_attribution_pronoun count: 2\n",
      "- field is_attribution_second_hand count: 10\n",
      "- field is_complex count: 23\n",
      "- field is_compound_names count: 9\n",
      "- field is_contributed_to count: 2\n",
      "- field is_dictionary_error count: 20\n",
      "- field is_disambiguation count: 4\n",
      "- field is_editing_error count: 3\n",
      "- field is_error count: 221\n",
      "- field is_foreign_names count: 1\n",
      "- field is_gender_confusion count: 2\n",
      "- field is_initials_error count: 5\n",
      "- field is_interesting count: 198\n",
      "- field is_layout_or_design count: 3\n",
      "- field is_list count: 15\n",
      "- field is_lookup_error count: 5\n",
      "- field is_no_html count: 5\n",
      "- field is_not_hard_news count: 16\n",
      "- field is_possessive count: 1\n",
      "- field is_pronouns count: 4\n",
      "- field is_proper_noun count: 40\n",
      "- field is_quote_distance count: 12\n",
      "- field is_said_verb count: 29\n",
      "- field is_short_n_gram count: 5\n",
      "- field is_software_error count: 1\n",
      "- field is_spanish count: 1\n",
      "- field is_sports count: 6\n",
      "- field is_straightforward count: 74\n",
      "- field is_title count: 33\n",
      "- field is_title_complex count: 26\n",
      "- field is_title_prefix count: 8\n"
     ]
    }
   ],
   "source": [
    "# Generate counts for each field.\n",
    "\n",
    "# declare variables\n",
    "field_names_view = None\n",
    "field_names_list = None\n",
    "current_field_name = None\n",
    "my_kwargs = None\n",
    "kwarg_name = None\n",
    "kwarg_value = None\n",
    "evaluation_qs = None\n",
    "filtered_qs = None\n",
    "filtered_count = -1\n",
    "\n",
    "# first, get all evaluation instances with label = \"prelim_month\" and event type \"remove_tags\".\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( event_type = \"remove_tags\" )\n",
    "\n",
    "# get view of keys.\n",
    "field_names_view = six.viewkeys( FIELD_NAME_TO_TRAITS_MAP )\n",
    "\n",
    "# convert to sorted list\n",
    "field_names_list = list( field_names_view )\n",
    "field_names_list.sort()\n",
    "\n",
    "# loop over things in FIELD_NAME_TO_TRAITS_MAP.\n",
    "for current_field_name in field_names_list:\n",
    "    \n",
    "    #print( \"current field name: {}\".format( current_field_name ) )\n",
    "    \n",
    "    # filter and count records where the current field is True.\n",
    "    my_kwargs = {}\n",
    "    kwarg_name = current_field_name\n",
    "    kwarg_value = True\n",
    "    my_kwargs[ kwarg_name ] = kwarg_value\n",
    "    #print( my_kwargs )\n",
    "    \n",
    "    # filter.\n",
    "    filtered_qs = evaluation_qs.filter( **my_kwargs )\n",
    "    \n",
    "    # count\n",
    "    filtered_count = filtered_qs.count()\n",
    "\n",
    "    # output\n",
    "    print( \"- field {} count: {}\".format( current_field_name, str( filtered_count ) ) )\n",
    "    \n",
    "#-- END loop over field names. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| field name | field count | tag count |\n",
    "|------|------|------|\n",
    "| field is_ambiguous count | 67 | 12 |\n",
    "| field is_attribution_compound count | 1 | 1 |\n",
    "| field is_attribution_follow_on count | 24 | 24 |\n",
    "| field is_attribution_pronoun count | 2 | 2 |\n",
    "| field is_attribution_second_hand count | 10 | 10 |\n",
    "| field is_complex count | 23 | 23 |\n",
    "| field is_compound_names count | 9 | 9 |\n",
    "| field is_contributed_to count | 2 | 2 |\n",
    "| field is_dictionary_error count | 20 | 20 |\n",
    "| field is_disambiguation count | 4 | 4 |\n",
    "| field is_editing_error count | 3 | 3 |\n",
    "| field is_error count | 221 | 204 |\n",
    "| field is_foreign_names count | 1 | 1 |\n",
    "| field is_gender_confusion count | 2 | 2 |\n",
    "| field is_initials_error count | 5 | 5 |\n",
    "| field is_interesting count | 198 | 198 |\n",
    "| field is_layout_or_design count | 3 | 3 |\n",
    "| field is_list count | 15 | 15 |\n",
    "| field is_lookup_error count | 5 | 5 |\n",
    "| field is_no_html count | 5 | 5 |\n",
    "| field is_not_hard_news count | 16 | 12 |\n",
    "| field is_possessive count | 1 | 1 |\n",
    "| field is_pronouns count | 4 | 4 |\n",
    "| field is_proper_noun count | 40 | 40 |\n",
    "| field is_quote_distance count | 12 | 12 |\n",
    "| field is_said_verb count | 29 | 29 |\n",
    "| field is_short_n_gram count | 5 | 5 |\n",
    "| field is_software_error count | 1 | 1 |\n",
    "| field is_spanish count | 1 | 1 |\n",
    "| field is_sports count | 6 | 6 |\n",
    "| field is_straightforward count | 74 | 74 |\n",
    "| field is_title count | 33 | 34 |\n",
    "| field is_title_complex count | 26 | 26 |\n",
    "| field is_title_prefix count | 8 | 8 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth coding fixed\n",
    "\n",
    "- For some, the error will be on the part of the human coder.  For human error, we create a new \"`ground_truth`\" record that we will correct, so we preserve original coding (and evidence of errors) in case we want or need that information later.  Below, we have a table of the articles where we had to fix ground truth.  To find the original coding, click the Article link.\n",
    "- Denoted by records with \"`is_ground_truth_fixed`\" set to True in the `Reliability_Names_Evaluation` table in django:  [http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?is_ground_truth_fixed__exact=1&label=prelim_month&o=-1.7.8.3.5](http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?is_ground_truth_fixed__exact=1&label=prelim_month&o=-1.7.8.3.5)\n",
    "- 130 total (130/415 = 31.3% - this is a lot - is this right?)\n",
    "\n",
    "    - 4 duplicates, so call it 126.\n",
    "    - based on \"`prelim_month_human`\" `Reliability_Names` tag, 135 disagreements between original and corrected coding.  Probably some merging needed here?\n",
    "    - of those, 4 are same person and article, but different `Reliability_Names` record, so disagreements that had to be corrected twice because of rebuilding `Reliability_Names` for the article (either human error, or something else weird).  SQL:\n",
    "\n",
    "            SELECT sarne1.person_name,\n",
    "                sarne1.id,\n",
    "                sarne1.status,\n",
    "                sarne1.original_reliability_names_id,\n",
    "                sarne1.article_id,\n",
    "                sarne1.is_duplicate,\n",
    "                sarne2.is_duplicate,\n",
    "                sarne2.id,\n",
    "                sarne2.status,\n",
    "                sarne2.original_reliability_names_id,\n",
    "                sarne2.article_id\n",
    "            FROM sourcenet_analysis_reliability_names_evaluation AS sarne1,\n",
    "                sourcenet_analysis_reliability_names_evaluation AS sarne2\n",
    "            WHERE sarne1.id != sarne2.id\n",
    "                AND sarne1.label = 'prelim_month'\n",
    "                AND sarne2.label = 'prelim_month'\n",
    "                AND sarne1.event_type = 'remove_tags'\n",
    "                AND sarne2.event_type = 'remove_tags'\n",
    "                AND sarne1.is_ground_truth_fixed = TRUE\n",
    "                AND sarne2.is_ground_truth_fixed = TRUE\n",
    "                AND sarne1.article_id = sarne2.article_id\n",
    "                AND sarne1.person_name = sarne2.person_name\n",
    "                AND sarne1.original_reliability_names_id != sarne2.original_reliability_names_id\n",
    "                AND sarne2.original_reliability_names_id > sarne1.original_reliability_names_id\n",
    "            ORDER BY sarne1.id ASC;\n",
    "\n",
    "        Results (looks like these are ones that had to be merged, so ... minimze - when ambiguity, assume error in creating data, treat as duplicates so reduce count by number of duplicates):\n",
    "\n",
    "| person_name | id | status | original_reliability_names_id | article_id | id | status | original_reliability_names_id | article_id |\n",
    "|--|--|--|--|--|--|--|--|--|\n",
    "| Jeff Hawkins | 33 | ERROR | 9408 | [21007](https://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=21007) | 34 | ERROR | 9414 | 21007 |\n",
    "| Fritz Wahlfield | 405 | ERROR | 10330 | [22415](https://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=22415) | 407 | CORRECT | 10997 | 22415 |\n",
    "| John Agar | 610 | ERROR | 8917 | [23904](https://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23904) | 611 | ERROR | 8918 | 23904 |\n",
    "| Rachael Recker | 620 | ERROR | 8968 | [23920](https://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23920) | 619 | ERROR | 8976 | 23920 |\n",
    "\n",
    "- work:\n",
    "\n",
    "    - set all with \"is_ground_truth_fixed\" = True so \"is_to_do\" = True and \"`work_status = \"metadata_review\"`\".\n",
    "    - update all with `is_ground_truth_updated = True` so `is_human_error = True`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mark all ground truth updates as TODO\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T20:56:17.519327Z",
     "start_time": "2018-01-30T20:56:17.251741Z"
    }
   },
   "outputs": [],
   "source": [
    "# get all evaluation records with label = \"prelim_month\", is_ground_truth_fixed = True, and event_type = \"remove_tags\".\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( is_ground_truth_fixed = True )\n",
    "evaluation_qs = evaluation_qs.filter( event_type = \"remove_tags\" )\n",
    "\n",
    "# count?\n",
    "eval_count = evaluation_qs.count()\n",
    "print( \"record count: {}\".format( str( eval_count ) ) )\n",
    "\n",
    "# loop, setting \"is_to_do\" to True on each and saving.\n",
    "for current_eval in evaluation_qs:\n",
    "    \n",
    "    # set is_to_do to True and set work_status to \"metadata_review\".\n",
    "    current_eval.is_to_do = True\n",
    "    current_eval.work_status = \"metadata_review\"\n",
    "    current_eval.save()\n",
    "    \n",
    "#-- END loop over QuerySet. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mark all ground truth updates as human error\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T19:05:58.911127Z",
     "start_time": "2018-02-03T19:05:58.634517Z"
    }
   },
   "outputs": [],
   "source": [
    "# get all evaluation records with label = \"prelim_month\", is_ground_truth_fixed = True, and event_type = \"remove_tags\".\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( is_ground_truth_fixed = True )\n",
    "evaluation_qs = evaluation_qs.filter( event_type = \"remove_tags\" )\n",
    "\n",
    "# count?\n",
    "eval_count = evaluation_qs.count()\n",
    "print( \"record count: {}\".format( str( eval_count ) ) )\n",
    "\n",
    "# loop, setting \"is_human_error\" to True on each and saving.\n",
    "for current_eval in evaluation_qs:\n",
    "    \n",
    "    # set is_to_do to True and set work_status to \"metadata_review\".\n",
    "    current_eval.is_human_error = True\n",
    "    current_eval.save()\n",
    "    \n",
    "#-- END loop over QuerySet. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count ground truth updates\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: go through all \"is_to_do\" and update the metadata booleans.\n",
    "    \n",
    "- Link: [https://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?is_to_do__exact=1](https://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?is_to_do__exact=1)\n",
    "- for duplicates, set \"`is_duplicate`\" to True on one of the two records.  If two records for a given person because of the computer finding a person and human missing them, mark the computer coder's evaluation record as the duplicate.\n",
    "- for skipped, not a human error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-07T02:42:53.757489Z",
     "start_time": "2018-02-07T02:42:53.746860Z"
    }
   },
   "outputs": [],
   "source": [
    "# get all evaluation records with:\n",
    "# - label = \"prelim_month\"\n",
    "# - is_ground_truth_fixed = True\n",
    "# - event_type = \"remove_tags\"\n",
    "# - is_duplicate = False\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( is_ground_truth_fixed = True )\n",
    "evaluation_qs = evaluation_qs.filter( event_type = \"remove_tags\" )\n",
    "# evaluation_qs = evaluation_qs.filter( is_to_do = True )  # 130 originally\n",
    "evaluation_qs = evaluation_qs.filter( is_duplicate = False )  # now 123!\n",
    "\n",
    "# count?\n",
    "eval_count = evaluation_qs.count()\n",
    "print( \"record count: {}\".format( str( eval_count ) ) )\n",
    "\n",
    "# 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: update counts of characterizations above.  When filtering for counts:\n",
    "    \n",
    "- filter out duplicates when generating counts (include only where \"`is_duplicate`\" = False).\n",
    "- filter out skipped when generating counts (include only where \"`is_skipped`\" = False).\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-07T02:49:14.149056Z",
     "start_time": "2018-02-07T02:49:14.134707Z"
    }
   },
   "outputs": [],
   "source": [
    "# get all evaluation records with:\n",
    "# - label = \"prelim_month\"\n",
    "# - is_ground_truth_fixed = True\n",
    "# - is_human_error = True\n",
    "# - event_type = \"remove_tags\"\n",
    "# - is_duplicate = False\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( is_ground_truth_fixed = True )\n",
    "evaluation_qs = evaluation_qs.filter( is_human_error = True )\n",
    "evaluation_qs = evaluation_qs.filter( event_type = \"remove_tags\" )\n",
    "# evaluation_qs = evaluation_qs.filter( is_to_do = True )  # 130 originally\n",
    "evaluation_qs = evaluation_qs.filter( is_duplicate = False )  # now 123!\n",
    "\n",
    "# count?\n",
    "eval_count = evaluation_qs.count()\n",
    "print( \"record count: {}\".format( str( eval_count ) ) )\n",
    "\n",
    "# 102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "- 130 total, 123 without duplicates, 102 if one removes those where human skipped a person because of limitation of the coding application (compound names).\n",
    "- 102/415 = 31.3% - this is a lot - is this right?)\n",
    "- number of affected articles?\n",
    "- characterization of the problems:\n",
    "\n",
    "    - is_missed_author:\n",
    "    - is_missed_subject:\n",
    "    - is_skipped (limitation of coding application):\n",
    "    - is_author_shb_subject: *\n",
    "    - is_subject_shb_author: *\n",
    "    - is_quoted_shb_mentioned: *\n",
    "    - is_mentioned_shb_quoted: *\n",
    "    - is_wrong_text_captured: *\n",
    "    - is_duplicate:\n",
    "    \n",
    "- note:\n",
    "\n",
    "    - evaluation 469 is both author and subject.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- work:\n",
    "\n",
    "    - figure out number of affected articles (should just be count of Article_Data by ground_truth user).\n",
    "    - update metadata for all disagreements (all \"`remove_tags`\" events)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability_Names records merged\n",
    "\n",
    "- For some, need to merge a single-name detection by Calais with full-name detection by ground_truth (an OpenCalais error - did not detect full name - combined with lookup error - didn't lookup the right person since missed part of his or her name).  Will still have subsequently deleted one or more duplicate rows.\n",
    "- Denoted by records with \"`event_type`\" set to \"merge\" in the Reliability_Names_Evaluation table in django: [http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=merge&label=prelim_month&o=-1.7.8.3.5](http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=merge&label=prelim_month&o=-1.7.8.3.5)\n",
    "- 18 total\n",
    "- TODO: need to check for the repeat thing SQL like in remove tags.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleted Reliability_Names records\n",
    "\n",
    "- Some records need to be deleted:\n",
    "\n",
    "    - are just broken, need to be deleted.\n",
    "    - cleanup after a merge (one stays, one goes).\n",
    "    - cleanup after rebuilding `Reliability_Names` (single names removed once again, etc.).\n",
    "    - ?\n",
    "\n",
    "- Denoted by records with \"`event_type`\" set to \"deleted\" in the `Reliability_Names_Evaluation` table in django: [http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=delete&label=prelim_month&o=-1.7.8.3.5](http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=delete&label=prelim_month&o=-1.7.8.3.5)\n",
    "- 181 total\n",
    "- TODO: need to provide examples.\n",
    "- TODO: need to check for the repeat thing SQL like in remove tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "\n",
    "- after correction:\n",
    "\n",
    "    - 2,446 overall coding decisions on people in \"`prelim_month`\".\n",
    "\n",
    "        - https://research.local/sourcenet/sourcenet/analysis/reliability/names/disagreement/view\n",
    "        - label: \"`prelim_month`\"\n",
    "        - coders to compare (1 ==>): 2\n",
    "        - Filter Type: \"Lookup\" (default)\n",
    "\n",
    "    - 294 disagreements in \"`prelim_month`\".\n",
    "    \n",
    "        - https://research.local/sourcenet/sourcenet/analysis/reliability/names/disagreement/view\n",
    "        - label: \"`prelim_month`\"\n",
    "        - coders to compare (1 ==>): 2\n",
    "        - Filter Type: \"Disagree\"\n",
    "\n",
    "    - 135 disagreements between humans and corrected.\n",
    "    \n",
    "        - https://research.local/sourcenet/sourcenet/analysis/reliability/names/disagreement/view\n",
    "        - label: \"`prelim_month_human`\"\n",
    "        - coders to compare (1 ==>): 2\n",
    "        - Filter Type: \"Disagree\"\n",
    "        - to more readily explore distribution of problems, added tag \"`prelim_month_human_disagree`\"to disagreements in \"`prelim_month_human`\":\n",
    "        \n",
    "            - all disagreements:\n",
    "            - label: \"`prelim_month_human`\"\n",
    "            - coders to compare (1 ==>): 2\n",
    "            - Filter Type: \"Lookup\"\n",
    "            - Reliability_Names tags: \"`prelim_month_human_disagree`\"\n",
    "            \n",
    "- TODO:\n",
    "\n",
    "    - look over human errors (disagreement between humans and corrected data).\n",
    "    \n",
    "        - Figure out types of error, counts of each type.  In SQL, filter on tag, then on other traits.\n",
    "        \n",
    "            - human miss: if coder 2 empty, 1 not, then human missed a person.\n",
    "            - human false detect: if coder 1 empty, 2 not, human erroneously detected person.\n",
    "            - \n",
    "            - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Disagreements - Human Error\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Human error - Per article (how many have ground truth?) and per decision?  How many errors, compared to total number of decisions, and what kind of errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disagreements - Computer Error\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Computer error - look over classes of error for trends (systemic error) and interesting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sourcenet (Python 3)",
   "language": "python",
   "name": "sourcenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "689px",
    "left": "0px",
    "right": "1118px",
    "top": "111px",
    "width": "322px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
