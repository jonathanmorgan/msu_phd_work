{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2018.02.09 - prelim - disagreement analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup---Imports\" data-toc-modified-id=\"Setup---Imports-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Setup - Imports</a></span></li><li><span><a href=\"#Setup---virtualenv-jupyter-kernel\" data-toc-modified-id=\"Setup---virtualenv-jupyter-kernel-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Setup - virtualenv jupyter kernel</a></span></li><li><span><a href=\"#Setup---Initialize-Django\" data-toc-modified-id=\"Setup---Initialize-Django-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Setup - Initialize Django</a></span></li></ul></li><li><span><a href=\"#Evaluating-disagreements\" data-toc-modified-id=\"Evaluating-disagreements-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Evaluating disagreements</a></span><ul class=\"toc-item\"><li><span><a href=\"#Disagreement-tracking-process\" data-toc-modified-id=\"Disagreement-tracking-process-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Disagreement tracking process</a></span></li><li><span><a href=\"#Data-in-Reliability_Names_Evaluation\" data-toc-modified-id=\"Data-in-Reliability_Names_Evaluation-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Data in <code>Reliability_Names_Evaluation</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Overall-disagreement-log\" data-toc-modified-id=\"Overall-disagreement-log-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Overall disagreement log</a></span><ul class=\"toc-item\"><li><span><a href=\"#flag-duplicates\" data-toc-modified-id=\"flag-duplicates-2.2.1.1\"><span class=\"toc-item-num\">2.2.1.1&nbsp;&nbsp;</span>flag duplicates</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#CURRENT\" data-toc-modified-id=\"CURRENT-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>CURRENT</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#review-tags\" data-toc-modified-id=\"review-tags-3.0.0.1\"><span class=\"toc-item-num\">3.0.0.1&nbsp;&nbsp;</span>review tags</a></span></li></ul></li><li><span><a href=\"#Ground-truth-coding-fixed\" data-toc-modified-id=\"Ground-truth-coding-fixed-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Ground truth coding fixed</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mark-all-ground-truth-updates-as-TODO\" data-toc-modified-id=\"Mark-all-ground-truth-updates-as-TODO-3.0.1.1\"><span class=\"toc-item-num\">3.0.1.1&nbsp;&nbsp;</span>Mark all ground truth updates as TODO</a></span></li><li><span><a href=\"#Mark-all-ground-truth-updates-as-human-error\" data-toc-modified-id=\"Mark-all-ground-truth-updates-as-human-error-3.0.1.2\"><span class=\"toc-item-num\">3.0.1.2&nbsp;&nbsp;</span>Mark all ground truth updates as human error</a></span></li><li><span><a href=\"#Count-ground-truth-updates\" data-toc-modified-id=\"Count-ground-truth-updates-3.0.1.3\"><span class=\"toc-item-num\">3.0.1.3&nbsp;&nbsp;</span>Count ground truth updates</a></span></li></ul></li><li><span><a href=\"#Reliability_Names-records-merged\" data-toc-modified-id=\"Reliability_Names-records-merged-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Reliability_Names records merged</a></span></li><li><span><a href=\"#Deleted-Reliability_Names-records\" data-toc-modified-id=\"Deleted-Reliability_Names-records-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Deleted Reliability_Names records</a></span></li></ul></li><li><span><a href=\"#Evaluating-Disagreements---Human-Error\" data-toc-modified-id=\"Evaluating-Disagreements---Human-Error-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Evaluating Disagreements - Human Error</a></span></li><li><span><a href=\"#Disagreements---Computer-Error\" data-toc-modified-id=\"Disagreements---Computer-Error-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Disagreements - Computer Error</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Imports\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T02:02:33.673689Z",
     "start_time": "2018-02-12T02:02:33.669660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages imported at 2018-02-12 02:02:33.671218\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import six\n",
    "\n",
    "print( \"packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - virtualenv jupyter kernel\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "If you are using a virtualenv, make sure that you:\n",
    "\n",
    "- have installed your virtualenv as a kernel.\n",
    "- choose the kernel for your virtualenv as the kernel for your notebook (Kernel --> Change kernel).\n",
    "\n",
    "Since I use a virtualenv, need to get that activated somehow inside this notebook.  One option is to run `../dev/wsgi.py` in this notebook, to configure the python environment manually as if you had activated the `sourcenet` virtualenv.  To do this, you'd make a code cell that contains:\n",
    "\n",
    "    %run ../dev/wsgi.py\n",
    "    \n",
    "This is sketchy, however, because of the changes it makes to your Python environment within the context of whatever your current kernel is.  I'd worry about collisions with the actual Python 3 kernel.  Better, one can install their virtualenv as a separate kernel.  Steps:\n",
    "\n",
    "- activate your virtualenv:\n",
    "\n",
    "        workon sourcenet\n",
    "\n",
    "- in your virtualenv, install the package `ipykernel`.\n",
    "\n",
    "        pip install ipykernel\n",
    "\n",
    "- use the ipykernel python program to install the current environment as a kernel:\n",
    "\n",
    "        python -m ipykernel install --user --name <env_name> --display-name \"<display_name>\"\n",
    "        \n",
    "    `sourcenet` example:\n",
    "    \n",
    "        python -m ipykernel install --user --name sourcenet --display-name \"sourcenet (Python 3)\"\n",
    "        \n",
    "More details: [http://ipython.readthedocs.io/en/stable/install/kernel_install.html](http://ipython.readthedocs.io/en/stable/install/kernel_install.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T02:02:39.773245Z",
     "start_time": "2018-02-12T02:02:39.767163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jonathanmorgan/work/sourcenet/django/research/work/msu_phd_work/methods'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Initialize Django\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, initialize my dev django project, so I can run code in this notebook that references my django models and can talk to the database using my project's settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T02:02:42.407194Z",
     "start_time": "2018-02-12T02:02:42.399409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "django initialized at 2018-02-12 02:02:42.404986\n"
     ]
    }
   ],
   "source": [
    "%run django_init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T02:02:44.537563Z",
     "start_time": "2018-02-12T02:02:44.534726Z"
    }
   },
   "outputs": [],
   "source": [
    "# django imports\n",
    "from sourcenet_analysis.models import Reliability_Names_Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating disagreements\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Look at stats for disagreements and evaluation, including human and computer errors.\n",
    "\n",
    "- Notebook with work details: [2017.07.01-work_log-prelim_month-evaluate_disagreements.ipynb](https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/methods/2017.07.01-work_log-prelim_month-evaluate_disagreements.ipynb)\n",
    "\n",
    "Process: Look at each instance where there is a disagreement and make sure the human coding is correct.\n",
    "\n",
    "Most are probably instances where the computer screwed up, but since we are calling this human coding \"ground truth\", want to winnow out as much human error as possible.\n",
    "\n",
    "For each disagreement, to check for coder error (like just capturing a name part for a person whose full name was in the story), click the \"Article ID\" in the column that has a link to article ID. It will take you to a view of the article where all the people who coded the article are included, with each detection of a mention or quotation displayed next to the paragraph where the person was originally first detected.\n",
    "\n",
    "If not human error, remove TODO tag, filling in details on the diagreement in the record in `Reliability_Names_Evaluation` for the removal of the tag (details: [Disagreement tracking process](#Disagreement-tracking-process)).\n",
    "\n",
    "If human error:\n",
    "\n",
    "- 1) look at all the disagreements for the article.\n",
    "- 2) remove all TODO tags from all disagreements, and fill in details for each.\n",
    "- 3) follow steps below to create ground_truth copy and fix it.\n",
    "- 4) rebuild Reliability_Names for article and cleanup.\n",
    "- 5) then, do any deletes or merges you need to do, so you only do them once.\n",
    "\n",
    "Pull together some numbers and analysis from disagreement work:\n",
    "\n",
    "- counts of disagreements.\n",
    "- which were human and computer error.\n",
    "- ratio of human error to machine error.\n",
    "- proportion of human and machine errors.\n",
    "- overall number of disagreements compared to all decisions.\n",
    "- characterisation of potential for systematic issues (not as bad as I feared).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disagreement tracking process\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "From [2017.07.01-work_log-prelim_month-evaluate_disagreements.ipynb - Disagreement resolution](https://research.local:8000/user/jonathanmorgan/notebooks/work/sourcenet/django/research/work/msu_phd_work/methods/2017.07.01-work_log-prelim_month-evaluate_disagreements.ipynb#Disagreement-resolution):\n",
    "\n",
    "For each disagreement, click on the article ID link in the row to go to the article and check to see if the human coding for the disagreement in question is correct ( [http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/](http://research.local/sourcenet/sourcenet/article/article_data/view_with_text/) ).\n",
    "\n",
    "Once you've evaluated and verified the human coding, remove the \"`TODO`\" tag from the current record (either from the single-article view above if you've removed all disagreements, or from the disagreement view if not):\n",
    "\n",
    "- Click the checkbox in the \"**select**\" column next to the record whose evaluation is complete.\n",
    "- In the \"**Reliability names action:**\" field, select \"_Remove tag(s) from selected_\".\n",
    "- In the \"**Tag(s) - (comma-delimited):**\" field, enter \"_`TODO`_\" (without the quotes).\n",
    "- Click the \"**Do Action**\" button.\n",
    "- This will also place information on the `Reliability_Names` record into a `Reliability_Names_Evaluation` record in the database.  The message that results from this action completing will include a link to the record (the first number in the output).  Click the link to open the record and update it with additional details.  Specifically:\n",
    "\n",
    "    - status - status of human coder's coding:\n",
    "        \n",
    "        - If the human coder got it right, status is \"CORRECT\", even if OpenCalais had an egregious error.\n",
    "        - If this is because the coding screen couldn't capture compound names initially, set status to \"INCOMPLETE\", set the status message to \"SKIPPED because screen couldn't deal with compound names\", put the compound name string in notes, and then add tag \"compound_names\".\n",
    "        - if the OC coder had an issue because we had to smoosh all paragraphs together because it didn't deal well with HTML markup in the body of text it processed, set status to \"CORRECT\", set status message to \"OC ERROR because of formatting\", and then explain the problem in the notes.  If the article is a list or column with odd formatting, consider flagging the article for removal from the study.\n",
    "        - if you have to update ground truth, set \"`status`\" to \"ERROR\".\n",
    "        - else, use your best judgment.\n",
    "            \n",
    "    - if problems caused by automated coder error, click the \"`is_automated_error`\" checkbox.\n",
    "    - update the \"`status_message`\" so it contains a brief description of what exactly happened (should have been mentioned, should have been quoted, missed the person entirely, etc.).\n",
    "    - update \"`Notes`\" with more details.\n",
    "    \n",
    "        - If should have been quoted or mentioned, note the graf # and paragraph text of the paragraph that indicates this.\n",
    "    \n",
    "    - add \"`Tags`\" if appropriate (for sports articles, for example, add \"sports\" tag).\n",
    "\n",
    "_**NOTE: Always remove TODO tag first, so you have a record of status for each Reliability Names record.  Then, once you've done that, you can merge, delete, etc.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data in `Reliability_Names_Evaluation`\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall disagreement log\n",
    "\n",
    "- Track each Reliability_Names record with disagreement that we evaluate (All \"remove tags\" events with label \"prelim_month\"):\n",
    "- Moved to `Reliability_Names_Evaluation` table in django: [http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?label=prelim_month&o=-1.7.8.3.5](http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?label=prelim_month&o=-1.7.8.3.5)\n",
    "- [All \"remove tags\" events with label \"prelim_month\"](https://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=remove_tags&label=prelim_month&o=-1.7.8.3.5)\n",
    "- 428 total records.\n",
    "- of those, 13 are same person and article, but different `Reliability_Names` record, so disagreements that had to be corrected twice because of rebuilding `Reliability_Names` for the article (either human error, or something weird).  SQL:\n",
    "\n",
    "        SELECT sarne1.person_name,\n",
    "            sarne1.id,\n",
    "            sarne1.status,\n",
    "            sarne1.original_reliability_names_id,\n",
    "            sarne1.is_duplicate,\n",
    "            sarne2.is_duplicate,\n",
    "            sarne2.id,\n",
    "            sarne2.status,\n",
    "            sarne2.original_reliability_names_id\n",
    "        FROM sourcenet_analysis_reliability_names_evaluation AS sarne1,\n",
    "            sourcenet_analysis_reliability_names_evaluation AS sarne2\n",
    "        WHERE sarne1.id != sarne2.id\n",
    "            AND sarne1.label = 'prelim_month'\n",
    "            AND sarne2.label = 'prelim_month'\n",
    "            AND sarne1.event_type = 'remove_tags'\n",
    "            AND sarne2.event_type = 'remove_tags'\n",
    "            AND sarne1.article_id = sarne2.article_id\n",
    "            AND sarne1.person_name = sarne2.person_name\n",
    "            AND sarne1.original_reliability_names_id != sarne2.original_reliability_names_id\n",
    "            AND sarne2.original_reliability_names_id > sarne1.original_reliability_names_id\n",
    "        ORDER BY sarne1.id ASC;\n",
    "\n",
    "- So, 428 - 13 = 415 unique disagreements.\n",
    "- Could regenerate Reliability_Names without `ground_truth` to look at original counts?  Should be able to...  Just need to make sure I remember all steps...\n",
    "\n",
    "    - would need to clear single names, then I'd be left with disagreements.  Not worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flag duplicates\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "TODO:\n",
    "\n",
    "- duplicates\n",
    "\n",
    "    - Make sure that for each pair of duplicates, one has \"is_duplicate\" checked.\n",
    "    - to start, mark all that have duplicates as \"is_to_do\" = True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: get IDs of records with duplicates**\n",
    "\n",
    "    SELECT DISTINCT ( sarne1.id )\n",
    "    FROM sourcenet_analysis_reliability_names_evaluation AS sarne1,\n",
    "        sourcenet_analysis_reliability_names_evaluation AS sarne2\n",
    "    WHERE sarne1.id != sarne2.id\n",
    "        AND sarne1.label = 'prelim_month'\n",
    "        AND sarne2.label = 'prelim_month'\n",
    "        AND sarne1.event_type = 'remove_tags'\n",
    "        AND sarne2.event_type = 'remove_tags'\n",
    "        AND sarne1.article_id = sarne2.article_id\n",
    "        AND sarne1.person_name = sarne2.person_name\n",
    "        AND sarne1.original_reliability_names_id != sarne2.original_reliability_names_id\n",
    "        AND sarne2.original_reliability_names_id > sarne1.original_reliability_names_id\n",
    "    ORDER BY sarne1.id ASC;\n",
    "\n",
    "and\n",
    "\n",
    "    SELECT DISTINCT ( sarne2.id )\n",
    "    FROM sourcenet_analysis_reliability_names_evaluation AS sarne1,\n",
    "        sourcenet_analysis_reliability_names_evaluation AS sarne2\n",
    "    WHERE sarne1.id != sarne2.id\n",
    "        AND sarne1.label = 'prelim_month'\n",
    "        AND sarne2.label = 'prelim_month'\n",
    "        AND sarne1.event_type = 'remove_tags'\n",
    "        AND sarne2.event_type = 'remove_tags'\n",
    "        AND sarne1.article_id = sarne2.article_id\n",
    "        AND sarne1.person_name = sarne2.person_name\n",
    "        AND sarne1.original_reliability_names_id != sarne2.original_reliability_names_id\n",
    "        AND sarne2.original_reliability_names_id > sarne1.original_reliability_names_id\n",
    "    ORDER BY sarne2.id ASC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T19:21:30.434131Z",
     "start_time": "2018-02-09T19:21:30.381468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Got IDs that contain duplicates, now tag them as todo\n",
    "ids_to_process = []\n",
    "ids_to_process.append( 15 )\n",
    "ids_to_process.append( 33 )\n",
    "ids_to_process.append( 75 )\n",
    "ids_to_process.append( 405 )\n",
    "ids_to_process.append( 435 )\n",
    "ids_to_process.append( 512 )\n",
    "ids_to_process.append( 556 )\n",
    "ids_to_process.append( 586 )\n",
    "ids_to_process.append( 610 )\n",
    "ids_to_process.append( 620 )\n",
    "ids_to_process.append( 635 )\n",
    "ids_to_process.append( 646 )\n",
    "ids_to_process.append( 16 )\n",
    "ids_to_process.append( 34 )\n",
    "ids_to_process.append( 76 )\n",
    "ids_to_process.append( 407 )\n",
    "ids_to_process.append( 432 )\n",
    "ids_to_process.append( 513 )\n",
    "ids_to_process.append( 517 )\n",
    "ids_to_process.append( 558 )\n",
    "ids_to_process.append( 596 )\n",
    "ids_to_process.append( 611 )\n",
    "ids_to_process.append( 619 )\n",
    "ids_to_process.append( 637 )\n",
    "ids_to_process.append( 651 )\n",
    "\n",
    "# retrieve model instances.\n",
    "\n",
    "# get all evaluation records with label = \"prelim_month\" and IDs in our list.\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( pk__in = ids_to_process )\n",
    "\n",
    "# count?\n",
    "eval_count = evaluation_qs.count()\n",
    "print( \"record count: {}\".format( str( eval_count ) ) )\n",
    "\n",
    "# loop, setting \"is_to_do\" to True on each and saving.\n",
    "for current_eval in evaluation_qs:\n",
    "    \n",
    "    # set is_to_do to True and set work_status to \"metadata_review\".\n",
    "    current_eval.is_to_do = True\n",
    "    current_eval.work_status = \"duplicate_processing\"\n",
    "    current_eval.save()\n",
    "    \n",
    "#-- END loop over QuerySet. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out, many of these were a missed person by human coder, that, once fixed, revealed a problem with the automated coding.  So, many were actually not duplicates, they were two separate issues with the same person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CURRENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### review tags\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "TODO:\n",
    "\n",
    "- get all tags.\n",
    "- make boolean columns for each tag.\n",
    "- for each record, set flags based on tags.\n",
    "- then, flag all as todo and \"review_disagreements\", and go through them all to set metadata flags, and pick good ones for writing about disagreements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T03:42:04.707110Z",
     "start_time": "2018-02-12T03:42:03.751343Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 428 evaluations records.\n",
      "--> Count of articles with no tags: 188\n",
      "- ambiguity ( 2 )\n",
      "- ambiguous ( 10 )\n",
      "- complex ( 23 )\n",
      "- complex_title ( 1 )\n",
      "- complex_titles ( 25 )\n",
      "- compound_attribution ( 1 )\n",
      "- compound_names ( 9 )\n",
      "- contributed_to ( 2 )\n",
      "- dictionary_error ( 20 )\n",
      "- disambiguation ( 4 )\n",
      "- editing_error ( 3 )\n",
      "- error ( 204 )\n",
      "- follow_on_attribution ( 24 )\n",
      "- foreign_names ( 1 )\n",
      "- gender_confusion ( 2 )\n",
      "- initials ( 5 )\n",
      "- interesting ( 198 )\n",
      "- layout_or_design ( 3 )\n",
      "- list ( 14 )\n",
      "- lists ( 1 )\n",
      "- lookup ( 5 )\n",
      "- no_html ( 5 )\n",
      "- non_news ( 12 )\n",
      "- possessive ( 1 )\n",
      "- pronoun_attribution ( 2 )\n",
      "- pronouns ( 4 )\n",
      "- proper_noun ( 2 )\n",
      "- proper_nouns ( 38 )\n",
      "- quote_distance ( 12 )\n",
      "- said_verb ( 29 )\n",
      "- second_hand ( 10 )\n",
      "- short_n-gram ( 5 )\n",
      "- software_error ( 1 )\n",
      "- spanish ( 1 )\n",
      "- sports ( 6 )\n",
      "- straightforward ( 74 )\n",
      "- title_prefix ( 8 )\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "django-taggit documentation: https://github.com/alex/django-taggit\n",
    "\n",
    "Adding tags to a model:\n",
    "\n",
    "    from django.db import models\n",
    "    \n",
    "    from taggit.managers import TaggableManager\n",
    "    \n",
    "    class Food(models.Model):\n",
    "        # ... fields here\n",
    "    \n",
    "        tags = TaggableManager()\n",
    "\n",
    "Interacting with a model that has tags:\n",
    "\n",
    "    >>> apple = Food.objects.create(name=\"apple\")\n",
    "    >>> apple.tags.add(\"red\", \"green\", \"delicious\")\n",
    "    >>> apple.tags.all()\n",
    "    [<Tag: red>, <Tag: green>, <Tag: delicious>]\n",
    "    >>> apple.tags.remove(\"green\")\n",
    "    >>> apple.tags.all()\n",
    "    [<Tag: red>, <Tag: delicious>]\n",
    "    >>> Food.objects.filter(tags__name__in=[\"red\"])\n",
    "    [<Food: apple>, <Food: cherry>]\n",
    "    \n",
    "    # include only those with certain tags.\n",
    "    #tags_in_list = [ \"prelim_unit_test_001\", \"prelim_unit_test_002\", \"prelim_unit_test_003\", \"prelim_unit_test_004\", \"prelim_unit_test_005\", \"prelim_unit_test_006\", \"prelim_unit_test_007\" ]\n",
    "    tags_in_list = [ \"grp_month\", ]\n",
    "    if ( len( tags_in_list ) > 0 ):\n",
    "    \n",
    "        # filter\n",
    "        print( \"filtering to just articles with tags: \" + str( tags_in_list ) )\n",
    "        grp_article_qs = grp_article_qs.filter( tags__name__in = tags_in_list )\n",
    "        \n",
    "    #-- END check to see if we have a specific list of tags we want to include --#\n",
    "\n",
    "'''\n",
    "\n",
    "# imports\n",
    "from sourcenet_analysis.models import Reliability_Names_Evaluation\n",
    "\n",
    "# declare variables\n",
    "evaluation_qs = None\n",
    "record_count = -1\n",
    "record_counter = -1\n",
    "current_record = None\n",
    "tag_to_count_map = {}\n",
    "tag_qs = None\n",
    "tag_list = None\n",
    "current_tag = \"\"\n",
    "cleaned_tag = \"\"\n",
    "current_count = -1\n",
    "tag_count = -1\n",
    "no_tags_list = []\n",
    "\n",
    "# get all evaluation records with label = \"prelim_month\" and event_type = \"remove_tags\".\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( event_type = \"remove_tags\" )\n",
    "\n",
    "# first, just make sure that worked.\n",
    "record_count = evaluation_qs.count()\n",
    "\n",
    "# Check count of articles retrieved.\n",
    "print( \"Got \" + str( record_count ) + \" evaluations records.\" )\n",
    "\n",
    "# loop over evaluations.\n",
    "no_tags_count = 0\n",
    "for current_record in evaluation_qs:\n",
    "\n",
    "    # get tags.\n",
    "    # current_article.tags.add( tag_value )\n",
    "    tag_qs = current_record.tags.all()\n",
    "    \n",
    "    # output the tags.\n",
    "    #print( \"- Tags for record \" + str( current_record.id ) + \" : \" + str( tag_qs ) )\n",
    "    \n",
    "    # count tags\n",
    "    tag_count = tag_qs.count()\n",
    "    \n",
    "    # got tags?\n",
    "    if ( tag_count > 0 ):\n",
    "    \n",
    "        # loop over tags.\n",
    "        for current_tag in tag_qs:\n",
    "\n",
    "            # standardize\n",
    "            cleaned_tag = str( current_tag )\n",
    "\n",
    "            # to lower case\n",
    "            cleaned_tag = cleaned_tag.lower()\n",
    "\n",
    "            # strip()\n",
    "            cleaned_tag = cleaned_tag.strip()\n",
    "\n",
    "            # in map?  Get current count.\n",
    "            current_count = 0\n",
    "            if ( cleaned_tag in tag_to_count_map ): \n",
    "\n",
    "                # It is in map - get counter for it.\n",
    "                current_count = tag_to_count_map.get( cleaned_tag, None )\n",
    "\n",
    "            #-- END check to see if tag in map --#\n",
    "\n",
    "            # increment count and store.\n",
    "            current_count += 1\n",
    "            tag_to_count_map[ cleaned_tag ] = current_count\n",
    "\n",
    "        #-- END loop over tags --#\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # increment no_tag_counter bby 1.\n",
    "        no_tags_list.append( current_record )\n",
    "        \n",
    "    #-- END check to see if tags or not --#\n",
    "    \n",
    "#-- END loop over records --#\n",
    "\n",
    "# output number of tagless evaluations\n",
    "no_tags_count = len( no_tags_list )\n",
    "print( \"--> Count of articles with no tags: {}\".format( str( no_tags_count ) ) )\n",
    "\n",
    "# output tags\n",
    "key_view = six.viewkeys( tag_to_count_map )\n",
    "tag_list = list( key_view )\n",
    "tag_list.sort()\n",
    "for tag_string in tag_list:\n",
    "\n",
    "    # print each tag and its count.\n",
    "    current_count = tag_to_count_map.get( tag_string, -1 )\n",
    "    print( \"- {} ( {} )\".format( str( tag_string ), str( current_count ) ) )\n",
    "    \n",
    "#-- END print tags. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "--> Count of articles with no tags: 188\n",
    "\n",
    "Got 428 evaluations records.\n",
    "\n",
    "- ambiguity ( 2 )\n",
    "- ambiguous ( 10 )\n",
    "- complex ( 23 )\n",
    "- complex_title ( 1 )\n",
    "- complex_titles ( 25 )\n",
    "- compound_attribution ( 1 )\n",
    "- compound_names ( 9 )\n",
    "- contributed_to ( 2 )\n",
    "- dictionary_error ( 20 )\n",
    "- disambiguation ( 4 )\n",
    "- editing_error ( 3 )\n",
    "- error ( 204 )\n",
    "- follow_on_attribution ( 24 )\n",
    "- foreign_names ( 1 )\n",
    "- gender_confusion ( 2 )\n",
    "- initials ( 5 )\n",
    "- interesting ( 198 )\n",
    "- layout_or_design ( 3 )\n",
    "- list ( 14 )\n",
    "- lists ( 1 )\n",
    "- lookup ( 5 )\n",
    "- no_html ( 5 )\n",
    "- non_news ( 12 )\n",
    "- possessive ( 1 )\n",
    "- pronoun_attribution ( 2 )\n",
    "- pronouns ( 4 )\n",
    "- proper_noun ( 2 )\n",
    "- proper_nouns ( 38 )\n",
    "- quote_distance ( 12 )\n",
    "- said_verb ( 29 )\n",
    "- second_hand ( 10 )\n",
    "- short_n-gram ( 5 )\n",
    "- software_error ( 1 )\n",
    "- spanish ( 1 )\n",
    "- sports ( 6 )\n",
    "- straightforward ( 74 )\n",
    "- title_prefix ( 8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tags to create:\n",
    "\n",
    "- is_ambiguous = ambiguous, ambiguity\n",
    "- --> is_attribution_compound = compound_attribution\n",
    "- --> is_attribution_follow_on = follow_on_attribution\n",
    "- --> is_attribution_pronoun = pronoun_attribution\n",
    "- --> is_attribution_second_hand = second_hand\n",
    "- is_complex = complex\n",
    "- --> is_compound_names = compound_names\n",
    "- --> is_contributed_to (and is_subject_shb_author) = contributed_to\n",
    "- --> is_dictionary_error = dictionary_error\n",
    "- --> is_disambiguation = disambiguation\n",
    "- --> is_editing_error = editing_error\n",
    "- is_error = error\n",
    "- --> is_foreign_names = foreign_names\n",
    "- --> is_gender_confusion = gender_confusion\n",
    "- --> is_initials_error = initials\n",
    "- is_interesting = interesting\n",
    "- --> is_layout_or_design = layout_or_design\n",
    "- is_list = list, lists\n",
    "- --> is_lookup_error = lookup\n",
    "- is_no_html = no_html\n",
    "- is_not_hard_news = non_news\n",
    "- --> is_possessive = possessive\n",
    "- --> is_pronouns = pronouns\n",
    "- --> is_proper_noun (and is_not_a_person) = proper_noun, proper_nouns\n",
    "- --> is_quote_distance = quote_distance\n",
    "- --> is_said_verb = said_verb\n",
    "- --> is_short_n_gram = is_short_n-gram\n",
    "- --> is_software_error = software_error\n",
    "- --> is_spanish = spanish\n",
    "- is_sports = sports\n",
    "- is_straightforward = straightforward\n",
    "- is_title = complex_title, complex_titles, title_prefix\n",
    "- is_title_complex = complex_title, complex_titles\n",
    "- is_title_prefix = title_prefix\n",
    "\n",
    "Remember:\n",
    "\n",
    "- If error (\"-->\"), make sure to set \"is_error\", as well.\n",
    "- And, \"is_automated_error\" if automated error.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- verified that all fields are in model.\n",
    "- admin:\n",
    "\n",
    "    - make sure that all fields are in admin.\n",
    "    - reorganizing to make it a little less crazy.\n",
    "    - make sure all are in limit list.\n",
    "    \n",
    "- Need to automatically set the flags based on the tag values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-10T19:39:39.830453Z",
     "start_time": "2018-02-10T19:39:39.195648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 428 evaluations records.\n",
      "- ambiguity\n",
      "- ambiguous\n",
      "- complex\n",
      "- complex_title\n",
      "- complex_titles\n",
      "- compound_attribution\n",
      "- compound_names\n",
      "- contributed_to\n",
      "- dictionary_error\n",
      "- disambiguation\n",
      "- editing_error\n",
      "- error\n",
      "- follow_on_attribution\n",
      "- foreign_names\n",
      "- gender_confusion\n",
      "- initials\n",
      "- interesting\n",
      "- layout_or_design\n",
      "- list\n",
      "- lists\n",
      "- lookup\n",
      "- no_html\n",
      "- non_news\n",
      "- possessive\n",
      "- pronoun_attribution\n",
      "- pronouns\n",
      "- proper_noun\n",
      "- proper_nouns\n",
      "- quote_distance\n",
      "- said_verb\n",
      "- second_hand\n",
      "- short_n-gram\n",
      "- software_error\n",
      "- spanish\n",
      "- sports\n",
      "- straightforward\n",
      "- title_prefix\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "django-taggit documentation: https://github.com/alex/django-taggit\n",
    "\n",
    "Adding tags to a model:\n",
    "\n",
    "    from django.db import models\n",
    "    \n",
    "    from taggit.managers import TaggableManager\n",
    "    \n",
    "    class Food(models.Model):\n",
    "        # ... fields here\n",
    "    \n",
    "        tags = TaggableManager()\n",
    "\n",
    "Interacting with a model that has tags:\n",
    "\n",
    "    >>> apple = Food.objects.create(name=\"apple\")\n",
    "    >>> apple.tags.add(\"red\", \"green\", \"delicious\")\n",
    "    >>> apple.tags.all()\n",
    "    [<Tag: red>, <Tag: green>, <Tag: delicious>]\n",
    "    >>> apple.tags.remove(\"green\")\n",
    "    >>> apple.tags.all()\n",
    "    [<Tag: red>, <Tag: delicious>]\n",
    "    >>> Food.objects.filter(tags__name__in=[\"red\"])\n",
    "    [<Food: apple>, <Food: cherry>]\n",
    "    \n",
    "    # include only those with certain tags.\n",
    "    #tags_in_list = [ \"prelim_unit_test_001\", \"prelim_unit_test_002\", \"prelim_unit_test_003\", \"prelim_unit_test_004\", \"prelim_unit_test_005\", \"prelim_unit_test_006\", \"prelim_unit_test_007\" ]\n",
    "    tags_in_list = [ \"grp_month\", ]\n",
    "    if ( len( tags_in_list ) > 0 ):\n",
    "    \n",
    "        # filter\n",
    "        print( \"filtering to just articles with tags: \" + str( tags_in_list ) )\n",
    "        grp_article_qs = grp_article_qs.filter( tags__name__in = tags_in_list )\n",
    "        \n",
    "    #-- END check to see if we have a specific list of tags we want to include --#\n",
    "\n",
    "'''\n",
    "\n",
    "# imports\n",
    "from sourcenet_analysis.models import Reliability_Names_Evaluation\n",
    "\n",
    "# declare variables\n",
    "evaluation_qs = None\n",
    "record_count = -1\n",
    "record_counter = -1\n",
    "current_record = None\n",
    "tag_set = set()\n",
    "tag_qs = None\n",
    "tag_list = None\n",
    "current_tag = \"\"\n",
    "cleaned_tag = \"\"\n",
    "\n",
    "# get all evaluation records with label = \"prelim_month\" and event_type = \"remove_tags\".\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( event_type = \"remove_tags\" )\n",
    "\n",
    "# first, just make sure that worked.\n",
    "record_count = evaluation_qs.count()\n",
    "\n",
    "# Check count of articles retrieved.\n",
    "print( \"Got \" + str( record_count ) + \" evaluations records.\" )\n",
    "\n",
    "# loop over evaluations.\n",
    "for current_record in evaluation_qs:\n",
    "\n",
    "    # get tags.\n",
    "    # current_article.tags.add( tag_value )\n",
    "    tag_qs = current_record.tags.all()\n",
    "    \n",
    "    # output the tags.\n",
    "    #print( \"- Tags for record \" + str( current_record.id ) + \" : \" + str( tag_qs ) )\n",
    "    \n",
    "    # loop over tags.\n",
    "    for current_tag in tag_qs:\n",
    "        \n",
    "        # standardize\n",
    "        cleaned_tag = str( current_tag )\n",
    "\n",
    "        # to lower case\n",
    "        cleaned_tag = cleaned_tag.lower()\n",
    "        \n",
    "        # strip()\n",
    "        cleaned_tag = cleaned_tag.strip()\n",
    "        \n",
    "        # in set?\n",
    "        if ( cleaned_tag not in tag_set ): \n",
    "        \n",
    "            # If no, add to set.\n",
    "            tag_set.add( cleaned_tag )\n",
    "        \n",
    "    #-- END loop over tags --#\n",
    "    \n",
    "#-- END loop over records --#\n",
    "\n",
    "# output\n",
    "tag_list = list( tag_set )\n",
    "tag_list.sort()\n",
    "for tag_string in tag_list:\n",
    "\n",
    "    # print each tag.\n",
    "    print( \"- {}\".format( str( tag_string ) ) )\n",
    "    \n",
    "#-- END print tags. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth coding fixed\n",
    "\n",
    "- For some, the error will be on the part of the human coder.  For human error, we create a new \"`ground_truth`\" record that we will correct, so we preserve original coding (and evidence of errors) in case we want or need that information later.  Below, we have a table of the articles where we had to fix ground truth.  To find the original coding, click the Article link.\n",
    "- Denoted by records with \"`is_ground_truth_fixed`\" set to True in the `Reliability_Names_Evaluation` table in django:  [http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?is_ground_truth_fixed__exact=1&label=prelim_month&o=-1.7.8.3.5](http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?is_ground_truth_fixed__exact=1&label=prelim_month&o=-1.7.8.3.5)\n",
    "- 130 total (130/415 = 31.3% - this is a lot - is this right?)\n",
    "\n",
    "    - 4 duplicates, so call it 126.\n",
    "    - based on \"`prelim_month_human`\" `Reliability_Names` tag, 135 disagreements between original and corrected coding.  Probably some merging needed here?\n",
    "    - of those, 4 are same person and article, but different `Reliability_Names` record, so disagreements that had to be corrected twice because of rebuilding `Reliability_Names` for the article (either human error, or something else weird).  SQL:\n",
    "\n",
    "            SELECT sarne1.person_name,\n",
    "                sarne1.id,\n",
    "                sarne1.status,\n",
    "                sarne1.original_reliability_names_id,\n",
    "                sarne1.article_id,\n",
    "                sarne1.is_duplicate,\n",
    "                sarne2.is_duplicate,\n",
    "                sarne2.id,\n",
    "                sarne2.status,\n",
    "                sarne2.original_reliability_names_id,\n",
    "                sarne2.article_id\n",
    "            FROM sourcenet_analysis_reliability_names_evaluation AS sarne1,\n",
    "                sourcenet_analysis_reliability_names_evaluation AS sarne2\n",
    "            WHERE sarne1.id != sarne2.id\n",
    "                AND sarne1.label = 'prelim_month'\n",
    "                AND sarne2.label = 'prelim_month'\n",
    "                AND sarne1.event_type = 'remove_tags'\n",
    "                AND sarne2.event_type = 'remove_tags'\n",
    "                AND sarne1.is_ground_truth_fixed = TRUE\n",
    "                AND sarne2.is_ground_truth_fixed = TRUE\n",
    "                AND sarne1.article_id = sarne2.article_id\n",
    "                AND sarne1.person_name = sarne2.person_name\n",
    "                AND sarne1.original_reliability_names_id != sarne2.original_reliability_names_id\n",
    "                AND sarne2.original_reliability_names_id > sarne1.original_reliability_names_id\n",
    "            ORDER BY sarne1.id ASC;\n",
    "\n",
    "        Results (looks like these are ones that had to be merged, so ... minimze - when ambiguity, assume error in creating data, treat as duplicates so reduce count by number of duplicates):\n",
    "\n",
    "| person_name | id | status | original_reliability_names_id | article_id | id | status | original_reliability_names_id | article_id |\n",
    "|--|--|--|--|--|--|--|--|--|\n",
    "| Jeff Hawkins | 33 | ERROR | 9408 | [21007](https://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=21007) | 34 | ERROR | 9414 | 21007 |\n",
    "| Fritz Wahlfield | 405 | ERROR | 10330 | [22415](https://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=22415) | 407 | CORRECT | 10997 | 22415 |\n",
    "| John Agar | 610 | ERROR | 8917 | [23904](https://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23904) | 611 | ERROR | 8918 | 23904 |\n",
    "| Rachael Recker | 620 | ERROR | 8968 | [23920](https://research.local/sourcenet/sourcenet/article/article_data/view_with_text/?article_id=23920) | 619 | ERROR | 8976 | 23920 |\n",
    "\n",
    "- work:\n",
    "\n",
    "    - set all with \"is_ground_truth_fixed\" = True so \"is_to_do\" = True and \"`work_status = \"metadata_review\"`\".\n",
    "    - update all with `is_ground_truth_updated = True` so `is_human_error = True`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mark all ground truth updates as TODO\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-30T20:56:17.519327Z",
     "start_time": "2018-01-30T20:56:17.251741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record count: 130\n"
     ]
    }
   ],
   "source": [
    "# get all evaluation records with label = \"prelim_month\", is_ground_truth_fixed = True, and event_type = \"remove_tags\".\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( is_ground_truth_fixed = True )\n",
    "evaluation_qs = evaluation_qs.filter( event_type = \"remove_tags\" )\n",
    "\n",
    "# count?\n",
    "eval_count = evaluation_qs.count()\n",
    "print( \"record count: {}\".format( str( eval_count ) ) )\n",
    "\n",
    "# loop, setting \"is_to_do\" to True on each and saving.\n",
    "for current_eval in evaluation_qs:\n",
    "    \n",
    "    # set is_to_do to True and set work_status to \"metadata_review\".\n",
    "    current_eval.is_to_do = True\n",
    "    current_eval.work_status = \"metadata_review\"\n",
    "    current_eval.save()\n",
    "    \n",
    "#-- END loop over QuerySet. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mark all ground truth updates as human error\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T19:05:58.911127Z",
     "start_time": "2018-02-03T19:05:58.634517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record count: 127\n"
     ]
    }
   ],
   "source": [
    "# get all evaluation records with label = \"prelim_month\", is_ground_truth_fixed = True, and event_type = \"remove_tags\".\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( is_ground_truth_fixed = True )\n",
    "evaluation_qs = evaluation_qs.filter( event_type = \"remove_tags\" )\n",
    "\n",
    "# count?\n",
    "eval_count = evaluation_qs.count()\n",
    "print( \"record count: {}\".format( str( eval_count ) ) )\n",
    "\n",
    "# loop, setting \"is_human_error\" to True on each and saving.\n",
    "for current_eval in evaluation_qs:\n",
    "    \n",
    "    # set is_to_do to True and set work_status to \"metadata_review\".\n",
    "    current_eval.is_human_error = True\n",
    "    current_eval.save()\n",
    "    \n",
    "#-- END loop over QuerySet. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count ground truth updates\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: go through all \"is_to_do\" and update the metadata booleans.\n",
    "    \n",
    "- Link: [https://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?is_to_do__exact=1](https://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?is_to_do__exact=1)\n",
    "- for duplicates, set \"`is_duplicate`\" to True on one of the two records.  If two records for a given person because of the computer finding a person and human missing them, mark the computer coder's evaluation record as the duplicate.\n",
    "- for skipped, not a human error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-07T02:42:53.757489Z",
     "start_time": "2018-02-07T02:42:53.746860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record count: 123\n"
     ]
    }
   ],
   "source": [
    "# get all evaluation records with:\n",
    "# - label = \"prelim_month\"\n",
    "# - is_ground_truth_fixed = True\n",
    "# - event_type = \"remove_tags\"\n",
    "# - is_duplicate = False\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( is_ground_truth_fixed = True )\n",
    "evaluation_qs = evaluation_qs.filter( event_type = \"remove_tags\" )\n",
    "# evaluation_qs = evaluation_qs.filter( is_to_do = True )  # 130 originally\n",
    "evaluation_qs = evaluation_qs.filter( is_duplicate = False )  # now 123!\n",
    "\n",
    "# count?\n",
    "eval_count = evaluation_qs.count()\n",
    "print( \"record count: {}\".format( str( eval_count ) ) )\n",
    "\n",
    "# 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: update counts of characterizations above.  When filtering for counts:\n",
    "    \n",
    "- filter out duplicates when generating counts (include only where \"`is_duplicate`\" = False).\n",
    "- filter out skipped when generating counts (include only where \"`is_skipped`\" = False).\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-07T02:49:14.149056Z",
     "start_time": "2018-02-07T02:49:14.134707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record count: 102\n"
     ]
    }
   ],
   "source": [
    "# get all evaluation records with:\n",
    "# - label = \"prelim_month\"\n",
    "# - is_ground_truth_fixed = True\n",
    "# - is_human_error = True\n",
    "# - event_type = \"remove_tags\"\n",
    "# - is_duplicate = False\n",
    "evaluation_qs = Reliability_Names_Evaluation.objects.filter( label = \"prelim_month\" )\n",
    "evaluation_qs = evaluation_qs.filter( is_ground_truth_fixed = True )\n",
    "evaluation_qs = evaluation_qs.filter( is_human_error = True )\n",
    "evaluation_qs = evaluation_qs.filter( event_type = \"remove_tags\" )\n",
    "# evaluation_qs = evaluation_qs.filter( is_to_do = True )  # 130 originally\n",
    "evaluation_qs = evaluation_qs.filter( is_duplicate = False )  # now 123!\n",
    "\n",
    "# count?\n",
    "eval_count = evaluation_qs.count()\n",
    "print( \"record count: {}\".format( str( eval_count ) ) )\n",
    "\n",
    "# 102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "- 130 total, 123 without duplicates, 102 if one removes those where human skipped a person because of limitation of the coding application (compound names).\n",
    "- 102/415 = 31.3% - this is a lot - is this right?)\n",
    "- number of affected articles?\n",
    "- characterization of the problems:\n",
    "\n",
    "    - is_missed_author:\n",
    "    - is_missed_subject:\n",
    "    - is_skipped (limitation of coding application):\n",
    "    - is_author_shb_subject: *\n",
    "    - is_subject_shb_author: *\n",
    "    - is_quoted_shb_mentioned: *\n",
    "    - is_mentioned_shb_quoted: *\n",
    "    - is_wrong_text_captured: *\n",
    "    - is_duplicate:\n",
    "    \n",
    "- note:\n",
    "\n",
    "    - evaluation 469 is both author and subject.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- work:\n",
    "\n",
    "    - figure out number of affected articles (should just be count of Article_Data by ground_truth user).\n",
    "    - update metadata for all disagreements (all \"`remove_tags`\" events)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability_Names records merged\n",
    "\n",
    "- For some, need to merge a single-name detection by Calais with full-name detection by ground_truth (an OpenCalais error - did not detect full name - combined with lookup error - didn't lookup the right person since missed part of his or her name).  Will still have subsequently deleted one or more duplicate rows.\n",
    "- Denoted by records with \"`event_type`\" set to \"merge\" in the Reliability_Names_Evaluation table in django: [http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=merge&label=prelim_month&o=-1.7.8.3.5](http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=merge&label=prelim_month&o=-1.7.8.3.5)\n",
    "- 18 total\n",
    "- TODO: need to check for the repeat thing SQL like in remove tags.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleted Reliability_Names records\n",
    "\n",
    "- Some records need to be deleted:\n",
    "\n",
    "    - are just broken, need to be deleted.\n",
    "    - cleanup after a merge (one stays, one goes).\n",
    "    - cleanup after rebuilding `Reliability_Names` (single names removed once again, etc.).\n",
    "    - ?\n",
    "\n",
    "- Denoted by records with \"`event_type`\" set to \"deleted\" in the `Reliability_Names_Evaluation` table in django: [http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=delete&label=prelim_month&o=-1.7.8.3.5](http://research.local/sourcenet/admin/sourcenet_analysis/reliability_names_evaluation/?event_type__exact=delete&label=prelim_month&o=-1.7.8.3.5)\n",
    "- 181 total\n",
    "- TODO: need to provide examples.\n",
    "- TODO: need to check for the repeat thing SQL like in remove tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "\n",
    "- after correction:\n",
    "\n",
    "    - 2,446 overall coding decisions on people in \"`prelim_month`\".\n",
    "\n",
    "        - https://research.local/sourcenet/sourcenet/analysis/reliability/names/disagreement/view\n",
    "        - label: \"`prelim_month`\"\n",
    "        - coders to compare (1 ==>): 2\n",
    "        - Filter Type: \"Lookup\" (default)\n",
    "\n",
    "    - 294 disagreements in \"`prelim_month`\".\n",
    "    \n",
    "        - https://research.local/sourcenet/sourcenet/analysis/reliability/names/disagreement/view\n",
    "        - label: \"`prelim_month`\"\n",
    "        - coders to compare (1 ==>): 2\n",
    "        - Filter Type: \"Disagree\"\n",
    "\n",
    "    - 135 disagreements between humans and corrected.\n",
    "    \n",
    "        - https://research.local/sourcenet/sourcenet/analysis/reliability/names/disagreement/view\n",
    "        - label: \"`prelim_month_human`\"\n",
    "        - coders to compare (1 ==>): 2\n",
    "        - Filter Type: \"Disagree\"\n",
    "        - to more readily explore distribution of problems, added tag \"`prelim_month_human_disagree`\"to disagreements in \"`prelim_month_human`\":\n",
    "        \n",
    "            - all disagreements:\n",
    "            - label: \"`prelim_month_human`\"\n",
    "            - coders to compare (1 ==>): 2\n",
    "            - Filter Type: \"Lookup\"\n",
    "            - Reliability_Names tags: \"`prelim_month_human_disagree`\"\n",
    "            \n",
    "- TODO:\n",
    "\n",
    "    - look over human errors (disagreement between humans and corrected data).\n",
    "    \n",
    "        - Figure out types of error, counts of each type.  In SQL, filter on tag, then on other traits.\n",
    "        \n",
    "            - human miss: if coder 2 empty, 1 not, then human missed a person.\n",
    "            - human false detect: if coder 1 empty, 2 not, human erroneously detected person.\n",
    "            - \n",
    "            - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Disagreements - Human Error\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Human error - Per article (how many have ground truth?) and per decision?  How many errors, compared to total number of decisions, and what kind of errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disagreements - Computer Error\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Computer error - look over classes of error for trends (systemic error) and interesting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sourcenet (Python 3)",
   "language": "python",
   "name": "sourcenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "689px",
    "left": "0px",
    "right": "1118px",
    "top": "111px",
    "width": "322px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
